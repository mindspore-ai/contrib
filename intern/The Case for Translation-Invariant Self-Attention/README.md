# Translation-Invariant Self-Attention in Transformer-Based Language Models

This code is a MindSpore implementation of the paper **"The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models"**.

## Repository

You can find the full implementation on GitHub:
- [tisa GitHub Repository](https://github.com/ulmewennberg/tisa)

## Paper with Code

For more details and related resources, visit:
- [Paper with Code](https://paperswithcode.com/paper/the-case-for-translation-invariant-self)