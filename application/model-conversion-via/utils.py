import mindspore
from mindspore import nn,ops,Tensor
import copy
import numpy as np

def DPTrain(images, teacher, student, criterion_s, lr_gamma, clipping='auto', c=1e-4, e=0.001, sigma=100, batch_size=256, multiT=False, n_teachers=None, teachers=None):
    '''
    :param images: synthetic data (generated by generator in our case)
    :param teacher: teacher model
    :param student: student model
    :param criterion_s: loss function (KLdiv in our case)
    :param lr_gamma: learning rate
    :param clipping: clipping or normalizing
    :param c: clipping bound
    :param e: a positive stability constant (0.001 in our case)
    :param sigma: parameter of Gaussian noise
    :param batch_size: batch size
    :param multiT: multi teachers or not
    :param n_teachers: number of teachers if multi teachers
    :param teachers: list of teachers if multi teachers
    :return: a new output of student which achieves DP
    '''

    images = Tensor(images)
    grad_op = ops.GradOperation(get_all=True)
    t_out = teacher(images)
    s_out = student(ops.stop_gradient(images))
    if multiT:
        g = []
        for j in range(n_teachers):
            t_out_j = teachers[j](images)
            loss = criterion_s(s_out,t_out.stop_gradient())
            s_g = grad_op(criterion_s)(s_out, ops.stop_gradient(t_out_j))
            if clipping =='abadi':
                s_g = ops.clip_by_value(s_g, -c, c)
            else:
                norm = ops.norm(s_g, ord=2)
                s_g = s_g/(norm + e)
            g.append(s_g)
        s_g = sum(g)/n_teachers
    else:
        loss_old = criterion_s(s_out, ops.stop_gradient(t_out))
        s_g = grad_op(criterion_s)(s_out, ops.stop_gradient(t_out))[0]
        s_g_copy = s_g.copy()
        if clipping == 'abadi':
            s_g = ops.clip_by_value(s_g, -c, c)
        else:
            norm = ops.norm(s_g, ord=2)
            s_g = s_g / (norm + e)
        s_ = s_g.sum()/batch_size
        s_g = ops.fill(mindspore.float32,s_g.shape, s_)
    if clipping == 'abadi':
        noise = Tensor(np.random.normal(0, sigma*c, size=t_out.shape)/batch_size, dtype=t_out.dtype)
    else:
        noise = Tensor(np.random.normal(0, sigma, size=t_out.shape)/batch_size, dtype=t_out.dtype)
    s_out_new = s_out - lr_gamma*(s_g+noise)
    return s_out_new

if __name__ == "__main__":
    teacher = nn.Dense(10,5)
    student = nn.Dense(10,5)
    criterion_s = nn.KLDivLoss(reduction='batchmean')

    images = ops.randn(256,10)

    s_out_new = DPTrain(
            images = images,
            teacher = teacher,
            student = student,
            criterion_s = criterion_s,
            lr_gamma=0.01,
            clipping='auto',
            c=1e-4, e=0.001,
            sigma=100,
            batch_size=256,
            multiT=False,
            n_teachers=None,
            teachers=None)
    print(s_out_new)