{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 训练"
      ],
      "metadata": {
        "id": "mrglcXc3PDfD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KE7oZtzOwaP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "from mindspore import nn\n",
        "from mindspore.common import set_seed\n",
        "from mindspore import context\n",
        "from mindspore.context import ParallelMode\n",
        "from mindspore.communication.management import init, get_group_size, get_rank\n",
        "import mindspore\n",
        "import mindspore.dataset as ds\n",
        "from mindspore import load_checkpoint, load_param_into_net, save_checkpoint\n",
        "from src.image_transform import Normalize, NumpyResize, TransporeAndDiv\n",
        "from src.dataset import ImageDataset\n",
        "from src.network_D import DNet4_4_Train, DNetNext_Train, DNet4_4_Last, DNetNext_Last\n",
        "from src.network_G import GNet4_4_Train, GNet4_4_last, GNetNext_Train, GNetNext_Last\n",
        "from src.optimizer import AllLossD, AllLossG\n",
        "\n",
        "from model_utils.config import config\n",
        "from model_utils.moxing_adapter import moxing_wrapper\n",
        "from model_utils.device_adapter import get_device_id, get_device_num"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_every(num):\n",
        "    set_seed(num)\n",
        "    np.random.seed(num)\n",
        "\n",
        "set_every(1)\n",
        "\n",
        "def _get_rank_info():\n",
        "    \"\"\"\n",
        "    get rank size and rank id\n",
        "    \"\"\"\n",
        "    rank_size = int(os.environ.get(\"RANK_SIZE\", 1))\n",
        "    if rank_size > 1:\n",
        "        rank_size = get_group_size()\n",
        "        rank_id = get_rank()\n",
        "    else:\n",
        "        rank_size = rank_id = None\n",
        "\n",
        "    return rank_size, rank_id\n",
        "\n",
        "def cell_deepcopy(gnet, avg_gnet):\n",
        "    \"\"\"cell_deepcopy\"\"\"\n",
        "    for p, avg_p in zip(gnet.trainable_params(),\n",
        "                        avg_gnet.trainable_params()):\n",
        "        avg_p.set_data(p.clone())\n",
        "\n",
        "def cell_deepcopy_update(gnet, avg_gnet):\n",
        "    \"\"\"cell_deepcopy_update\"\"\"\n",
        "    for p, avg_p in zip(gnet.trainable_params(),\n",
        "                        avg_gnet.trainable_params()):\n",
        "        new_p = avg_p * 0.999 + p * 0.001\n",
        "        avg_p.set_data(new_p)\n",
        "\n",
        "def save_checkpoint_g(avg, gnet, dnet, ckpt_dir, i_batch):\n",
        "    \"\"\"save_checkpoint\"\"\"\n",
        "    save_checkpoint(gnet, os.path.join(ckpt_dir, \"G_{}.ckpt\".format(i_batch)))\n",
        "    save_checkpoint(avg, os.path.join(ckpt_dir, \"AvG_{}.ckpt\".format(i_batch)))\n",
        "    save_checkpoint(dnet, os.path.join(ckpt_dir, \"D_{}.ckpt\".format(i_batch)))\n",
        "\n",
        "def modelarts_pre_process():\n",
        "    '''modelarts pre process function.'''\n",
        "    config.ckpt_save_dir = os.path.join(config.output_path, config.ckpt_save_dir)\n",
        "\n",
        "def getDataset(args, size=None):\n",
        "    \"\"\"getDataset\n",
        "\n",
        "    Returns:\n",
        "        output.\n",
        "    \"\"\"\n",
        "    transformList = [NumpyResize(size), TransporeAndDiv(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        "    return ImageDataset(args.train_data_path, transform=transformList)\n",
        "\n",
        "def cal_each_batch_alpha():\n",
        "    \"\"\"buildNoiseData\"\"\"\n",
        "    each_batch_alpha = []\n",
        "    for index in range(len(config.scales)):\n",
        "        this_batch = config.num_batch[index]\n",
        "        new_batch_alpha = []\n",
        "        alphas = -1\n",
        "        new_jumps = config.alpha_jumps[index] / config.device_num\n",
        "        for i in range(this_batch):\n",
        "            if i % config.alpha_size_jumps[index] == 0:\n",
        "                alphas += 1\n",
        "            if i < new_jumps * config.alpha_size_jumps[index]:\n",
        "                new_batch_alpha.append(1 - alphas / new_jumps)\n",
        "            else:\n",
        "                new_batch_alpha.append(0.0)\n",
        "        each_batch_alpha.append(new_batch_alpha)\n",
        "    return each_batch_alpha"
      ],
      "metadata": {
        "id": "ACJ3CZgdPNMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getOptimizerD(dnet, args):\n",
        "    \"\"\"getOptimizerD\n",
        "\n",
        "    Returns:\n",
        "        output.\n",
        "    \"\"\"\n",
        "    manager = nn.DynamicLossScaleUpdateCell(loss_scale_value=2 ** args.loss_scale_value,\n",
        "                                            scale_factor=args.scale_factor, scale_window=args.scale_factor)\n",
        "    lossCell = AllLossD(dnet)\n",
        "    opti = nn.Adam(dnet.trainable_params(), beta1=0.0001, beta2=0.99, learning_rate=args.lr)\n",
        "    train_network = nn.TrainOneStepWithLossScaleCell(lossCell, opti, scale_sense=manager)\n",
        "    return train_network\n",
        "\n",
        "def getOptimizerG(gnet, dnet, args):\n",
        "    \"\"\"getOptimizerG\n",
        "\n",
        "    Returns:\n",
        "        output.\n",
        "    \"\"\"\n",
        "    manager = nn.DynamicLossScaleUpdateCell(loss_scale_value=2 ** args.loss_scale_value,\n",
        "                                            scale_factor=args.scale_factor, scale_window=args.scale_factor)\n",
        "    lossCell = AllLossG(gnet, dnet)\n",
        "    opti = nn.Adam(gnet.trainable_params(),\n",
        "                   beta1=0.0001, beta2=0.99, learning_rate=args.lr)\n",
        "    train_network = nn.TrainOneStepWithLossScaleCell(lossCell, opti, scale_sense=manager)\n",
        "    return train_network"
      ],
      "metadata": {
        "id": "AVXmilIKPfqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def buildNoiseData(n_samples):\n",
        "    \"\"\"buildNoiseData\n",
        "\n",
        "    Returns:\n",
        "        output.\n",
        "    \"\"\"\n",
        "    inputLatent = np.random.randn(n_samples, 512)\n",
        "    inputLatent = mindspore.Tensor(inputLatent, mindspore.float32)\n",
        "    return inputLatent"
      ],
      "metadata": {
        "id": "uIpT3yTrPmZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@moxing_wrapper(pre_process=modelarts_pre_process)\n",
        "def run_train():\n",
        "    \"\"\"buildNoiseData\"\"\"\n",
        "    cfg = config\n",
        "    context.set_context(mode=context.GRAPH_MODE, device_target=cfg.device_target)\n",
        "    cfg.device_num = get_device_num()\n",
        "    print(\"checkpoint:\", cfg.ckpt_save_dir)\n",
        "    if not os.path.exists(config.ckpt_save_dir):\n",
        "        os.mkdir(config.ckpt_save_dir)\n",
        "    if cfg.device_target == \"Ascend\":\n",
        "        device_id = get_device_id()\n",
        "        context.set_context(device_id=device_id)\n",
        "        if cfg.device_num > 1:\n",
        "            context.reset_auto_parallel_context()\n",
        "            context.set_auto_parallel_context(device_num=cfg.device_num, parallel_mode=ParallelMode.DATA_PARALLEL,\n",
        "                                              gradients_mean=True)\n",
        "            init()\n",
        "    each_batch_alpha = cal_each_batch_alpha()\n",
        "    for scale_index, scale in enumerate(cfg.scales):\n",
        "        this_scale_checkpoint = os.path.join(cfg.ckpt_save_dir, str(scale))\n",
        "        if not os.path.exists(this_scale_checkpoint):\n",
        "            os.mkdir(this_scale_checkpoint)\n",
        "        if scale == 4:\n",
        "            dnet = DNet4_4_Train(cfg.depth[scale_index], leakyReluLeak=0.2, sizeDecisionLayer=1, dimInput=3)\n",
        "            gnet = GNet4_4_Train(512, cfg.depth[scale_index], leakyReluLeak=0.2, dimOutput=3)\n",
        "            avg_gnet = GNet4_4_Train(512, cfg.depth[scale_index], leakyReluLeak=0.2, dimOutput=3)\n",
        "        elif scale == 8:\n",
        "            last_dnet = DNet4_4_Last(dnet)\n",
        "            last_gnet = GNet4_4_last(gnet)\n",
        "            dnet = DNetNext_Train(cfg.depth[scale_index], last_Dnet=last_dnet, leakyReluLeak=0.2, dimInput=3)\n",
        "            gnet = GNetNext_Train(cfg.depth[scale_index], last_Gnet=last_gnet, leakyReluLeak=0.2, dimOutput=3)\n",
        "            last_avg_gnet = GNet4_4_last(avg_gnet)\n",
        "            avg_gnet = GNetNext_Train(cfg.depth[scale_index], last_Gnet=last_avg_gnet, leakyReluLeak=0.2, dimOutput=3)\n",
        "        else:\n",
        "            last_dnet = DNetNext_Last(dnet)\n",
        "            last_gnet = GNetNext_Last(gnet)\n",
        "            dnet = DNetNext_Train(cfg.depth[scale_index], last_Dnet=last_dnet, leakyReluLeak=0.2, dimInput=3)\n",
        "            gnet = GNetNext_Train(cfg.depth[scale_index], last_gnet, leakyReluLeak=0.2, dimOutput=3)\n",
        "            last_avg_gnet = GNetNext_Last(avg_gnet)\n",
        "            avg_gnet = GNetNext_Train(cfg.depth[scale_index], last_avg_gnet, leakyReluLeak=0.2, dimOutput=3)\n",
        "        cell_deepcopy(gnet, avg_gnet)\n",
        "        if cfg.resume_load_scale != -1 and scale < cfg.resume_load_scale:\n",
        "            continue\n",
        "        elif cfg.resume_load_scale != -1 and scale == cfg.resume_load_scale:\n",
        "            param_dict_g = load_checkpoint(cfg.resume_check_g)\n",
        "            param_dict_d = load_checkpoint(cfg.resume_check_d)\n",
        "            load_param_into_net(gnet, param_dict_g)\n",
        "            load_param_into_net(dnet, param_dict_d)\n",
        "            continue\n",
        "        optimizerD = getOptimizerD(dnet, cfg)\n",
        "        optimizerG = getOptimizerG(gnet, dnet, cfg)\n",
        "        dbLoader = getDataset(cfg, (scale, scale))\n",
        "        rank_size, rank_id = _get_rank_info()\n",
        "        if rank_id:\n",
        "            this_scale_checkpoint = os.path.join(this_scale_checkpoint, \"rank_{}\".format(rank_id))\n",
        "        if not os.path.exists(this_scale_checkpoint):\n",
        "            os.mkdir(this_scale_checkpoint)\n",
        "        dataset = ds.GeneratorDataset(dbLoader, column_names=[\"data\", \"label\"], shuffle=True,\n",
        "                                      num_shards=rank_size, shard_id=rank_id)\n",
        "        dataset = dataset.batch(batch_size=cfg.batch_size, drop_remainder=True)\n",
        "        dataset_iter = dataset.create_tuple_iterator()\n",
        "        i_batch = 0\n",
        "        time_stamp = time.time()\n",
        "        while i_batch < cfg.num_batch[scale_index] / cfg.device_num:\n",
        "            epoch = 0\n",
        "            for data in dataset_iter:\n",
        "                alpha = each_batch_alpha[scale_index][i_batch]\n",
        "                alpha = mindspore.Tensor(alpha, mindspore.float32)\n",
        "                inputs_real = data[0]\n",
        "                n_samples = inputs_real.shape[0]\n",
        "                inputLatent = buildNoiseData(n_samples)\n",
        "                fake_image = gnet(inputLatent, alpha)\n",
        "                lossD, overflow, _ = optimizerD(inputs_real, fake_image, alpha)\n",
        "                inputNoise = buildNoiseData(n_samples)\n",
        "                lossG, overflow, _ = optimizerG(inputNoise, alpha)\n",
        "                cell_deepcopy_update(gnet=gnet, avg_gnet=avg_gnet)\n",
        "                i_batch += 1\n",
        "                if i_batch >= cfg.num_batch[scale_index] / cfg.device_num:\n",
        "                    break\n",
        "                if i_batch % 100 == 0:\n",
        "                    time_now = time.time()\n",
        "                    print('batch_i:{} alpha:{} loss G:{} loss D:{} overflow:{}'.format(i_batch, alpha,\n",
        "                                                                                       lossG, lossD, overflow))\n",
        "                    print(\"per step time is \", (time_now - time_stamp)/100, \"s\")\n",
        "                    time_stamp = time_now\n",
        "                if (i_batch + 1) % cfg.model_save_step == 0:\n",
        "                    save_checkpoint_g(avg_gnet, gnet, dnet, this_scale_checkpoint, i_batch)\n",
        "            epoch += 1\n",
        "        save_checkpoint_g(avg_gnet, gnet, dnet, this_scale_checkpoint, i_batch)\n",
        "\n",
        "run_train()"
      ],
      "metadata": {
        "id": "jjgqwiYTPsKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 测试"
      ],
      "metadata": {
        "id": "1EnEVsNeQIKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "from mindspore import context\n",
        "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
        "import mindspore\n",
        "from src.network_G import GNet4_4_Train, GNet4_4_last, GNetNext_Train, GNetNext_Last\n",
        "from src.image_transform import Normalize, TransporeAndMul, Resize\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "aA9fIT0YQEhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preLauch():\n",
        "    \"\"\"parse the console argument\"\"\"\n",
        "    parser = argparse.ArgumentParser(description='MindSpore PGAN training')\n",
        "    parser.add_argument('--device_id', type=int, default=0,\n",
        "                        help='device id of Ascend (Default: 0)')\n",
        "    parser.add_argument('--checkpoint_g', type=str, default='',\n",
        "                        help='checkpoint of g net (default )')\n",
        "    parser.add_argument('--img_out', type=str,\n",
        "                        default='img_eval', help='the dir of output img')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    context.set_context(device_id=args.device_id,\n",
        "                        mode=context.GRAPH_MODE,\n",
        "                        device_target=\"GPU\")\n",
        "    # if not exists 'img_out', make it\n",
        "    if not os.path.exists(args.img_out):\n",
        "        os.mkdir(args.img_out)\n",
        "    return args"
      ],
      "metadata": {
        "id": "2A8YW9WgQUKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def buildNoiseData(n_samples):\n",
        "    \"\"\"buildNoiseData\n",
        "\n",
        "    Returns:\n",
        "        output.\n",
        "    \"\"\"\n",
        "    inputLatent = np.random.randn(n_samples, 512)\n",
        "    inputLatent = mindspore.Tensor(inputLatent, mindspore.float32)\n",
        "    return inputLatent\n",
        "\n",
        "def image_compose(out_images, size=(8, 8)):\n",
        "    \"\"\"image_compose\n",
        "\n",
        "    Returns:\n",
        "        output.\n",
        "    \"\"\"\n",
        "    to_image = Image.new('RGB', (size[0] * 128, size[1] * 128))\n",
        "    for y in range(size[0]):\n",
        "        for x in range(size[1]):\n",
        "            from_image = Image.fromarray(out_images[y * size[0] + x])\n",
        "            to_image.paste(from_image, (x * 128, y * 128))\n",
        "    return to_image\n",
        "\n",
        "def resizeTensor(data, out_size_image):\n",
        "    \"\"\"resizeTensor\n",
        "\n",
        "    Returns:\n",
        "        output.\n",
        "    \"\"\"\n",
        "    out_data_size = (data.shape[0], data.shape[\n",
        "        1], out_size_image[0], out_size_image[1])\n",
        "    outdata = []\n",
        "    data = data.asnumpy()\n",
        "    data = np.clip(data, a_min=-1, a_max=1)\n",
        "    transformList = [Normalize((-1., -1., -1.), (2, 2, 2)), TransporeAndMul(), Resize(out_size_image)]\n",
        "    for img in range(out_data_size[0]):\n",
        "        processed = data[img]\n",
        "        for transform in transformList:\n",
        "            processed = transform(processed)\n",
        "        processed = np.array(processed)\n",
        "        outdata.append(processed)\n",
        "    return outdata"
      ],
      "metadata": {
        "id": "q2BuiNEMQV-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_eval():\n",
        "    args = preLauch()\n",
        "    scales = [4, 8, 16, 32, 64, 128]\n",
        "    depth = [512, 512, 512, 512, 256, 128]\n",
        "    for scale_index, scale in enumerate(scales):\n",
        "        if scale == 4:\n",
        "            avg_gnet = GNet4_4_Train(512, depth[scale_index], leakyReluLeak=0.2, dimOutput=3)\n",
        "        elif scale == 8:\n",
        "            last_avg_gnet = GNet4_4_last(avg_gnet)\n",
        "            avg_gnet = GNetNext_Train(depth[scale_index], last_Gnet=last_avg_gnet, leakyReluLeak=0.2, dimOutput=3)\n",
        "        else:\n",
        "            last_avg_gnet = GNetNext_Last(avg_gnet)\n",
        "            avg_gnet = GNetNext_Train(depth[scale_index], last_avg_gnet, leakyReluLeak=0.2, dimOutput=3)\n",
        "    param_dict_g = load_checkpoint(args.checkpoint_g)\n",
        "    load_param_into_net(avg_gnet, param_dict_g)\n",
        "    inputNoise = buildNoiseData(64)\n",
        "    gen_imgs_eval = avg_gnet(inputNoise, 0.0)\n",
        "    out_images = resizeTensor(gen_imgs_eval, (128, 128))\n",
        "    to_image = image_compose(out_images)\n",
        "    to_image.save(os.path.join(args.img_out, \"result.jpg\"))\n",
        "\n",
        "run_eval()"
      ],
      "metadata": {
        "id": "RFhbg8sbQbdB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
