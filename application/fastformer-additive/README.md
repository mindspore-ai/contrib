This code is a mindspore implementation of fastformer-additive-attention-is-all-you-need which is available at https://github.com/wilile26811249/Fastformer-PyTorch.
paperswidthcocde link is https://paperswithcode.com/paper/fastformer-additive-attention-is-all-you-need.