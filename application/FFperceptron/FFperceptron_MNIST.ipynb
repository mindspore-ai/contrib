{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57f0adc-f88f-42a8-9c7a-8751e734c5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: GPU\n",
      "Epoch 1:\tLoss 59030.00390625\tTime 56s\tTest Error 12.299999999999997%\n",
      "Epoch 2:\tLoss 30825.529296875\tTime 110s\tTest Error 9.350000000000009%\n",
      "Epoch 3:\tLoss 22349.34375\tTime 166s\tTest Error 7.8999999999999915%\n",
      "Epoch 4:\tLoss 18233.3671875\tTime 221s\tTest Error 6.780000000000001%\n",
      "Epoch 5:\tLoss 15429.23046875\tTime 276s\tTest Error 6.179999999999993%\n",
      "Epoch 6:\tLoss 13746.412109375\tTime 331s\tTest Error 5.679999999999993%\n",
      "Epoch 7:\tLoss 12297.1728515625\tTime 387s\tTest Error 5.210000000000008%\n",
      "Epoch 8:\tLoss 11008.462890625\tTime 442s\tTest Error 4.939999999999998%\n",
      "Epoch 9:\tLoss 10210.99609375\tTime 500s\tTest Error 4.579999999999998%\n",
      "Epoch 10:\tLoss 9538.9736328125\tTime 556s\tTest Error 4.200000000000003%\n",
      "Epoch 11:\tLoss 8841.65625\tTime 612s\tTest Error 3.9599999999999937%\n",
      "Epoch 12:\tLoss 8372.162109375\tTime 667s\tTest Error 4.039999999999992%\n",
      "Epoch 13:\tLoss 7836.30517578125\tTime 723s\tTest Error 3.8400000000000034%\n",
      "Epoch 14:\tLoss 7419.05322265625\tTime 778s\tTest Error 3.3100000000000023%\n",
      "Epoch 15:\tLoss 7070.126953125\tTime 834s\tTest Error 3.4200000000000017%\n",
      "Epoch 16:\tLoss 6766.45068359375\tTime 892s\tTest Error 3.450000000000003%\n",
      "Epoch 17:\tLoss 6537.4833984375\tTime 948s\tTest Error 3.6499999999999915%\n",
      "Epoch 18:\tLoss 6259.82666015625\tTime 1003s\tTest Error 3.5%\n",
      "Epoch 19:\tLoss 5981.49658203125\tTime 1061s\tTest Error 3.3599999999999994%\n",
      "Epoch 20:\tLoss 5829.15478515625\tTime 1115s\tTest Error 3.1599999999999966%\n",
      "Epoch 21:\tLoss 5573.1982421875\tTime 1173s\tTest Error 3.1700000000000017%\n",
      "Epoch 22:\tLoss 5379.2705078125\tTime 1231s\tTest Error 3.0799999999999983%\n",
      "Epoch 23:\tLoss 5286.93505859375\tTime 1289s\tTest Error 3.0900000000000034%\n",
      "Epoch 24:\tLoss 5114.296875\tTime 1348s\tTest Error 3.1299999999999955%\n",
      "Epoch 25:\tLoss 5082.86865234375\tTime 1403s\tTest Error 3.019999999999996%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import mindspore as ms\n",
    "import mindspore.dataset as ds\n",
    "from mindspore import nn, ops, Tensor\n",
    "from mindspore.dataset.transforms import Compose\n",
    "from mindspore.common.initializer import HeUniform\n",
    "from mindspore.dataset.vision import ToTensor, Normalize\n",
    "\n",
    "\n",
    "def one_hot_encode(img0, lab):\n",
    "    img = img0.copy()\n",
    "    img[:, :10] = img0.min()\n",
    "    img[range(img0.shape[0]), lab] = img0.max()\n",
    "    return img\n",
    "\n",
    "#Load MNIST Data\n",
    "data_path = './MNIST_data/'\n",
    "\n",
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize((0.1307,), (0.3081,)),\n",
    "    lambda x: x.flatten()\n",
    "])\n",
    "\n",
    "train_loader = ds.MnistDataset(data_path, usage='train')\n",
    "train_loader = train_loader.map(operations=transform, input_columns=\"image\")\n",
    "train_loader = train_loader.batch(60000)\n",
    "\n",
    "test_loader = ds.MnistDataset(data_path, usage='test')\n",
    "test_loader = test_loader.map(operations=transform, input_columns=\"image\")\n",
    "test_loader = test_loader.batch(10000)\n",
    "\n",
    "\n",
    "ms.context.set_context(device_target=\"GPU\")\n",
    "print('Using device:', ms.context.get_context('device_target'))\n",
    "device = ms.context.get_context('device_target')\n",
    "\n",
    "for data in train_loader.create_dict_iterator():\n",
    "    img0 = data[\"image\"].asnumpy()\n",
    "    lab = data[\"label\"].asnumpy()\n",
    "    break\n",
    "\n",
    "for data in test_loader.create_dict_iterator():\n",
    "    img0_tst = data[\"image\"].asnumpy()\n",
    "    lab_tst = data[\"label\"].asnumpy()\n",
    "    break\n",
    "\n",
    "# Forward Forward Applied to a Single Perceptron for MNIST Classification\n",
    "n_input, n_out = 784, 125\n",
    "batch_size, learning_rate = 10, 0.0003\n",
    "g_threshold = 10\n",
    "epochs = 250\n",
    "\n",
    "perceptron = nn.SequentialCell(\n",
    "    nn.Dense(n_input, n_out, weight_init=HeUniform(), has_bias=True),\n",
    "    nn.ReLU()\n",
    ")\n",
    "\n",
    "optimizer = nn.Adam(perceptron.trainable_params(), learning_rate=learning_rate)\n",
    "\n",
    "# Define forward propagation and loss calculations\n",
    "def forward_fn(img_pos_batch, img_neg_batch):\n",
    "    g_pos = (perceptron(img_pos_batch)**2).mean(axis=1)\n",
    "    loss_pos = ops.log(1 + ops.exp(-(g_pos - g_threshold))).sum()\n",
    "\n",
    "    g_neg = (perceptron(img_neg_batch)**2).mean(axis=1)\n",
    "    loss_neg = ops.log(1 + ops.exp(g_neg - g_threshold)).sum()\n",
    "\n",
    "    loss = loss_pos + loss_neg\n",
    "    return loss\n",
    "\n",
    "# Define the gradient function\n",
    "grad_fn = ops.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=False)\n",
    "\n",
    "N_trn = img0.shape[0] #Use all training images (60000)\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    img = img0.copy()\n",
    "\n",
    "    for i in range(10):  # Random jittering of training images up to 2 pixels\n",
    "        dx, dy = ops.randint(-2, 2, (2,))\n",
    "        dx, dy = int(dx.asnumpy()), int(dy.asnumpy())\n",
    "\n",
    "        # Scroll the rows and columns separately\n",
    "        img[i] = ops.roll(Tensor(img0[i].reshape(28, 28)), shifts=dy, dims=0).flatten().asnumpy()\n",
    "        img[i] = ops.roll(Tensor(img[i].reshape(28, 28)), shifts=dx, dims=1).flatten().asnumpy()\n",
    "\n",
    "    perm = np.random.permutation(N_trn)\n",
    "    img_pos = one_hot_encode(img[perm], lab[perm])\n",
    "\n",
    "    lab_permuted = Tensor(lab[perm], dtype=ms.int32)\n",
    "    rand_integers = ops.randint(low=1, high=10, size=(lab_permuted.shape))\n",
    "    lab_neg = ops.add(lab_permuted, rand_integers)\n",
    "    lab_neg = ops.select(ops.greater(lab_neg, 9), ops.sub(lab_neg, 10), lab_neg).asnumpy()\n",
    "    img_neg = one_hot_encode(img[perm], lab_neg)  # Bad data (random error in label)\n",
    "\n",
    "    L_tot = 0\n",
    "\n",
    "    for i in range(0, N_trn, batch_size):\n",
    "        perceptron.set_train(True)\n",
    "\n",
    "        # Goodness and loss for good data in batch\n",
    "        img_pos_batch = img_pos[i:i+batch_size]\n",
    "        img_pos_batch = Tensor(img_pos_batch, dtype=ms.float32)\n",
    "\n",
    "        # Goodness and loss for bad data in batch\n",
    "        img_neg_batch = img_neg[i:i+batch_size]\n",
    "        img_neg_batch = Tensor(img_neg_batch, dtype=ms.float32)\n",
    "\n",
    "        loss = forward_fn(img_pos_batch, img_neg_batch)\n",
    "        L_tot += loss.asnumpy() # Accumulate total loss for epoch\n",
    "\n",
    "        grads = grad_fn(img_pos_batch, img_neg_batch)[1] # Compute gradients\n",
    "        optimizer(grads) # Update parameters\n",
    "\n",
    "    # Test model with validation set\n",
    "    N_tst = img0_tst.shape[0]  # Use all test images (10000)\n",
    "    \n",
    "    #Evaluate goodness for all test images and labels 0...9\n",
    "    g_tst = ops.zeros((10, N_tst), dtype=ms.float32)\n",
    "    for n in range(10):\n",
    "        img_tst = one_hot_encode(img0_tst, n * np.ones_like(lab_tst))\n",
    "        img_tst = Tensor(img_tst, dtype=ms.float32)\n",
    "        g_tst[n] = ((perceptron(img_tst)**2).mean(axis=1))\n",
    "    predicted_label = g_tst.argmax(axis=0)\n",
    "    \n",
    "    # Count number of correctly classified images in validation set\n",
    "    correct_predictions = predicted_label == Tensor(lab_tst, dtype=ms.int32)\n",
    "    Ncorrect = correct_predictions.sum().asnumpy()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}:\\tLoss {L_tot}\\tTime {round(time.time() - tic)}s\\tTest Error {100 - Ncorrect / N_tst * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acf5077-cd77-4b1b-bfb1-601c781dd608",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3.9.0",
   "language": "python",
   "name": "python-3.9.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
