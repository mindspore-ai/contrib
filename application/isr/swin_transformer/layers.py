""" Some layers for build Cell """
import math
import warnings
from itertools import repeat
import collections.abc
import mindspore as ms
from mindspore import nn


def _ntuple(n):
    def parse(x):
        if isinstance(x, collections.abc.Iterable) and not isinstance(x, str):
            return tuple(x)
        return tuple(repeat(x, n))
    return parse


to_1tuple = _ntuple(1)
to_2tuple = _ntuple(2)
to_3tuple = _ntuple(3)
to_4tuple = _ntuple(4)
to_ntuple = _ntuple


def _trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if mean < a - 2 * std or mean > b + 2 * std:
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    # Values are generated by using a truncated uniform distribution and
    # then using the inverse CDF for the normal distribution.
    # Get upper and lower cdf values
    l = norm_cdf((a - mean) / std)
    u = norm_cdf((b - mean) / std)

    # Uniformly fill tensor with values from [l, u], then translate to
    # [2l-1, 2u-1].
    tensor = ms.ops.uniform(tensor.shape, ms.Tensor(2 * l - 1), ms.Tensor(2 * u - 1))

    # Use inverse cdf transform for normal distribution to get truncated
    # standard normal
    tensor = ms.ops.erfinv(tensor)

    # Transform to proper mean, std
    tensor = tensor * (std * math.sqrt(2.)) + mean

    # Clamp to ensure it's in the proper range
    tensor = ms.ops.clamp(tensor, min=a, max=b)
    return tensor


class DropPath(nn.Cell):
    """
    Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).
    """
    def __init__(self, drop_prob, ndim=2):
        super().__init__()
        self.drop = nn.Dropout(p=1 - drop_prob)
        shape = (1,) + (1,) * (ndim + 1)
        self.ndim = ndim
        self.mask = ms.Tensor(ms.ops.ones(shape), dtype=ms.float32)

    def construct(self, x):
        if not self.training:
            return x
        mask = ms.ops.Tile()(self.mask, (x.shape[0],) + (1,) * (self.ndim + 1))
        out = self.drop(mask)
        out = out * x
        return out