# Copyright 2021 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License Version 2.0(the "License");
# you may not use this file except in compliance with the License.
# you may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0#
#
# Unless required by applicable law or agreed to in writing software
# distributed under the License is distributed on an "AS IS" BASIS
# WITHOUT WARRANT IES OR CONITTONS OF ANY KINDï¼Œ either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ====================================================================================

"""Learning rate scheduler."""


from collections import Counter
import numpy as np


def linear_warmup_lr(current_step, warmup_steps, base_lr, init_lr):
    """Linear learning rate."""
    lr_inc = (float(base_lr) - float(init_lr)) / float(warmup_steps)
    lr = float(init_lr) + lr_inc * current_step
    return lr


def warmup_step_lr_1(
        lr,
        lr_epochs,
        steps_per_epoch,
        warmup_epochs,
        max_epoch,
        gamma=0.1):
    """Warmup step learning rate.
       This warmup warmups lr by *STEPS*
    """
    base_lr = lr
    warmup_init_lr = 0
    total_steps = int(max_epoch * steps_per_epoch)
    warmup_steps = int(warmup_epochs * steps_per_epoch)
    milestones = lr_epochs
    milestones_steps = []
    for milestone in milestones:
        milestones_step = milestone * steps_per_epoch
        milestones_steps.append(milestones_step)

    lr_each_step = []
    lr = base_lr
    milestones_steps_counter = Counter(milestones_steps)
    for i in range(total_steps):
        if i < warmup_steps:
            lr = linear_warmup_lr(i + 1, warmup_steps, base_lr, warmup_init_lr)
        else:
            lr = lr * gamma ** milestones_steps_counter[i]
        lr_each_step.append(lr)

    return np.array(lr_each_step).astype(np.float32)


def warmup_step_lr(
        base_lr,
        lr_epochs,
        steps_per_epoch,
        warmup_epochs,
        max_epoch,
        gamma=0.1):
    """Warmup step learning rate.
       This warmup warmups lr by *EPOCHS*
    """
    lr_each_epoch = []
    milestones_steps_counter = Counter(lr_epochs)
    lr = float(base_lr)
    for i in range(max_epoch):
        if i < warmup_epochs:
            lr = linear_warmup_lr(i + 1, warmup_epochs, base_lr, 0)
        else:
            lr = lr * gamma ** milestones_steps_counter[i]
        lr_each_epoch.append(lr)

    lr_each_step = []
    for epoch_lr in lr_each_epoch:
        lr_each_step += [epoch_lr] * steps_per_epoch

    return np.array(lr_each_step).astype(np.float32)


def multi_step_lr(lr, milestones, steps_per_epoch, max_epoch, gamma=0.1):
    return warmup_step_lr(
        lr,
        milestones,
        steps_per_epoch,
        0,
        max_epoch,
        gamma=gamma)


def step_lr(lr, epoch_size, steps_per_epoch, max_epoch, gamma=0.1):
    lr_epochs = []
    for i in range(1, max_epoch):
        if i % epoch_size[0] == 0:
            lr_epochs.append(i)
    return multi_step_lr(
        lr,
        lr_epochs,
        steps_per_epoch,
        max_epoch,
        gamma=gamma)
