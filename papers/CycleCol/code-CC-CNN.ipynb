{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fec7a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#本模块用于读取数据，生成加载器，相关参数设置如下：\n",
    "\n",
    "left_path='./data/color'\n",
    "right_path='./data/mono'\n",
    "split_ratio=0.8\n",
    "\n",
    "import random\n",
    "import cv2\n",
    "import mindspore.dataset as ds\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "def load_data_path(left_path,right_path,split_ratio=0.03):\n",
    "    #load data path\n",
    "    left_images=os.listdir(left_path)\n",
    "    right_images=os.listdir(right_path)\n",
    "    left_images.sort()\n",
    "    right_images.sort()\n",
    "    left_images_path = sorted(glob.glob(left_path + \"/*.png\"))\n",
    "    right_images_path = sorted(glob.glob(right_path + \"/*.png\"))\n",
    "    #print(left_images)\n",
    "    #print(right_images)\n",
    "    #print(left_images)\n",
    "    \n",
    "    #left_images_path=[left_path+'/'+img for img in left_images ]\n",
    "    #right_images_path=[right_path+'/'+img for img in right_images]\n",
    "    \n",
    "    #print(left_images_path[:10])\n",
    "    #print(right_images_path[:10])\n",
    "    \n",
    "    #split data\n",
    "    data_length=len(left_images_path)\n",
    "    temp=[(l,r) for l,r in zip(left_images_path,right_images_path)]\n",
    "    #print(temp[:10])  \n",
    "    random.shuffle(temp)\n",
    "    num_traindata=int(split_ratio*data_length)\n",
    "    train_data_path=temp[0:num_traindata]\n",
    "    #print(num_traindata)  638\n",
    "    val_data_path=temp[num_traindata:]\n",
    "    \n",
    "    return train_data_path,val_data_path\n",
    "\n",
    "#tra,val=load_data_path('/home/ma-user/work/data/left_2ndhisteq_rgb','/home/ma-user/work/data/left_long_exposure')\n",
    "def data_generator(data_path,is_train=True):\n",
    "    #input data_path:list of tuple with (left,right)\n",
    "    #output  dataset generator\n",
    "    #\n",
    "    #print('******************reading data*****************')\n",
    "    for data in data_path:\n",
    "        #print(data)\n",
    "        left_img=cv2.imread(data[0])\n",
    "        right_img=cv2.imread(data[1])\n",
    "        ###########resize\n",
    "        left_img = cv2.resize(left_img, (200, 100), interpolation=cv2.INTER_CUBIC) \n",
    "        right_img = cv2.resize(right_img, (200, 100),  interpolation=cv2.INTER_CUBIC)\n",
    "        ########Resize\n",
    "        #print(left_img.shape)\n",
    "        left_img_rotate = _rotateImage_(left_img)\n",
    "        right_img_rotate = _rotateImage_(right_img)\n",
    "        \n",
    "        img_shape = left_img_rotate.shape\n",
    "        left_geo_feat = _getGeometryFeat_(img_shape)\n",
    "        right_geo_feat = _getGeometryFeat_(img_shape)\n",
    "        \n",
    "        left_img_rotate = _centerImage_(left_img_rotate)\n",
    "        right_img_rotate = _centerImage_(right_img_rotate)\n",
    "        left_geo_feat = _centerImage_(left_geo_feat)\n",
    "        right_geo_feat = _centerImage_(right_geo_feat)\n",
    "        \n",
    "        #left_img_rotate = np.expand_dims(left_img_rotate, 0)\n",
    "        #right_img_rotate = np.expand_dims(right_img_rotate, 0)\n",
    "        #left_geo_feat = np.expand_dims(left_geo_feat, 0)\n",
    "        #right_geo_feat = np.expand_dims(right_geo_feat, 0)\n",
    "        #print(left_geo_feat.shape)\n",
    "        left_input = np.concatenate([left_img_rotate, left_geo_feat], axis = 2)\n",
    "        right_input = np.concatenate([right_img_rotate, right_geo_feat], axis = 2)\n",
    "        left_input=np.moveaxis(left_input,2,0)\n",
    "        right_input=np.moveaxis(right_input,2,0)\n",
    "        #left_input = np.expand_dims(left_input, 0)\n",
    "        #right_input = np.expand_dims(right_input, 0)\n",
    "        if is_train == True:\n",
    "            VUY_map = np.concatenate((left_img_rotate,right_img_rotate),axis=2)\n",
    "            VUY_map = np.moveaxis(VUY_map,2,0)\n",
    "            #VUY_map = np.expand_dims(VUY_map, 0)\n",
    "            traindata=[left_input, right_input]\n",
    "            yield traindata,VUY_map\n",
    "        else:\n",
    "            yield traindata\n",
    "\n",
    "            \n",
    "class imgDataset():\n",
    "    def __init__(self, tra):\n",
    "        super(imgDataset, self).__init__()\n",
    "        self.tra=tra\n",
    "        \n",
    "        #for data,label in enumerate(self.data_generator(self.tra)):\n",
    "        #   self.imgs.append(data)\n",
    "        #   self.labels.append(label)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return self.data_generator([self.tra[index]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tra)\n",
    "    \n",
    "    def data_generator(self,data_path,is_train=True):\n",
    "        #input data_path:list of tuple with (left,right)\n",
    "        #output  dataset generator\n",
    "        #\n",
    "        #print('******************reading data*****************')\n",
    "        for data in data_path:\n",
    "            #print(data)\n",
    "            left_img=cv2.imread(data[0])\n",
    "            right_img=cv2.imread(data[1])\n",
    "            ###########resize\n",
    "            \n",
    "            left_img = cv2.resize(left_img, (128, 80), interpolation=cv2.INTER_CUBIC) \n",
    "            right_img = cv2.resize(right_img, (128, 80),  interpolation=cv2.INTER_CUBIC)\n",
    "            ########Resize\n",
    "            #print(left_img.shape)\n",
    "            left_img_rotate = self._rotateImage_(left_img)\n",
    "            right_img_rotate = self._rotateImage_(right_img)\n",
    "        \n",
    "            img_shape = left_img_rotate.shape\n",
    "            left_geo_feat = self._getGeometryFeat_(img_shape)\n",
    "            right_geo_feat = self._getGeometryFeat_(img_shape)\n",
    "        \n",
    "            left_img_rotate =self._centerImage_(left_img_rotate)\n",
    "            right_img_rotate = self._centerImage_(right_img_rotate)\n",
    "            left_geo_feat = self._centerImage_(left_geo_feat)\n",
    "            right_geo_feat = self._centerImage_(right_geo_feat)\n",
    "        \n",
    "            #left_img_rotate = np.expand_dims(left_img_rotate, 0)\n",
    "            #right_img_rotate = np.expand_dims(right_img_rotate, 0)\n",
    "            #left_geo_feat = np.expand_dims(left_geo_feat, 0)\n",
    "            #right_geo_feat = np.expand_dims(right_geo_feat, 0)\n",
    "            #print(left_geo_feat.shape)\n",
    "            left_input = np.concatenate([left_img_rotate, left_geo_feat], axis = 2)\n",
    "            right_input = np.concatenate([right_img_rotate, right_geo_feat], axis = 2)\n",
    "            left_input=np.moveaxis(left_input,2,0)\n",
    "            right_input=np.moveaxis(right_input,2,0)\n",
    "            #left_input = np.expand_dims(left_input, 0)\n",
    "            #right_input = np.expand_dims(right_input, 0)\n",
    "            if is_train == True:\n",
    "                VUY_map = np.concatenate((left_img_rotate,right_img_rotate),axis=2)\n",
    "                VUY_map = np.moveaxis(VUY_map,2,0)\n",
    "                #VUY_map = np.expand_dims(VUY_map, 0)\n",
    "                traindata=[left_input, right_input]\n",
    "                return traindata,VUY_map\n",
    "            else:\n",
    "                return traindata\n",
    "            \n",
    "    def _centerImage_(self,img):\n",
    "        img = img.astype(np.float32)\n",
    "        return img\n",
    "    def _rotateImage_(self,img):\n",
    "        (h, w) = img.shape[:2]\n",
    "        center=(w/2-0.5,h/2-0.5)\n",
    "        M = cv2.getRotationMatrix2D(center, 180, 1.0)\n",
    "        rotated = cv2.warpAffine(img, M, (w, h))\n",
    "        return rotated\n",
    "        \n",
    "    def _getGeometryFeat_(self,img_shape):\n",
    "        H = img_shape[0]\n",
    "        W = img_shape[1]\n",
    "        feat = np.zeros((H,W,2))\n",
    "        for j in range(H):\n",
    "            for i in range(W):\n",
    "                feat[j,i,0]=np.min([j-0,H-1-j])/(H-1)*1.0            \n",
    "                feat[j,i,1]=np.min([i-0,W-1-i])/(W-1)*1.0\n",
    "        return feat\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77ce0003",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputshape:(1,2,5,h,w)\n",
    "#outputshape:(1,6,h,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a48d498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore as mds\n",
    "import mindspore.nn as nn\n",
    "import numpy as np\n",
    "import mindspore.numpy as mdsnp\n",
    "mds.context.set_context(device_target='Ascend',mode=mds.context.PYNATIVE_MODE)#  GRAPH_MODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ad900ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class resnetblock(nn.Cell):\n",
    "    def __init__(self,infea,outfea,stra=1,pad='same',df='NCHW'):\n",
    "        super(resnetblock,self).__init__()\n",
    "        self.conv1=nn.Conv2d(in_channels=infea,out_channels=outfea,kernel_size=3,stride=stra,pad_mode=pad,data_format=df)\n",
    "        self.bn1=nn.BatchNorm2d(num_features=outfea,data_format=df)\n",
    "        self.act=nn.ReLU()\n",
    "        self.conv2=nn.Conv2d(in_channels=outfea,out_channels=outfea,kernel_size=3,stride=stra,pad_mode=pad,data_format=df)\n",
    "        self.bn2=nn.BatchNorm2d(num_features=outfea,data_format=df)\n",
    "    \n",
    "        \n",
    "    def construct(self,input_img):\n",
    "        x=self.bn1(self.conv1(input_img))\n",
    "        x=self.act(x)\n",
    "        x=self.bn2(self.conv2(x))\n",
    "        return self.act(input_img)+x\n",
    "\n",
    "    \n",
    "    \n",
    "class UniFeature(nn.Cell):\n",
    "    def __init__(self,infea,outfea,pad='same',df='NCHW'): #3    8\n",
    "        super(UniFeature,self).__init__()\n",
    "        self.conv1=nn.Conv2d(in_channels=infea,out_channels=outfea,kernel_size=5,stride=1,pad_mode=pad,data_format=df)\n",
    "        self.bn1=nn.BatchNorm2d(num_features=outfea,data_format=df)\n",
    "        self.conv2=nn.Conv2d(in_channels=outfea,out_channels=outfea,kernel_size=3,stride=1,pad_mode=pad,data_format=df)\n",
    "        self.bn2=nn.BatchNorm2d(num_features=outfea,data_format=df)\n",
    "        self.conv3=nn.Conv2d(in_channels=outfea,out_channels=outfea,kernel_size=3,stride=1,pad_mode=pad,data_format=df)\n",
    "        self.bn3=nn.BatchNorm2d(num_features=outfea,data_format=df)\n",
    "        self.conv4=nn.Conv2d(in_channels=outfea,out_channels=outfea,kernel_size=3,stride=1,pad_mode=pad,data_format=df)\n",
    "        self.bn4=nn.BatchNorm2d(num_features=outfea,data_format=df)\n",
    "        self.act=nn.ReLU()\n",
    "        \n",
    "    def construct(self,input_img): #(none,none,3)\n",
    "        x=self.bn1(self.conv1(input_img))\n",
    "        x=self.act(x)\n",
    "        x=self.bn2(self.conv2(x))\n",
    "        x=self.act(x)\n",
    "        x=self.bn3(self.conv3(x))\n",
    "        x=self.act(x)\n",
    "        x=self.bn4(self.conv4(x))\n",
    "        return self.act(x)\n",
    "class processgeofeature(nn.Cell):\n",
    "    def __init__(self,infea,outfea,pad='same',df='NCHW'): #2  8  2\n",
    "        super(processgeofeature,self).__init__()\n",
    "        self.conv1=nn.Conv2d(in_channels=infea,out_channels=outfea,kernel_size=5,stride=1,pad_mode=pad,data_format=df)\n",
    "        self.bn1=nn.BatchNorm2d(num_features=outfea,data_format=df)\n",
    "        self.conv2=nn.Conv2d(in_channels=outfea,out_channels=outfea,kernel_size=3,stride=1,pad_mode=pad,data_format=df)\n",
    "        self.bn2=nn.BatchNorm2d(num_features=outfea,data_format=df)\n",
    "        self.conv3=nn.Conv2d(in_channels=outfea,out_channels=outfea,kernel_size=3,stride=1,pad_mode=pad,data_format=df)\n",
    "        self.bn3=nn.BatchNorm2d(num_features=outfea,data_format=df)\n",
    "        self.conv4=nn.Conv2d(in_channels=outfea,out_channels=infea,kernel_size=3,stride=1,pad_mode=pad,data_format=df)\n",
    "        self.bn4=nn.BatchNorm2d(num_features=infea,data_format=df)\n",
    "        self.act=nn.ReLU()\n",
    "    def construct(self,input_geo): # (none,none,2)\n",
    "        x=self.bn1(self.conv1(input_geo))\n",
    "        x=self.act(x)\n",
    "        x=self.bn2(self.conv2(x))\n",
    "        x=self.act(x)\n",
    "        x=self.bn3(self.conv3(x))\n",
    "        x=self.act(x)\n",
    "        x=self.bn4(self.conv4(x))\n",
    "        return self.act(x)\n",
    "    \n",
    "\n",
    "class addconv3d(nn.Cell):\n",
    "    def __init__(self,infea,outfea,stra=(1,1,1),ks=(3,3,3),pad='same',df='NCDHW'):\n",
    "        super(addconv3d,self).__init__()\n",
    "        self.conv3d1=nn.Conv3d(in_channels=infea,out_channels=outfea,kernel_size=ks,stride=stra,pad_mode=pad,data_format=df)\n",
    "        self.bn3d1=nn.BatchNorm3d(num_features=outfea,data_format=df)\n",
    "        self.act=nn.ReLU()\n",
    "    def construct(self,x):\n",
    "        x=self.bn3d1(self.conv3d1(x))\n",
    "        return self.act(x)\n",
    "\n",
    "    \n",
    "class convdownsampling(nn.Cell):\n",
    "    def __init__(self,infea,outfea,stra=(2,2,2),ks=(3,3,3),pad='same',df='NCDHW'):\n",
    "        super(convdownsampling,self).__init__()\n",
    "        self.addconv3d1=addconv3d(infea,outfea,stra)\n",
    "        self.addconv3d2=addconv3d(outfea,outfea)\n",
    "        self.addconv3d3=addconv3d(outfea,outfea)\n",
    "    def construct(self,x):\n",
    "        x=self.addconv3d1(x)\n",
    "        x=self.addconv3d2(x)\n",
    "        return self.addconv3d3(x)\n",
    "\n",
    "\n",
    "    \n",
    "class convupsampling(nn.Cell):\n",
    "    def __init__(self,infea,outfea,stra=(2,2,2),ks=(3,3,3),pad='same',df='NCDHW'):\n",
    "        super(convupsampling,self).__init__()\n",
    "        self.deconv3d=nn.Conv3dTranspose(in_channels=infea,out_channels=outfea,kernel_size=ks,stride=stra,pad_mode=pad,data_format=df)\n",
    "        self.bn3d=nn.BatchNorm3d(num_features=outfea,data_format=df)\n",
    "        self.act=nn.ReLU()\n",
    "    def construct(self,x):\n",
    "        x=self.bn3d(self.deconv3d(x))\n",
    "        return self.act(x)\n",
    "\n",
    "\n",
    "    \n",
    "class learnreg(nn.Cell):\n",
    "    def __init__(self,infea,outfea,stra=1,pad='same',df='NCDHW'): #20 8 1\n",
    "        super(learnreg,self).__init__()\n",
    "        self.conv3d1=addconv3d(infea,outfea)\n",
    "        self.conv3d2=addconv3d(outfea,outfea)  #C=8\n",
    "        self.downsampling1=convdownsampling(outfea,2*outfea)\n",
    "        self.downsampling2=convdownsampling(2*outfea,4*outfea)\n",
    "        self.upsampling1=convupsampling(4*outfea,2*outfea)\n",
    "        self.upsampling2=convupsampling(2*outfea,outfea)\n",
    "        self.conv3dlast=addconv3d(outfea,1)\n",
    "        self.add=mds.ops.Add()\n",
    "    def construct(self,x):\n",
    "        x=self.conv3d2(self.conv3d1(x)) \n",
    "        #print(x.shape)\n",
    "        temp1=x#c\n",
    "        x=self.downsampling1(x)\n",
    "        #print(x.shape)\n",
    "        temp2=x  #2c\n",
    "        x=self.downsampling2(x)  #4c\n",
    "        #print(x.shape)\n",
    "        x=self.add(self.upsampling1(x),temp2) #2c\n",
    "        #print(x.shape)\n",
    "        x=self.add(self.upsampling2(x),temp1) #c\n",
    "        #print(x.shape)\n",
    "        x=self.conv3dlast(x)\n",
    "        #print(x.shape)\n",
    "        return x\n",
    "\n",
    "#test=mds.Tensor(np.random.randint(0,10,[10,3,4,8,16]),mds.float32)\n",
    "#testmo=learnreg(3,6)\n",
    "#out=testmo(test)\n",
    "#print(out.shape)        \n",
    "            \n",
    "        \n",
    "class cyclecolornet(nn.Cell):\n",
    "    def __init__(self,base_num_filter=8,max_d=48,num_res=8):\n",
    "        super(cyclecolornet,self).__init__()\n",
    "        #self.ufea=UniFeature(3,base_num_filter) # inc base_num_filter\n",
    "        #self.res=resnetblock(base_num_filter,base_num_filter)\n",
    "        #self.pad=nn.Pad(paddings=((0,0),(0,0),(max_d,0),(0,0)),mode='CONSTANT') #  BCHW\n",
    "        self.pad=mds.ops.Pad(paddings=((0,0),(0,0),(max_d,0),(0,0))) #  BCHW\n",
    "        self.concat_dim1=mds.ops.Concat(1)\n",
    "        self.softmax=mds.ops.Softmax(axis=1)\n",
    "        \n",
    "        self.createunifea=UniFeature(3,base_num_filter)\n",
    "        self.resnet=resnetblock(base_num_filter,base_num_filter)\n",
    "        self.createcomfea=processgeofeature(2,base_num_filter)\n",
    "        self.learnreg=learnreg((base_num_filter+2)*2,base_num_filter)\n",
    "        self.base_num_filter=base_num_filter\n",
    "        self.max_d=max_d\n",
    "        self.num_res=num_res\n",
    "    def construct(self,inputs):\n",
    "        #########################################################\n",
    "        #########################################################\n",
    "        left_input =inputs[:,0,:,:,:]\n",
    "        right_input=inputs[:,1,:,:,:]\n",
    "        #left_input,right_input=inputs\n",
    "        #################################################\n",
    "        ################################################\n",
    "        #print(left_input.shape,right_input.shape,type(left_input))\n",
    "        \n",
    "        result1=self.precons([left_input,right_input])\n",
    "        #print(result1.shape)\n",
    "        result1_reverse=self.__img_reverse__(result1)\n",
    "        right_input_reverse=self.__img_reverse__(right_input)\n",
    "        result2=self.precons([right_input_reverse,result1_reverse])\n",
    "        \n",
    "        result2=self.__img_reverse__(result2)\n",
    "        result1_img=self.__getvuydata__(result1)\n",
    "        result2_img=self.__getvuydata__(result2)\n",
    "        #print(result1_img.shape,type(result1_img))\n",
    "        put_img_volume=self.__concatimg__([result1_img,result2_img])\n",
    "        return put_img_volume\n",
    "\n",
    "    def precons(self,inputs):#5 h w ,5 h w\n",
    "        #if len(inputs.shape) == 5:\n",
    "        #    inputs=inputs[0]\n",
    "        left_input,right_input=inputs\n",
    "        #print(left_input.shape,right_input.shape)\n",
    "        left_img=self.__getvuydata__(left_input)\n",
    "        right_img=self.__getvuydata__(right_input)\n",
    "        left_geo_feat=self.__getgeofeat__(left_input)\n",
    "        right_geo_feat=self.__getgeofeat__(right_input)\n",
    "        l_app_feature=self.createunifea(left_img) \n",
    "        \n",
    "        #print('l_app:',l_app_feature.shape)\n",
    "        \n",
    "        for res in range(self.num_res):\n",
    "            l_app_feature=self.resnet(l_app_feature)# 3->8\n",
    "            \n",
    "        #print('l_app_after:',l_app_feature.shape)\n",
    "        \n",
    "        r_app_feature=self.createunifea(right_img)\n",
    "        \n",
    "        #print('r_app:',r_app_feature.shape)\n",
    "        \n",
    "        for res in range(self.num_res):\n",
    "            r_app_feature=self.resnet(r_app_feature) #3->8\n",
    "        \n",
    "        #print('r_app_after:',r_app_feature.shape)\n",
    "        \n",
    "       \n",
    "        l_geo_feature=self.createcomfea(left_geo_feat) #2->8 -> 2\n",
    "        #print(l_geo_feature.shape)\n",
    "\n",
    "        r_geo_feature=self.createcomfea(right_geo_feat) #2 -> 8->2\n",
    "        l_feature=self.__concatimg__([l_app_feature,l_geo_feature]) #10=8+2\n",
    "        r_feature=self.__concatimg__([r_app_feature,r_geo_feature]) #10=8+2\n",
    "        #print(l_feature.shape)\n",
    "        unifeature=[l_feature,r_feature]\n",
    "        #print(l_feature.shape)\n",
    "        fv=self.__getfeaturevolume__(unifeature,self.max_d)#N 20 D H W\n",
    "        #print(fv.shape)\n",
    "        cv=self.learnreg(fv) # n 1 d h w\n",
    "        #print(cv.shape)\n",
    "        squeze=mds.ops.Squeeze(1)\n",
    "        wv=squeze(cv) #n d h w\n",
    "        unidata=[wv,right_img]\n",
    "        clolorization_result=self.__getweightaverage__(unidata,self.max_d)#bchw\n",
    "        #print(clolorization_result.shape)\n",
    "        output_result=self.__concatimg__([clolorization_result,left_geo_feat])\n",
    "        return output_result\n",
    "    def __img_reverse__(self,img): \n",
    "        #print(img.shape)\n",
    "        \n",
    "        return mds.Tensor(np.fliplr(img.asnumpy()))        \n",
    "        \n",
    "        \n",
    "    def __getvuydata__(self,inputs):\n",
    "        #print(inputs[:,0:3,:,:].shape)\n",
    "        return inputs[:,0:3,:,:] # BCHW\n",
    "    def __getgeofeat__(self,inputs):\n",
    "        #print(inputs[:,3:5,:,:].shape)\n",
    "        return inputs[:,3:5,:,:]\n",
    "    def __getyadta__(self,inputs):\n",
    "        #print(inputs[:,2:3,:,:].shape)\n",
    "        return inputs[:,2:3,:,:]\n",
    "    def __getvudata__(self,inputs):\n",
    "        #print(inputs[:,0:2,:,:].shape)\n",
    "        return inputs[:,0:2,:,:] \n",
    "    def __concatimg__(self,inputs): #BCHW\n",
    "        l,r=inputs\n",
    "        ops=mds.ops.Concat(axis=1)\n",
    "        return ops((l,r))\n",
    "    def __getfeaturevolume__(self,inputs,max_d):\n",
    "        left_tensor,right_tensor=inputs     \n",
    "        shape=right_tensor.shape   \n",
    "        #print(shape)\n",
    "        right_tensor=self.pad(right_tensor)\n",
    "        #print(right_tensor.shape)\n",
    "        disparity_costs=[]\n",
    "        for d in reversed(range(max_d)):\n",
    "            left_tensor_slice=left_tensor\n",
    "            slice_op=mds.ops.Slice()\n",
    "            \n",
    "            right_tensor_slice=slice_op(right_tensor,(0,0,d,0),(shape[0],shape[1],shape[2],shape[3]))  \n",
    "            #print(left_tensor_slice.shape,type(right_tensor_slice))\n",
    "            cost=self.concat_dim1((left_tensor_slice,right_tensor_slice))\n",
    "            disparity_costs.append(cost)\n",
    "        stack=mds.ops.Stack(axis=2)\n",
    "        feature_volume=stack(disparity_costs)\n",
    "        return feature_volume\n",
    "        #output N 2C D H W\n",
    "    def __getweightaverage__(self,inputs,max_d):\n",
    "        fv,right_image=inputs\n",
    "        weight=self.softmax(fv)  # bdhw\n",
    "        ref_V=right_image[:,0:1,:,:]\n",
    "        ref_U=right_image[:,1:2,:,:]\n",
    "        ref_Y=right_image[:,2:3,:,:]\n",
    "        \n",
    "        \n",
    "        right_tensor=ref_U\n",
    "        shape=right_tensor.shape\n",
    "        right_tensor=self.pad(right_tensor)\n",
    "        disparity_costs=[]\n",
    "        for d in reversed(range(max_d)):\n",
    "            slice_op=mds.ops.Slice()\n",
    "            right_tensor_slice=slice_op(right_tensor,(0,0,d,0),(shape[0],shape[1],shape[2],shape[3]))\n",
    "            disparity_costs.append(right_tensor_slice)\n",
    "        stack=mds.ops.Stack(axis=2)\n",
    "        cost_volume=stack(disparity_costs)\n",
    "        squeeze=mds.ops.Squeeze(1)\n",
    "        values=squeeze(cost_volume) #b d h w\n",
    "        mul=mds.ops.Mul()\n",
    "        c=mul(weight,values)\n",
    "        reduce_sum=mds.ops.ReduceSum(keep_dims=False)\n",
    "        U_map=reduce_sum(c,1)\n",
    "        \n",
    "        right_tensor=ref_V\n",
    "        shape=right_tensor.shape\n",
    "        right_tensor=self.pad(right_tensor)\n",
    "        disparity_costs=[]\n",
    "        for d in reversed(range(max_d)):\n",
    "            slice_op=mds.ops.Slice()\n",
    "            right_tensor_slice=slice_op(right_tensor,(0,0,d,0),(shape[0],shape[1],shape[2],shape[3]))\n",
    "            disparity_costs.append(right_tensor_slice)\n",
    "        stack=mds.ops.Stack(axis=2)\n",
    "        cost_volume=stack(disparity_costs)\n",
    "        squeeze=mds.ops.Squeeze(1)\n",
    "        values=squeeze(cost_volume) #b d h w\n",
    "        mul=mds.ops.Mul()\n",
    "        c=mul(weight,values)\n",
    "        reduce_sum=mds.ops.ReduceSum(keep_dims=False)\n",
    "        V_map=reduce_sum(c,1)\n",
    "        \n",
    "        right_tensor=ref_Y\n",
    "        shape=right_tensor.shape\n",
    "        right_tensor=self.pad(right_tensor)\n",
    "        disparity_costs=[]\n",
    "        for d in reversed(range(max_d)):\n",
    "            slice_op=mds.ops.Slice()\n",
    "            right_tensor_slice=slice_op(right_tensor,(0,0,d,0),(shape[0],shape[1],shape[2],shape[3]))\n",
    "            disparity_costs.append(right_tensor_slice)\n",
    "        stack=mds.ops.Stack(axis=2)\n",
    "        cost_volume=stack(disparity_costs)\n",
    "        squeeze=mds.ops.Squeeze(1)\n",
    "        values=squeeze(cost_volume) #b d h w\n",
    "        mul=mds.ops.Mul()\n",
    "        c=mul(weight,values)\n",
    "        reduce_sum=mds.ops.ReduceSum(keep_dims=False)\n",
    "        Y_map=reduce_sum(c,1)\n",
    "        \n",
    "        stack=mds.ops.Stack(axis=1)\n",
    "        VUY_map=stack([V_map,U_map,Y_map])\n",
    "        return VUY_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8785656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ebf9f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import Model\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d93135d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('./data/color/00420.png', './data/mono/00420.png'), ('./data/color/00693.png', './data/mono/00693.png')] 775\n"
     ]
    }
   ],
   "source": [
    "###加载数据  设置batchsize\n",
    "\n",
    "#print(tra)\n",
    "tra,val=load_data_path(left_path,right_path,split_ratio)\n",
    "print(tra[:2],len(tra))\n",
    "dataset=imgDataset(tra)\n",
    "#traindataset=ds.GeneratorDataset(data_generator(tra),column_names=['img','label'],num_parallel_workers=4)\n",
    "traindataset=ds.GeneratorDataset(dataset,column_names=['img','label'],num_parallel_workers=4)\n",
    "traindataset=traindataset.batch(1)\n",
    "NET=cyclecolornet()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9682486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import mindspore.nn as nn\n",
    "import numpy as np\n",
    "from mindspore.nn import Loss\n",
    "import mindspore.ops as ops\n",
    "import mindspore as ms\n",
    "from mindspore import Tensor\n",
    "class MISS_1(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(MISS_1,self).__init__()\n",
    "        self.lossfn=nn.SSIM()\n",
    "    def construct(self,data,label):\n",
    "        ssim=self.lossfn(data,label)\n",
    "        ones=mdsnp.full_like(ssim,1)\n",
    "        return ones-ssim\n",
    "class SLoss(nn.Cell):\n",
    "    def __init__(self, base_num_filter=8):\n",
    "        super(SLoss, self).__init__()\n",
    "        self.exp = ops.Exp()\n",
    "        self.conv2d = nn.Conv2d(in_channels = 1, out_channels = 3, kernel_size = 3)\n",
    "\n",
    "    def construct(self, pred, true):\n",
    "        loss = self.getloss(pred, true)\n",
    "        return loss\n",
    "\n",
    "    def getloss(self, pred, true):\n",
    "        \n",
    "        y_true = true[:, 0:6, 10:-10, 10:-10]\n",
    "        y_pred = pred[:, 0:6, 10:-10, 10:-10]\n",
    "        y_true_V = y_true[:, 0:1, :, :]\n",
    "        y_true_U = y_true[:, 1:2, :, :]\n",
    "        y_true_Y = y_true[:, 2:3, :, :]\n",
    "        y_true_reverse_V = y_true[:, 3:4, :, :]\n",
    "        y_true_reverse_U = y_true[:, 4:5, :, :]\n",
    "        y_true_reverse_Y = y_true[:, 5:6, :, :]\n",
    "\n",
    "        y_pred_V = y_pred[:, 0:1, :, :]\n",
    "        y_pred_U = y_pred[:, 1:2, :, :]\n",
    "        y_pred_Y = y_pred[:, 2:3, :, :]\n",
    "        y_pred_reverse_V = y_pred[:, 3:4, :, :]\n",
    "        y_pred_reverse_U = y_pred[:, 4:5, :, :]\n",
    "        y_pred_reverse_Y = y_pred[:, 5:6, :, :]\n",
    "        \n",
    "        #print(true)\n",
    "        #print(y_pred)\n",
    "        #ssim1=1\n",
    "        ssim1 = self.tf_ssim011(y_pred_Y, y_true_Y, max_val=255.0)\n",
    "        ssim2 = self.tf_ssim(y_pred_reverse_V, y_true_reverse_V, max_val=255.0)\n",
    "        ssim3 = self.tf_ssim(y_pred_reverse_U, y_true_reverse_U, max_val=255.0)\n",
    "\n",
    "        ssim = (ssim1 + ssim2 + ssim3) / 3.0\n",
    "        return 1 - ssim\n",
    "\n",
    "    def tf_ssim(self,img1, img2, max_val=1, cs_map=False, mean_metric=True):\n",
    "        K1 = 0.01\n",
    "        K2 = 0.03\n",
    "        L = max_val  # depth of image (255 in case the image has a different scale)\n",
    "        C1 = (K1 * L) ** 2\n",
    "        C2 = (K2 * L) ** 2\n",
    "        mu1 = self.conv2d(img1)\n",
    "        mu2 = self.conv2d(img2)\n",
    "        mu1_sq = mu1 * mu1\n",
    "        mu2_sq = mu2 * mu2\n",
    "        mu1_mu2 = mu1 * mu2\n",
    "\n",
    "        sigma1_sq = self.conv2d(img1 * img1) - mu1_sq\n",
    "        sigma2_sq = self.conv2d(img2 * img2) - mu2_sq\n",
    "        sigma12 = self.conv2d(img1 * img2) - mu1_mu2\n",
    "        if cs_map:\n",
    "            value = (((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) *\n",
    "                                                                  (sigma1_sq + sigma2_sq + C2)),\n",
    "                     (2.0 * sigma12 + C2) / (sigma1_sq + sigma2_sq + C2))\n",
    "        else:\n",
    "            value = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) *\n",
    "                                                                 (sigma1_sq + sigma2_sq + C2))\n",
    "        reducemean = ops.ReduceMean()\n",
    "        if mean_metric:\n",
    "            value = reducemean(value)\n",
    "        return value\n",
    "    \n",
    "    def tf_ssim011(self, img1, img2, max_val=1, mean_metric=True):\n",
    "        K1 = 0.01\n",
    "        K2 = 0.03\n",
    "        L = max_val  # depth of image (255 in case the image has a different scale)\n",
    "        C1 = (K1 * L) ** 2\n",
    "        C2 = (K2 * L) ** 2\n",
    "        #print(img1)\n",
    "        mu1 = self.conv2d(img1)\n",
    "        mu2 = self.conv2d(img2)\n",
    "        mu1_sq = mu1 * mu1\n",
    "        mu2_sq = mu2 * mu2\n",
    "        mu1_mu2 = mu1 * mu2\n",
    "\n",
    "        sigma1_sq = self.conv2d(img1 * img1) - mu1_sq\n",
    "        sigma2_sq = self.conv2d(img2 * img2) - mu2_sq\n",
    "        sigma12 = self.conv2d(img1 * img2) - mu1_mu2\n",
    "        value = (2.0 * sigma12 + C2) / (sigma1_sq + sigma2_sq + C2)\n",
    "        if mean_metric:\n",
    "            value = value.mean()\n",
    "        return value\n",
    "    \n",
    "testloss = SLoss()\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a572bd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start    training\n",
      "epoch: 1 step: 1, loss is 0.028251827\n",
      "epoch: 1 step: 2, loss is 0.027948678\n",
      "epoch: 1 step: 3, loss is 0.033882856\n",
      "epoch: 1 step: 4, loss is 0.038194418\n",
      "epoch: 1 step: 5, loss is 0.027364194\n",
      "epoch: 1 step: 6, loss is 0.030517042\n",
      "epoch: 1 step: 7, loss is 0.03376621\n",
      "epoch: 1 step: 8, loss is 0.06037593\n",
      "epoch: 1 step: 9, loss is 0.035456598\n",
      "epoch: 1 step: 10, loss is 0.051771462\n",
      "epoch: 1 step: 11, loss is 0.03462106\n",
      "epoch: 1 step: 12, loss is 0.035253048\n",
      "epoch: 1 step: 13, loss is 0.035629928\n",
      "epoch: 1 step: 14, loss is 0.032121718\n",
      "epoch: 1 step: 15, loss is 0.032562435\n",
      "epoch: 1 step: 16, loss is 0.023073375\n",
      "epoch: 1 step: 17, loss is 0.03265983\n",
      "epoch: 1 step: 18, loss is 0.032079518\n",
      "epoch: 1 step: 19, loss is 0.039499223\n",
      "epoch: 1 step: 20, loss is 0.034665048\n",
      "epoch: 1 step: 21, loss is 0.024638653\n",
      "epoch: 1 step: 22, loss is 0.032276154\n",
      "epoch: 1 step: 23, loss is 0.03088981\n",
      "epoch: 1 step: 24, loss is 0.035950243\n",
      "epoch: 1 step: 25, loss is 0.039882183\n",
      "epoch: 1 step: 26, loss is 0.034467936\n",
      "epoch: 1 step: 27, loss is 0.03278488\n",
      "epoch: 1 step: 28, loss is 0.03668332\n",
      "epoch: 1 step: 29, loss is 0.037017584\n",
      "epoch: 1 step: 30, loss is 0.036271095\n",
      "epoch: 1 step: 31, loss is 0.046373606\n",
      "epoch: 1 step: 32, loss is 0.03512007\n",
      "epoch: 1 step: 33, loss is 0.034528017\n",
      "epoch: 1 step: 34, loss is -0.024559379\n",
      "epoch: 1 step: 35, loss is 0.037746847\n",
      "epoch: 1 step: 36, loss is 0.028595924\n",
      "epoch: 1 step: 37, loss is 0.031844795\n",
      "epoch: 1 step: 38, loss is 0.005393088\n",
      "epoch: 1 step: 39, loss is 0.081270516\n",
      "epoch: 1 step: 40, loss is 0.077780545\n",
      "epoch: 1 step: 41, loss is 0.07765645\n",
      "epoch: 1 step: 42, loss is 0.08174801\n",
      "epoch: 1 step: 43, loss is 0.07953167\n",
      "epoch: 1 step: 44, loss is 0.07789087\n",
      "epoch: 1 step: 45, loss is 0.08030647\n",
      "epoch: 1 step: 46, loss is 0.06852341\n",
      "epoch: 1 step: 47, loss is 0.0808599\n",
      "epoch: 1 step: 48, loss is 0.07829714\n",
      "epoch: 1 step: 49, loss is 0.07354599\n",
      "epoch: 1 step: 50, loss is 0.066627264\n",
      "epoch: 1 step: 51, loss is 0.07502961\n",
      "epoch: 1 step: 52, loss is 0.07311934\n",
      "epoch: 1 step: 53, loss is 0.06677222\n",
      "epoch: 1 step: 54, loss is 0.07739705\n",
      "epoch: 1 step: 55, loss is 0.07778907\n",
      "epoch: 1 step: 56, loss is 0.07832962\n",
      "epoch: 1 step: 57, loss is 0.07381672\n",
      "epoch: 1 step: 58, loss is 0.083213985\n",
      "epoch: 1 step: 59, loss is 0.08069867\n",
      "epoch: 1 step: 60, loss is 0.081401885\n",
      "epoch: 1 step: 61, loss is 0.08530885\n",
      "epoch: 1 step: 62, loss is 0.079158545\n",
      "epoch: 1 step: 63, loss is 0.080171525\n",
      "epoch: 1 step: 64, loss is 0.077792466\n",
      "epoch: 1 step: 65, loss is 0.09043312\n",
      "epoch: 1 step: 66, loss is 0.037360847\n",
      "epoch: 1 step: 67, loss is 0.03066641\n",
      "epoch: 1 step: 68, loss is 0.04706216\n",
      "epoch: 1 step: 69, loss is 0.049051464\n",
      "epoch: 1 step: 70, loss is 0.0498147\n",
      "epoch: 1 step: 71, loss is 0.04334086\n",
      "epoch: 1 step: 72, loss is 0.039672077\n",
      "epoch: 1 step: 73, loss is 0.04776925\n",
      "epoch: 1 step: 74, loss is 0.0535717\n",
      "epoch: 1 step: 75, loss is 0.04650086\n",
      "epoch: 1 step: 76, loss is 0.045665085\n",
      "epoch: 1 step: 77, loss is 0.07815641\n",
      "epoch: 1 step: 78, loss is 0.08503121\n",
      "epoch: 1 step: 79, loss is 0.03220439\n",
      "epoch: 1 step: 80, loss is 0.07364458\n",
      "epoch: 1 step: 81, loss is 0.06815547\n",
      "epoch: 1 step: 82, loss is 0.0626089\n",
      "epoch: 1 step: 83, loss is 0.056065023\n",
      "epoch: 1 step: 84, loss is 0.09211761\n",
      "epoch: 1 step: 85, loss is 0.067524195\n",
      "epoch: 1 step: 86, loss is 0.059621513\n",
      "epoch: 1 step: 87, loss is 0.059423268\n",
      "epoch: 1 step: 88, loss is 0.054397643\n",
      "epoch: 1 step: 89, loss is 0.049381077\n",
      "epoch: 1 step: 90, loss is 0.07621145\n",
      "epoch: 1 step: 91, loss is 0.06457138\n",
      "epoch: 1 step: 92, loss is 0.027194679\n",
      "epoch: 1 step: 93, loss is 0.087981224\n",
      "epoch: 1 step: 94, loss is 0.0755437\n",
      "epoch: 1 step: 95, loss is 0.080922425\n",
      "epoch: 1 step: 96, loss is 0.08847779\n",
      "epoch: 1 step: 97, loss is 0.07600635\n",
      "epoch: 1 step: 98, loss is 0.08704716\n",
      "epoch: 1 step: 99, loss is 0.07587236\n",
      "epoch: 1 step: 100, loss is 0.11062932\n",
      "epoch: 1 step: 101, loss is 0.081061065\n",
      "epoch: 1 step: 102, loss is 0.099722326\n",
      "epoch: 1 step: 103, loss is 0.062766016\n",
      "epoch: 1 step: 104, loss is 0.0651536\n",
      "epoch: 1 step: 105, loss is 0.046667278\n",
      "epoch: 1 step: 106, loss is 0.048496306\n",
      "epoch: 1 step: 107, loss is 0.070403636\n",
      "epoch: 1 step: 108, loss is 0.050429046\n",
      "epoch: 1 step: 109, loss is 0.05801767\n",
      "epoch: 1 step: 110, loss is 0.049560845\n",
      "epoch: 1 step: 111, loss is 0.04312414\n",
      "epoch: 1 step: 112, loss is 0.049419105\n",
      "epoch: 1 step: 113, loss is 0.07511014\n",
      "epoch: 1 step: 114, loss is 0.062147856\n",
      "epoch: 1 step: 115, loss is 0.0421713\n",
      "epoch: 1 step: 116, loss is 0.068300426\n",
      "epoch: 1 step: 117, loss is 0.057594836\n",
      "epoch: 1 step: 118, loss is 0.06297588\n",
      "epoch: 1 step: 119, loss is 0.0643487\n",
      "epoch: 1 step: 120, loss is 0.052965105\n",
      "epoch: 1 step: 121, loss is 0.05685395\n",
      "epoch: 1 step: 122, loss is 0.06189972\n",
      "epoch: 1 step: 123, loss is 0.06925613\n",
      "epoch: 1 step: 124, loss is 0.06613994\n",
      "epoch: 1 step: 125, loss is 0.06355691\n",
      "epoch: 1 step: 126, loss is 0.06352854\n",
      "epoch: 1 step: 127, loss is 0.060799778\n",
      "epoch: 1 step: 128, loss is 0.07405347\n",
      "epoch: 1 step: 129, loss is 0.05600077\n",
      "epoch: 1 step: 130, loss is 0.054398\n",
      "epoch: 1 step: 131, loss is 0.07169193\n",
      "epoch: 1 step: 132, loss is 0.06253272\n",
      "epoch: 1 step: 133, loss is 0.060848534\n",
      "epoch: 1 step: 134, loss is 0.059033394\n",
      "epoch: 1 step: 135, loss is 0.07761669\n",
      "epoch: 1 step: 136, loss is 0.06104064\n",
      "epoch: 1 step: 137, loss is 0.07838559\n",
      "epoch: 1 step: 138, loss is 0.07630712\n",
      "epoch: 1 step: 139, loss is 0.049636662\n",
      "epoch: 1 step: 140, loss is 0.07042855\n",
      "epoch: 1 step: 141, loss is 0.055993855\n",
      "epoch: 1 step: 142, loss is 0.042318106\n",
      "epoch: 1 step: 143, loss is 0.045416415\n",
      "epoch: 1 step: 144, loss is 0.06542808\n",
      "epoch: 1 step: 145, loss is 0.06859547\n",
      "epoch: 1 step: 146, loss is 0.061362922\n",
      "epoch: 1 step: 147, loss is 0.061834514\n",
      "epoch: 1 step: 148, loss is 0.07489103\n",
      "epoch: 1 step: 149, loss is 0.07019466\n",
      "epoch: 1 step: 150, loss is 0.061128616\n",
      "epoch: 1 step: 151, loss is 0.06989694\n",
      "epoch: 1 step: 152, loss is 0.052534282\n",
      "epoch: 1 step: 153, loss is 0.056061268\n",
      "epoch: 1 step: 154, loss is 0.06191832\n",
      "epoch: 1 step: 155, loss is 0.07107717\n",
      "epoch: 1 step: 156, loss is 0.06579286\n",
      "epoch: 1 step: 157, loss is 0.06811857\n",
      "epoch: 1 step: 158, loss is 0.067214906\n",
      "epoch: 1 step: 159, loss is 0.043169677\n",
      "epoch: 1 step: 160, loss is 0.07071048\n",
      "epoch: 1 step: 161, loss is 0.070639074\n",
      "epoch: 1 step: 162, loss is 0.056725502\n",
      "epoch: 1 step: 163, loss is 0.071258545\n",
      "epoch: 1 step: 164, loss is 0.05463481\n",
      "epoch: 1 step: 165, loss is 0.054806888\n",
      "epoch: 1 step: 166, loss is 0.06542939\n",
      "epoch: 1 step: 167, loss is 0.05491942\n",
      "epoch: 1 step: 168, loss is 0.08546156\n",
      "epoch: 1 step: 169, loss is 0.03570491\n",
      "epoch: 1 step: 170, loss is 0.028508365\n",
      "epoch: 1 step: 171, loss is 0.03032142\n",
      "epoch: 1 step: 172, loss is 0.039727747\n",
      "epoch: 1 step: 173, loss is 0.029078066\n",
      "epoch: 1 step: 174, loss is 0.034029007\n",
      "epoch: 1 step: 175, loss is 0.031031787\n",
      "epoch: 1 step: 176, loss is 0.033439815\n",
      "epoch: 1 step: 177, loss is 0.040745437\n",
      "epoch: 1 step: 178, loss is 0.027756155\n",
      "epoch: 1 step: 179, loss is 0.034607947\n",
      "epoch: 1 step: 180, loss is 0.028907299\n",
      "epoch: 1 step: 181, loss is 0.030827343\n",
      "epoch: 1 step: 182, loss is 0.02350682\n",
      "epoch: 1 step: 183, loss is 0.027656734\n",
      "epoch: 1 step: 184, loss is 0.028016984\n",
      "epoch: 1 step: 185, loss is 0.027019024\n",
      "epoch: 1 step: 186, loss is 0.031904757\n",
      "epoch: 1 step: 187, loss is 0.033200324\n",
      "epoch: 1 step: 188, loss is 0.033839047\n",
      "epoch: 1 step: 189, loss is 0.027472079\n",
      "epoch: 1 step: 190, loss is 0.027607739\n",
      "epoch: 1 step: 191, loss is 0.0301826\n",
      "epoch: 1 step: 192, loss is 0.02913493\n",
      "epoch: 1 step: 193, loss is 0.029420674\n",
      "epoch: 1 step: 194, loss is 0.033612072\n",
      "epoch: 1 step: 195, loss is 0.03303665\n",
      "epoch: 1 step: 196, loss is 0.03342837\n",
      "epoch: 1 step: 197, loss is 0.028545916\n",
      "epoch: 1 step: 198, loss is 0.030076921\n",
      "epoch: 1 step: 199, loss is 0.03058511\n",
      "epoch: 1 step: 200, loss is 0.03376627\n",
      "epoch: 1 step: 201, loss is 0.03619194\n",
      "epoch: 1 step: 202, loss is 0.027874887\n",
      "epoch: 1 step: 203, loss is 0.031899393\n",
      "epoch: 1 step: 204, loss is 0.031264007\n",
      "epoch: 1 step: 205, loss is 0.035277665\n",
      "epoch: 1 step: 206, loss is 0.031985343\n",
      "epoch: 1 step: 207, loss is 0.037115037\n",
      "epoch: 1 step: 208, loss is 0.027709544\n",
      "epoch: 1 step: 209, loss is 0.033201694\n",
      "epoch: 1 step: 210, loss is 0.033016205\n",
      "epoch: 1 step: 211, loss is 0.033880234\n",
      "epoch: 1 step: 212, loss is 0.029329777\n",
      "epoch: 1 step: 213, loss is 0.031377554\n",
      "epoch: 1 step: 214, loss is 0.030328035\n",
      "epoch: 1 step: 215, loss is 0.032631397\n",
      "epoch: 1 step: 216, loss is 0.033869326\n",
      "epoch: 1 step: 217, loss is 0.026070416\n",
      "epoch: 1 step: 218, loss is 0.028826773\n",
      "epoch: 1 step: 219, loss is 0.034349144\n",
      "epoch: 1 step: 220, loss is 0.031890333\n",
      "epoch: 1 step: 221, loss is 0.024071217\n",
      "epoch: 1 step: 222, loss is 0.030559719\n",
      "epoch: 1 step: 223, loss is 0.033076584\n",
      "epoch: 1 step: 224, loss is 0.030423164\n",
      "epoch: 1 step: 225, loss is 0.04207039\n",
      "epoch: 1 step: 226, loss is 0.029148757\n",
      "epoch: 1 step: 227, loss is 0.024596274\n",
      "epoch: 1 step: 228, loss is 0.027848542\n",
      "epoch: 1 step: 229, loss is 0.03003472\n",
      "epoch: 1 step: 230, loss is 0.03441143\n",
      "epoch: 1 step: 231, loss is 0.03365451\n",
      "epoch: 1 step: 232, loss is 0.034160554\n",
      "epoch: 1 step: 233, loss is 0.03336972\n",
      "epoch: 1 step: 234, loss is 0.037460506\n",
      "epoch: 1 step: 235, loss is 0.035883605\n",
      "epoch: 1 step: 236, loss is 0.02917099\n",
      "epoch: 1 step: 237, loss is 0.039207637\n",
      "epoch: 1 step: 238, loss is 0.03489989\n",
      "epoch: 1 step: 239, loss is 0.04043895\n",
      "epoch: 1 step: 240, loss is -0.06469929\n",
      "epoch: 1 step: 241, loss is 0.035019875\n",
      "epoch: 1 step: 242, loss is 0.034720838\n",
      "epoch: 1 step: 243, loss is 0.022969723\n",
      "epoch: 1 step: 244, loss is 0.029511929\n",
      "epoch: 1 step: 245, loss is 0.036409616\n",
      "epoch: 1 step: 246, loss is 0.031143725\n",
      "epoch: 1 step: 247, loss is 0.03067565\n",
      "epoch: 1 step: 248, loss is 0.04253739\n",
      "epoch: 1 step: 249, loss is 0.036699295\n",
      "epoch: 1 step: 250, loss is 0.030487716\n",
      "epoch: 1 step: 251, loss is 0.028657496\n",
      "epoch: 1 step: 252, loss is 0.027266443\n",
      "epoch: 1 step: 253, loss is 0.030555427\n",
      "epoch: 1 step: 254, loss is 0.03191781\n",
      "epoch: 1 step: 255, loss is 0.03534746\n",
      "epoch: 1 step: 256, loss is 0.030079126\n",
      "epoch: 1 step: 257, loss is 0.0456717\n",
      "epoch: 1 step: 258, loss is 0.03162533\n",
      "epoch: 1 step: 259, loss is 0.028414309\n",
      "epoch: 1 step: 260, loss is 0.02944535\n",
      "epoch: 1 step: 261, loss is 0.029747784\n",
      "epoch: 1 step: 262, loss is 0.03430575\n",
      "epoch: 1 step: 263, loss is 0.036267996\n",
      "epoch: 1 step: 264, loss is 0.02826333\n",
      "epoch: 1 step: 265, loss is 0.04346639\n",
      "epoch: 1 step: 266, loss is 0.06039828\n",
      "epoch: 1 step: 267, loss is 0.03813529\n",
      "epoch: 1 step: 268, loss is 0.040994227\n",
      "epoch: 1 step: 269, loss is 0.04016608\n",
      "epoch: 1 step: 270, loss is 0.047366202\n",
      "epoch: 1 step: 271, loss is 0.023037434\n",
      "epoch: 1 step: 272, loss is 0.07612234\n",
      "epoch: 1 step: 273, loss is 0.07220936\n",
      "epoch: 1 step: 274, loss is 0.057883263\n",
      "epoch: 1 step: 275, loss is 0.030566692\n",
      "epoch: 1 step: 276, loss is 0.052484214\n",
      "epoch: 1 step: 277, loss is 0.090403795\n",
      "epoch: 1 step: 278, loss is 0.073807895\n",
      "epoch: 1 step: 279, loss is 0.07371473\n",
      "epoch: 1 step: 280, loss is 0.059501648\n",
      "epoch: 1 step: 281, loss is 0.0698995\n",
      "epoch: 1 step: 282, loss is 0.07202715\n",
      "epoch: 1 step: 283, loss is 0.021687746\n",
      "epoch: 1 step: 284, loss is 0.08677578\n",
      "epoch: 1 step: 285, loss is 0.08013034\n",
      "epoch: 1 step: 286, loss is 0.08194995\n",
      "epoch: 1 step: 287, loss is 0.071505845\n",
      "epoch: 1 step: 288, loss is 0.09360343\n",
      "epoch: 1 step: 289, loss is 0.23069507\n",
      "epoch: 1 step: 290, loss is 0.033038557\n",
      "epoch: 1 step: 291, loss is 0.030016422\n",
      "epoch: 1 step: 292, loss is 0.029861987\n",
      "epoch: 1 step: 293, loss is 0.031894863\n",
      "epoch: 1 step: 294, loss is 0.04805273\n",
      "epoch: 1 step: 295, loss is 0.03340119\n",
      "epoch: 1 step: 296, loss is 0.034206808\n",
      "epoch: 1 step: 297, loss is 0.03432113\n",
      "epoch: 1 step: 298, loss is 0.026403904\n",
      "epoch: 1 step: 299, loss is 0.03133744\n",
      "epoch: 1 step: 300, loss is 0.025484622\n",
      "epoch: 1 step: 301, loss is 0.021773994\n",
      "epoch: 1 step: 302, loss is 0.036966145\n",
      "epoch: 1 step: 303, loss is 0.03661853\n",
      "epoch: 1 step: 304, loss is 0.027633488\n",
      "epoch: 1 step: 305, loss is 0.030279458\n",
      "epoch: 1 step: 306, loss is 0.0326581\n",
      "epoch: 1 step: 307, loss is 0.025832355\n",
      "epoch: 1 step: 308, loss is 0.028043449\n",
      "epoch: 1 step: 309, loss is 0.044558287\n",
      "epoch: 1 step: 310, loss is 0.034548104\n",
      "epoch: 1 step: 311, loss is 0.02509737\n",
      "epoch: 1 step: 312, loss is 0.026697695\n",
      "epoch: 1 step: 313, loss is 0.036313593\n",
      "epoch: 1 step: 314, loss is 0.030534744\n",
      "epoch: 1 step: 315, loss is 0.029626071\n",
      "epoch: 1 step: 316, loss is 0.02741605\n",
      "epoch: 1 step: 317, loss is 0.034811318\n",
      "epoch: 1 step: 318, loss is 0.026284873\n",
      "epoch: 1 step: 319, loss is 0.025022805\n",
      "epoch: 1 step: 320, loss is 0.033798695\n",
      "epoch: 1 step: 321, loss is 0.06980276\n",
      "epoch: 1 step: 322, loss is 0.03512031\n",
      "epoch: 1 step: 323, loss is 0.027480304\n",
      "epoch: 1 step: 324, loss is 0.0353871\n",
      "epoch: 1 step: 325, loss is 0.03669685\n",
      "epoch: 1 step: 326, loss is 0.03262204\n",
      "epoch: 1 step: 327, loss is 0.034160912\n",
      "epoch: 1 step: 328, loss is 0.02664733\n",
      "epoch: 1 step: 329, loss is 0.027613163\n",
      "epoch: 1 step: 330, loss is 0.014339983\n",
      "epoch: 1 step: 331, loss is 0.03599626\n",
      "epoch: 1 step: 332, loss is 0.039164543\n",
      "epoch: 1 step: 333, loss is 0.021759093\n",
      "epoch: 1 step: 334, loss is 0.023693264\n",
      "epoch: 1 step: 335, loss is 0.03344536\n",
      "epoch: 1 step: 336, loss is 0.030433\n",
      "epoch: 1 step: 337, loss is 0.02970773\n",
      "epoch: 1 step: 338, loss is 0.027988195\n",
      "epoch: 1 step: 339, loss is 0.031882346\n",
      "epoch: 1 step: 340, loss is 0.030294359\n",
      "epoch: 1 step: 341, loss is 0.034807205\n",
      "epoch: 1 step: 342, loss is 0.032194436\n",
      "epoch: 1 step: 343, loss is 0.0340994\n",
      "epoch: 1 step: 344, loss is 0.034311533\n",
      "epoch: 1 step: 345, loss is 0.02962166\n",
      "epoch: 1 step: 346, loss is 0.02570039\n",
      "epoch: 1 step: 347, loss is 0.026565075\n",
      "epoch: 1 step: 348, loss is 0.035567224\n",
      "epoch: 1 step: 349, loss is 0.028816521\n",
      "epoch: 1 step: 350, loss is 0.026172161\n",
      "epoch: 1 step: 351, loss is 0.024713218\n",
      "epoch: 1 step: 352, loss is 0.033435524\n",
      "epoch: 1 step: 353, loss is 0.022665322\n",
      "epoch: 1 step: 354, loss is 0.028671443\n",
      "epoch: 1 step: 355, loss is 0.03044349\n",
      "epoch: 1 step: 356, loss is 0.03249669\n",
      "epoch: 1 step: 357, loss is 0.033348262\n",
      "epoch: 1 step: 358, loss is 0.023260772\n",
      "epoch: 1 step: 359, loss is 0.026341915\n",
      "epoch: 1 step: 360, loss is 0.02140528\n",
      "epoch: 1 step: 361, loss is 0.03918791\n",
      "epoch: 1 step: 362, loss is 0.031931162\n",
      "epoch: 1 step: 363, loss is 0.032476842\n",
      "epoch: 1 step: 364, loss is 0.029557228\n",
      "epoch: 1 step: 365, loss is 0.023034275\n",
      "epoch: 1 step: 366, loss is 0.027390182\n",
      "epoch: 1 step: 367, loss is 0.031547487\n",
      "epoch: 1 step: 368, loss is 0.034636974\n",
      "epoch: 1 step: 369, loss is 0.031222105\n",
      "epoch: 1 step: 370, loss is 0.025941193\n",
      "epoch: 1 step: 371, loss is 0.024785936\n",
      "epoch: 1 step: 372, loss is 0.025488079\n",
      "epoch: 1 step: 373, loss is 0.030892432\n",
      "epoch: 1 step: 374, loss is 0.02960968\n",
      "epoch: 1 step: 375, loss is 0.030526817\n",
      "epoch: 1 step: 376, loss is 0.034638703\n",
      "epoch: 1 step: 377, loss is 0.03172779\n",
      "epoch: 1 step: 378, loss is 0.03756064\n",
      "epoch: 1 step: 379, loss is 0.03489685\n",
      "epoch: 1 step: 380, loss is 0.030996561\n",
      "epoch: 1 step: 381, loss is 0.025494516\n",
      "epoch: 1 step: 382, loss is 0.03535527\n",
      "epoch: 1 step: 383, loss is 0.031401813\n",
      "epoch: 1 step: 384, loss is 0.031814277\n",
      "epoch: 1 step: 385, loss is 0.033902884\n",
      "epoch: 1 step: 386, loss is 0.035996914\n",
      "epoch: 1 step: 387, loss is 0.03576374\n",
      "epoch: 1 step: 388, loss is 0.04008341\n",
      "epoch: 1 step: 389, loss is 0.033303738\n",
      "epoch: 1 step: 390, loss is 0.031601846\n",
      "epoch: 1 step: 391, loss is 0.033498943\n",
      "epoch: 1 step: 392, loss is 0.030673206\n",
      "epoch: 1 step: 393, loss is 0.030825853\n",
      "epoch: 1 step: 394, loss is 0.022141516\n",
      "epoch: 1 step: 395, loss is 0.02874571\n",
      "epoch: 1 step: 396, loss is 0.035221756\n",
      "epoch: 1 step: 397, loss is 0.0315333\n",
      "epoch: 1 step: 398, loss is 0.035696328\n",
      "epoch: 1 step: 399, loss is 0.020762742\n",
      "epoch: 1 step: 400, loss is 0.044553578\n",
      "epoch: 1 step: 401, loss is 0.029594243\n",
      "epoch: 1 step: 402, loss is 0.0341444\n",
      "epoch: 1 step: 403, loss is 0.027816772\n",
      "epoch: 1 step: 404, loss is 0.02248311\n",
      "epoch: 1 step: 405, loss is 0.03275472\n",
      "epoch: 1 step: 406, loss is 0.035190284\n",
      "epoch: 1 step: 407, loss is 0.024740517\n",
      "epoch: 1 step: 408, loss is 0.034319222\n",
      "epoch: 1 step: 409, loss is 0.03994912\n",
      "epoch: 1 step: 410, loss is 0.028921127\n",
      "epoch: 1 step: 411, loss is 0.035388768\n",
      "epoch: 1 step: 412, loss is 0.04012084\n",
      "epoch: 1 step: 413, loss is 0.024246871\n",
      "epoch: 1 step: 414, loss is 0.030794144\n",
      "epoch: 1 step: 415, loss is 0.026198387\n",
      "epoch: 1 step: 416, loss is 0.026695311\n",
      "epoch: 1 step: 417, loss is 0.030354798\n",
      "epoch: 1 step: 418, loss is 0.029294193\n",
      "epoch: 1 step: 419, loss is 0.039960384\n",
      "epoch: 1 step: 420, loss is 0.038901806\n",
      "epoch: 1 step: 421, loss is 0.035025835\n",
      "epoch: 1 step: 422, loss is 0.027486145\n",
      "epoch: 1 step: 423, loss is 0.035101354\n",
      "epoch: 1 step: 424, loss is 0.025137603\n",
      "epoch: 1 step: 425, loss is 0.041053474\n",
      "epoch: 1 step: 426, loss is 0.027537763\n",
      "epoch: 1 step: 427, loss is 0.028621614\n",
      "epoch: 1 step: 428, loss is 0.037911654\n",
      "epoch: 1 step: 429, loss is 0.030413806\n",
      "epoch: 1 step: 430, loss is 0.021617651\n",
      "epoch: 1 step: 431, loss is 0.03167039\n",
      "epoch: 1 step: 432, loss is 0.025387287\n",
      "epoch: 1 step: 433, loss is 0.030073583\n",
      "epoch: 1 step: 434, loss is 0.009563148\n",
      "epoch: 1 step: 435, loss is 0.025397062\n",
      "epoch: 1 step: 436, loss is 0.031538486\n",
      "epoch: 1 step: 437, loss is 0.025952995\n",
      "epoch: 1 step: 438, loss is 0.03655809\n",
      "epoch: 1 step: 439, loss is 0.027862847\n",
      "epoch: 1 step: 440, loss is 0.026220977\n",
      "epoch: 1 step: 441, loss is 0.024624586\n",
      "epoch: 1 step: 442, loss is 0.04248649\n",
      "epoch: 1 step: 443, loss is 0.026848137\n",
      "epoch: 1 step: 444, loss is 0.031099558\n",
      "epoch: 1 step: 445, loss is 0.020614207\n",
      "epoch: 1 step: 446, loss is 0.035482347\n",
      "epoch: 1 step: 447, loss is 0.031095445\n",
      "epoch: 1 step: 448, loss is 0.030347168\n",
      "epoch: 1 step: 449, loss is 0.039218366\n",
      "epoch: 1 step: 450, loss is 0.034615815\n",
      "epoch: 1 step: 451, loss is 0.027913392\n",
      "epoch: 1 step: 452, loss is 0.038074434\n",
      "epoch: 1 step: 453, loss is 0.02777356\n",
      "epoch: 1 step: 454, loss is 0.02945739\n",
      "epoch: 1 step: 455, loss is 0.029494584\n",
      "epoch: 1 step: 456, loss is 0.029294193\n",
      "epoch: 1 step: 457, loss is 0.03867358\n",
      "epoch: 1 step: 458, loss is 0.031782866\n",
      "epoch: 1 step: 459, loss is 0.05782032\n",
      "epoch: 1 step: 460, loss is 0.022947252\n",
      "epoch: 1 step: 461, loss is 0.018244326\n",
      "epoch: 1 step: 462, loss is 0.030201256\n",
      "epoch: 1 step: 463, loss is 0.02046746\n",
      "epoch: 1 step: 464, loss is 0.07821721\n",
      "epoch: 1 step: 465, loss is 0.026402652\n",
      "epoch: 1 step: 466, loss is 0.030718803\n",
      "epoch: 1 step: 467, loss is 0.03821224\n",
      "epoch: 1 step: 468, loss is 0.023650587\n",
      "epoch: 1 step: 469, loss is 0.0304119\n",
      "epoch: 1 step: 470, loss is 0.03224039\n",
      "epoch: 1 step: 471, loss is 0.026535988\n",
      "epoch: 1 step: 472, loss is 0.031122744\n",
      "epoch: 1 step: 473, loss is 0.032865584\n",
      "epoch: 1 step: 474, loss is 0.021037817\n",
      "epoch: 1 step: 475, loss is 0.036026895\n",
      "epoch: 1 step: 476, loss is 0.0313012\n",
      "epoch: 1 step: 477, loss is 0.031492174\n",
      "epoch: 1 step: 478, loss is 0.03019911\n",
      "epoch: 1 step: 479, loss is 0.028522491\n",
      "epoch: 1 step: 480, loss is 0.029045045\n",
      "epoch: 1 step: 481, loss is 0.026530981\n",
      "epoch: 1 step: 482, loss is 0.038806736\n",
      "epoch: 1 step: 483, loss is -0.056132197\n",
      "epoch: 1 step: 484, loss is 0.031723022\n",
      "epoch: 1 step: 485, loss is 0.03142661\n",
      "epoch: 1 step: 486, loss is 0.028561175\n",
      "epoch: 1 step: 487, loss is 0.029230118\n",
      "epoch: 1 step: 488, loss is 0.029619038\n",
      "epoch: 1 step: 489, loss is 0.031838\n",
      "epoch: 1 step: 490, loss is 0.034635007\n",
      "epoch: 1 step: 491, loss is 0.024037302\n",
      "epoch: 1 step: 492, loss is 0.0288499\n",
      "epoch: 1 step: 493, loss is 0.02607447\n",
      "epoch: 1 step: 494, loss is 0.028304815\n",
      "epoch: 1 step: 495, loss is 0.049591005\n",
      "epoch: 1 step: 496, loss is 0.033110917\n",
      "epoch: 1 step: 497, loss is 0.031353295\n",
      "epoch: 1 step: 498, loss is 0.030267298\n",
      "epoch: 1 step: 499, loss is 0.030989468\n",
      "epoch: 1 step: 500, loss is 0.02421379\n",
      "epoch: 1 step: 501, loss is 0.028304577\n",
      "epoch: 1 step: 502, loss is 0.03187555\n",
      "epoch: 1 step: 503, loss is 0.024114132\n",
      "epoch: 1 step: 504, loss is 0.030649185\n",
      "epoch: 1 step: 505, loss is 0.02711916\n",
      "epoch: 1 step: 506, loss is 0.028695405\n",
      "epoch: 1 step: 507, loss is 0.040044546\n",
      "epoch: 1 step: 508, loss is -0.038006425\n",
      "epoch: 1 step: 509, loss is 0.028365076\n",
      "epoch: 1 step: 510, loss is 0.021337032\n",
      "epoch: 1 step: 511, loss is 0.033020794\n",
      "epoch: 1 step: 512, loss is 0.030632913\n",
      "epoch: 1 step: 513, loss is 0.03224939\n",
      "epoch: 1 step: 514, loss is 0.034098208\n",
      "epoch: 1 step: 515, loss is 0.03668636\n",
      "epoch: 1 step: 516, loss is 0.036586523\n",
      "epoch: 1 step: 517, loss is 0.024334252\n",
      "epoch: 1 step: 518, loss is 0.02776426\n",
      "epoch: 1 step: 519, loss is 0.032618046\n",
      "epoch: 1 step: 520, loss is 0.027648747\n",
      "epoch: 1 step: 521, loss is 0.032532156\n",
      "epoch: 1 step: 522, loss is 0.03423363\n",
      "epoch: 1 step: 523, loss is 0.022081137\n",
      "epoch: 1 step: 524, loss is 0.033465683\n",
      "epoch: 1 step: 525, loss is 0.02040863\n",
      "epoch: 1 step: 526, loss is 0.031832755\n",
      "epoch: 1 step: 527, loss is 0.026970148\n",
      "epoch: 1 step: 528, loss is 0.032883167\n",
      "epoch: 1 step: 529, loss is 0.028892815\n",
      "epoch: 1 step: 530, loss is 0.029885948\n",
      "epoch: 1 step: 531, loss is 0.034097254\n",
      "epoch: 1 step: 532, loss is 0.031588733\n",
      "epoch: 1 step: 533, loss is 0.032548428\n",
      "epoch: 1 step: 534, loss is 0.030682623\n",
      "epoch: 1 step: 535, loss is 0.029359639\n",
      "epoch: 1 step: 536, loss is 0.035459757\n",
      "epoch: 1 step: 537, loss is 0.036053956\n",
      "epoch: 1 step: 538, loss is 0.026762724\n",
      "epoch: 1 step: 539, loss is 0.032509327\n",
      "epoch: 1 step: 540, loss is 0.034629047\n",
      "epoch: 1 step: 541, loss is 0.03732896\n",
      "epoch: 1 step: 542, loss is 0.034759343\n",
      "epoch: 1 step: 543, loss is 0.035918236\n",
      "epoch: 1 step: 544, loss is 0.03547162\n",
      "epoch: 1 step: 545, loss is 0.041457474\n",
      "epoch: 1 step: 546, loss is 0.031381607\n",
      "epoch: 1 step: 547, loss is 0.03332001\n",
      "epoch: 1 step: 548, loss is 0.022472441\n",
      "epoch: 1 step: 549, loss is 0.035591602\n",
      "epoch: 1 step: 550, loss is 0.030206501\n",
      "epoch: 1 step: 551, loss is 0.026322544\n",
      "epoch: 1 step: 552, loss is 0.02274537\n",
      "epoch: 1 step: 553, loss is 0.032425582\n",
      "epoch: 1 step: 554, loss is 0.032135308\n",
      "epoch: 1 step: 555, loss is 0.0335536\n",
      "epoch: 1 step: 556, loss is 0.036318123\n",
      "epoch: 1 step: 557, loss is 0.039990902\n",
      "epoch: 1 step: 558, loss is 0.025342703\n",
      "epoch: 1 step: 559, loss is 0.034066737\n",
      "epoch: 1 step: 560, loss is 0.032422066\n",
      "epoch: 1 step: 561, loss is 0.029503345\n",
      "epoch: 1 step: 562, loss is 0.018238008\n",
      "epoch: 1 step: 563, loss is 0.035202086\n",
      "epoch: 1 step: 564, loss is 0.029641807\n",
      "epoch: 1 step: 565, loss is 0.035828292\n",
      "epoch: 1 step: 566, loss is 0.032693386\n",
      "epoch: 1 step: 567, loss is 0.029294193\n",
      "epoch: 1 step: 568, loss is 0.029263675\n",
      "epoch: 1 step: 569, loss is 0.03447789\n",
      "epoch: 1 step: 570, loss is 0.024574578\n",
      "epoch: 1 step: 571, loss is 0.029939175\n",
      "epoch: 1 step: 572, loss is 0.021802723\n",
      "epoch: 1 step: 573, loss is 0.03493625\n",
      "epoch: 1 step: 574, loss is 0.035475254\n",
      "epoch: 1 step: 575, loss is 0.028740108\n",
      "epoch: 1 step: 576, loss is 0.03518915\n",
      "epoch: 1 step: 577, loss is 0.029176474\n",
      "epoch: 1 step: 578, loss is 0.028198063\n",
      "epoch: 1 step: 579, loss is 0.027657032\n",
      "epoch: 1 step: 580, loss is 0.03241253\n",
      "epoch: 1 step: 581, loss is 0.03071189\n",
      "epoch: 1 step: 582, loss is 0.038942337\n",
      "epoch: 1 step: 583, loss is 0.030461848\n",
      "epoch: 1 step: 584, loss is 0.036067784\n",
      "epoch: 1 step: 585, loss is 0.034528494\n",
      "epoch: 1 step: 586, loss is 0.039472282\n",
      "epoch: 1 step: 587, loss is 0.027463615\n",
      "epoch: 1 step: 588, loss is 0.029908478\n",
      "epoch: 1 step: 589, loss is 0.028341472\n",
      "epoch: 1 step: 590, loss is 0.032147646\n",
      "epoch: 1 step: 591, loss is 0.028835773\n",
      "epoch: 1 step: 592, loss is 0.028441727\n",
      "epoch: 1 step: 593, loss is 0.04219961\n",
      "epoch: 1 step: 594, loss is 0.031373978\n",
      "epoch: 1 step: 595, loss is 0.032521427\n",
      "epoch: 1 step: 596, loss is 0.03248304\n",
      "epoch: 1 step: 597, loss is 0.034201622\n",
      "epoch: 1 step: 598, loss is 0.033178627\n",
      "epoch: 1 step: 599, loss is 0.08033222\n",
      "epoch: 1 step: 600, loss is 0.029922485\n",
      "epoch: 1 step: 601, loss is 0.033313036\n",
      "epoch: 1 step: 602, loss is 0.03388506\n",
      "epoch: 1 step: 603, loss is 0.027215242\n",
      "epoch: 1 step: 604, loss is 0.03880769\n",
      "epoch: 1 step: 605, loss is 0.030137002\n",
      "epoch: 1 step: 606, loss is 0.03220558\n",
      "epoch: 1 step: 607, loss is 0.03399712\n",
      "epoch: 1 step: 608, loss is 0.018309593\n",
      "epoch: 1 step: 609, loss is 0.03626442\n",
      "epoch: 1 step: 610, loss is 0.035264492\n",
      "epoch: 1 step: 611, loss is 0.03071171\n",
      "epoch: 1 step: 612, loss is 0.029402554\n",
      "epoch: 1 step: 613, loss is 0.033551276\n",
      "epoch: 1 step: 614, loss is 0.028051198\n",
      "epoch: 1 step: 615, loss is 0.034917653\n",
      "epoch: 1 step: 616, loss is 0.027218342\n",
      "epoch: 1 step: 617, loss is 0.03818828\n",
      "epoch: 1 step: 618, loss is 0.04984975\n",
      "epoch: 1 step: 619, loss is 0.030903518\n",
      "epoch: 1 step: 620, loss is 0.032492936\n",
      "epoch: 1 step: 621, loss is 0.044026196\n",
      "epoch: 1 step: 622, loss is 0.029740632\n",
      "epoch: 1 step: 623, loss is 0.029045522\n",
      "epoch: 1 step: 624, loss is 0.03973198\n",
      "epoch: 1 step: 625, loss is 0.028758287\n",
      "epoch: 1 step: 626, loss is 0.027939737\n",
      "epoch: 1 step: 627, loss is 0.038245022\n",
      "epoch: 1 step: 628, loss is 0.025689363\n",
      "epoch: 1 step: 629, loss is 0.032241583\n",
      "epoch: 1 step: 630, loss is 0.0455845\n",
      "epoch: 1 step: 631, loss is 0.040706217\n",
      "epoch: 1 step: 632, loss is 0.034304917\n",
      "epoch: 1 step: 633, loss is 0.07206088\n",
      "epoch: 1 step: 634, loss is 0.019776642\n",
      "epoch: 1 step: 635, loss is 0.031068146\n",
      "epoch: 1 step: 636, loss is -0.0010985136\n",
      "epoch: 1 step: 637, loss is 0.027058423\n",
      "epoch: 1 step: 638, loss is 0.038413823\n",
      "epoch: 1 step: 639, loss is 0.02239132\n",
      "epoch: 1 step: 640, loss is 0.052407026\n",
      "epoch: 1 step: 641, loss is 0.027594268\n",
      "epoch: 1 step: 642, loss is 0.033286095\n",
      "epoch: 1 step: 643, loss is 0.02557224\n",
      "epoch: 1 step: 644, loss is 0.023915946\n",
      "epoch: 1 step: 645, loss is 0.03538817\n",
      "epoch: 1 step: 646, loss is 0.03151959\n",
      "epoch: 1 step: 647, loss is 0.028930187\n",
      "epoch: 1 step: 648, loss is 0.03122425\n",
      "epoch: 1 step: 649, loss is 0.03379804\n",
      "epoch: 1 step: 650, loss is 0.041332304\n",
      "epoch: 1 step: 651, loss is 0.029515088\n",
      "epoch: 1 step: 652, loss is 0.022556603\n",
      "epoch: 1 step: 653, loss is -0.025454044\n",
      "epoch: 1 step: 654, loss is 0.048130095\n",
      "epoch: 1 step: 655, loss is 0.06430197\n",
      "epoch: 1 step: 656, loss is 0.06388968\n",
      "epoch: 1 step: 657, loss is 0.0579471\n",
      "epoch: 1 step: 658, loss is 0.032873154\n",
      "epoch: 1 step: 659, loss is 0.044733107\n",
      "epoch: 1 step: 660, loss is 0.040061057\n",
      "epoch: 1 step: 661, loss is 0.057740867\n",
      "epoch: 1 step: 662, loss is 0.03115511\n",
      "epoch: 1 step: 663, loss is 0.040634573\n",
      "epoch: 1 step: 664, loss is 0.045965195\n",
      "epoch: 1 step: 665, loss is 0.048968613\n",
      "epoch: 1 step: 666, loss is 0.040810645\n",
      "epoch: 1 step: 667, loss is 0.043477833\n",
      "epoch: 1 step: 668, loss is 0.0380916\n",
      "epoch: 1 step: 669, loss is 0.041424572\n",
      "epoch: 1 step: 670, loss is 0.05827266\n",
      "epoch: 1 step: 671, loss is 0.033879995\n",
      "epoch: 1 step: 672, loss is 0.051837683\n",
      "epoch: 1 step: 673, loss is 0.042909324\n",
      "epoch: 1 step: 674, loss is 0.047874272\n",
      "epoch: 1 step: 675, loss is 0.04749924\n",
      "epoch: 1 step: 676, loss is 0.053504944\n",
      "epoch: 1 step: 677, loss is 0.055844486\n",
      "epoch: 1 step: 678, loss is 0.047504485\n",
      "epoch: 1 step: 679, loss is 0.043665826\n",
      "epoch: 1 step: 680, loss is 0.051813662\n",
      "epoch: 1 step: 681, loss is 0.038830996\n",
      "epoch: 1 step: 682, loss is 0.03754741\n",
      "epoch: 1 step: 683, loss is 0.046615124\n",
      "epoch: 1 step: 684, loss is 0.045376122\n",
      "epoch: 1 step: 685, loss is 0.052087486\n",
      "epoch: 1 step: 686, loss is 0.040410817\n",
      "epoch: 1 step: 687, loss is -1.497962\n",
      "epoch: 1 step: 688, loss is 0.06087917\n",
      "epoch: 1 step: 689, loss is 0.060037196\n",
      "epoch: 1 step: 690, loss is 0.065017045\n",
      "epoch: 1 step: 691, loss is 0.048876286\n",
      "epoch: 1 step: 692, loss is 0.04876715\n",
      "epoch: 1 step: 693, loss is 0.051275432\n",
      "epoch: 1 step: 694, loss is 0.03894043\n",
      "epoch: 1 step: 695, loss is 0.06487805\n",
      "epoch: 1 step: 696, loss is 0.061147273\n",
      "epoch: 1 step: 697, loss is 0.07222772\n",
      "epoch: 1 step: 698, loss is 0.03448099\n",
      "epoch: 1 step: 699, loss is 0.06682062\n",
      "epoch: 1 step: 700, loss is 0.044110835\n",
      "epoch: 1 step: 701, loss is 0.06011057\n",
      "epoch: 1 step: 702, loss is 0.036475837\n",
      "epoch: 1 step: 703, loss is 0.04561466\n",
      "epoch: 1 step: 704, loss is 0.030538976\n",
      "epoch: 1 step: 705, loss is 0.050232112\n",
      "epoch: 1 step: 706, loss is 0.03333068\n",
      "epoch: 1 step: 707, loss is 0.047381878\n",
      "epoch: 1 step: 708, loss is 0.047857046\n",
      "epoch: 1 step: 709, loss is 0.045230567\n",
      "epoch: 1 step: 710, loss is 0.05626899\n",
      "epoch: 1 step: 711, loss is 0.048356354\n",
      "epoch: 1 step: 712, loss is 0.06166625\n",
      "epoch: 1 step: 713, loss is 0.04476601\n",
      "epoch: 1 step: 714, loss is 0.036112487\n",
      "epoch: 1 step: 715, loss is 0.054368913\n",
      "epoch: 1 step: 716, loss is 0.04500532\n",
      "epoch: 1 step: 717, loss is 0.04748112\n",
      "epoch: 1 step: 718, loss is 0.043815553\n",
      "epoch: 1 step: 719, loss is 0.045165062\n",
      "epoch: 1 step: 720, loss is 0.05480331\n",
      "epoch: 1 step: 721, loss is 0.04644561\n",
      "epoch: 1 step: 722, loss is 0.047454655\n",
      "epoch: 1 step: 723, loss is 0.04202223\n",
      "epoch: 1 step: 724, loss is 0.054568708\n",
      "epoch: 1 step: 725, loss is 0.042125523\n",
      "epoch: 1 step: 726, loss is 0.04560685\n",
      "epoch: 1 step: 727, loss is 0.060090363\n",
      "epoch: 1 step: 728, loss is 0.04614401\n",
      "epoch: 1 step: 729, loss is 0.051973164\n",
      "epoch: 1 step: 730, loss is 0.058592796\n",
      "epoch: 1 step: 731, loss is 0.048021376\n",
      "epoch: 1 step: 732, loss is 0.067414105\n",
      "epoch: 1 step: 733, loss is 0.05646181\n",
      "epoch: 1 step: 734, loss is 0.053036988\n",
      "epoch: 1 step: 735, loss is 0.07286072\n",
      "epoch: 1 step: 736, loss is 0.04094839\n",
      "epoch: 1 step: 737, loss is 0.041615784\n",
      "epoch: 1 step: 738, loss is 0.04166037\n",
      "epoch: 1 step: 739, loss is 0.059414864\n",
      "epoch: 1 step: 740, loss is 0.0730713\n",
      "epoch: 1 step: 741, loss is 0.045617163\n",
      "epoch: 1 step: 742, loss is 0.05938226\n",
      "epoch: 1 step: 743, loss is 0.060275376\n",
      "epoch: 1 step: 744, loss is 0.04362154\n",
      "epoch: 1 step: 745, loss is 0.04794264\n",
      "epoch: 1 step: 746, loss is 0.06519431\n",
      "epoch: 1 step: 747, loss is 0.0675233\n",
      "epoch: 1 step: 748, loss is 0.063124955\n",
      "epoch: 1 step: 749, loss is 0.03888744\n",
      "epoch: 1 step: 750, loss is 0.05877465\n",
      "epoch: 1 step: 751, loss is 0.066456854\n",
      "epoch: 1 step: 752, loss is 0.0484128\n",
      "epoch: 1 step: 753, loss is 0.056080043\n",
      "epoch: 1 step: 754, loss is 0.0371657\n",
      "epoch: 1 step: 755, loss is 0.04749447\n",
      "epoch: 1 step: 756, loss is 0.046140373\n",
      "epoch: 1 step: 757, loss is 0.036510468\n",
      "epoch: 1 step: 758, loss is 0.061945617\n",
      "epoch: 1 step: 759, loss is 0.057095826\n",
      "epoch: 1 step: 760, loss is 0.043362617\n",
      "epoch: 1 step: 761, loss is 0.04315883\n",
      "epoch: 1 step: 762, loss is 0.039295375\n",
      "epoch: 1 step: 763, loss is 0.049538314\n",
      "epoch: 1 step: 764, loss is 0.06173104\n",
      "epoch: 1 step: 765, loss is 0.038974285\n",
      "epoch: 1 step: 766, loss is 0.037320316\n",
      "epoch: 1 step: 767, loss is 0.069886744\n",
      "epoch: 1 step: 768, loss is 0.06532049\n",
      "epoch: 1 step: 769, loss is 0.06644839\n",
      "epoch: 1 step: 770, loss is 0.043321848\n",
      "epoch: 1 step: 771, loss is 0.041721344\n",
      "epoch: 1 step: 772, loss is 0.033090115\n",
      "epoch: 1 step: 773, loss is 0.034116805\n",
      "epoch: 1 step: 774, loss is 0.06563312\n",
      "epoch: 1 step: 775, loss is 0.049146652\n",
      "epoch: 2 step: 1, loss is 0.037178755\n",
      "epoch: 2 step: 2, loss is 0.05243033\n",
      "epoch: 2 step: 3, loss is 0.060420573\n",
      "epoch: 2 step: 4, loss is 0.06247902\n",
      "epoch: 2 step: 5, loss is 0.036141574\n",
      "epoch: 2 step: 6, loss is 0.031592786\n",
      "epoch: 2 step: 7, loss is 0.05460787\n",
      "epoch: 2 step: 8, loss is 0.055903375\n",
      "epoch: 2 step: 9, loss is 0.057098567\n",
      "epoch: 2 step: 10, loss is 0.059682608\n",
      "epoch: 2 step: 11, loss is 0.0704239\n",
      "epoch: 2 step: 12, loss is 0.06443542\n",
      "epoch: 2 step: 13, loss is 0.049758196\n",
      "epoch: 2 step: 14, loss is 0.060040295\n",
      "epoch: 2 step: 15, loss is 0.0622949\n",
      "epoch: 2 step: 16, loss is 0.06551725\n",
      "epoch: 2 step: 17, loss is 0.066016674\n",
      "epoch: 2 step: 18, loss is 0.06833601\n",
      "epoch: 2 step: 19, loss is 0.07366037\n",
      "epoch: 2 step: 20, loss is 0.065828145\n",
      "epoch: 2 step: 21, loss is 0.05895239\n",
      "epoch: 2 step: 22, loss is 0.06657004\n",
      "epoch: 2 step: 23, loss is 0.06310445\n",
      "epoch: 2 step: 24, loss is 0.061428726\n",
      "epoch: 2 step: 25, loss is -0.0687114\n",
      "epoch: 2 step: 26, loss is 0.033387363\n",
      "epoch: 2 step: 27, loss is 0.036351025\n",
      "epoch: 2 step: 28, loss is 0.03986597\n",
      "epoch: 2 step: 29, loss is 0.03675109\n",
      "epoch: 2 step: 30, loss is 0.037939608\n",
      "epoch: 2 step: 31, loss is 0.031181991\n",
      "epoch: 2 step: 32, loss is 0.032437027\n",
      "epoch: 2 step: 33, loss is 0.030940592\n",
      "epoch: 2 step: 34, loss is 0.046533763\n",
      "epoch: 2 step: 35, loss is 0.049274027\n",
      "epoch: 2 step: 36, loss is 0.035866737\n",
      "epoch: 2 step: 37, loss is 0.041330755\n",
      "epoch: 2 step: 38, loss is 0.030740023\n",
      "epoch: 2 step: 39, loss is 0.038067102\n",
      "epoch: 2 step: 40, loss is 0.028403759\n",
      "epoch: 2 step: 41, loss is 0.03170538\n",
      "epoch: 2 step: 42, loss is 0.035254538\n",
      "epoch: 2 step: 43, loss is 0.038880587\n",
      "epoch: 2 step: 44, loss is 0.03470826\n",
      "epoch: 2 step: 45, loss is 0.034348965\n",
      "epoch: 2 step: 46, loss is 0.026623487\n",
      "epoch: 2 step: 47, loss is 0.02929622\n",
      "epoch: 2 step: 48, loss is 0.034174263\n",
      "epoch: 2 step: 49, loss is 0.03477353\n",
      "epoch: 2 step: 50, loss is 0.043263018\n",
      "epoch: 2 step: 51, loss is 0.028539479\n",
      "epoch: 2 step: 52, loss is 0.033673346\n",
      "epoch: 2 step: 53, loss is 0.03102976\n",
      "epoch: 2 step: 54, loss is 0.03409201\n",
      "epoch: 2 step: 55, loss is 0.038805783\n",
      "epoch: 2 step: 56, loss is 0.031561077\n",
      "epoch: 2 step: 57, loss is 0.029710352\n",
      "epoch: 2 step: 58, loss is 0.029801428\n",
      "epoch: 2 step: 59, loss is 0.056233406\n",
      "epoch: 2 step: 60, loss is 0.041162193\n",
      "epoch: 2 step: 61, loss is 0.050564587\n",
      "epoch: 2 step: 62, loss is 0.056396008\n",
      "epoch: 2 step: 63, loss is 0.045781136\n",
      "epoch: 2 step: 64, loss is 0.051341057\n",
      "epoch: 2 step: 65, loss is 0.07296252\n",
      "epoch: 2 step: 66, loss is 0.0741685\n",
      "epoch: 2 step: 67, loss is 0.057482064\n",
      "epoch: 2 step: 68, loss is 0.07270312\n",
      "epoch: 2 step: 69, loss is 0.06817824\n",
      "epoch: 2 step: 70, loss is 0.07812828\n",
      "epoch: 2 step: 71, loss is 0.10456562\n",
      "epoch: 2 step: 72, loss is 0.06802654\n",
      "epoch: 2 step: 73, loss is 0.07069087\n",
      "epoch: 2 step: 74, loss is 0.0711506\n",
      "epoch: 2 step: 75, loss is 0.07003623\n",
      "epoch: 2 step: 76, loss is 0.07085514\n",
      "epoch: 2 step: 77, loss is 0.06697744\n",
      "epoch: 2 step: 78, loss is 0.073015034\n",
      "epoch: 2 step: 79, loss is 0.07048792\n",
      "epoch: 2 step: 80, loss is 0.069175005\n",
      "epoch: 2 step: 81, loss is 0.05790335\n",
      "epoch: 2 step: 82, loss is 0.070135355\n",
      "epoch: 2 step: 83, loss is 0.08383876\n",
      "epoch: 2 step: 84, loss is 0.06436008\n",
      "epoch: 2 step: 85, loss is 0.07086116\n",
      "epoch: 2 step: 86, loss is 0.075178325\n",
      "epoch: 2 step: 87, loss is 0.060376465\n",
      "epoch: 2 step: 88, loss is 0.04687071\n",
      "epoch: 2 step: 89, loss is 0.06978959\n",
      "epoch: 2 step: 90, loss is 0.06377429\n",
      "epoch: 2 step: 91, loss is 0.073087215\n",
      "epoch: 2 step: 92, loss is 0.06565815\n",
      "epoch: 2 step: 93, loss is 0.062312603\n",
      "epoch: 2 step: 94, loss is 0.057571173\n",
      "epoch: 2 step: 95, loss is 0.068701744\n",
      "epoch: 2 step: 96, loss is 0.049627125\n",
      "epoch: 2 step: 97, loss is 0.07483238\n",
      "epoch: 2 step: 98, loss is 0.070342064\n",
      "epoch: 2 step: 99, loss is 0.07568866\n",
      "epoch: 2 step: 100, loss is 0.070483744\n",
      "epoch: 2 step: 101, loss is 0.07245231\n",
      "epoch: 2 step: 102, loss is 0.05808139\n",
      "epoch: 2 step: 103, loss is 0.07316661\n",
      "epoch: 2 step: 104, loss is 0.07363796\n",
      "epoch: 2 step: 105, loss is 0.065015316\n",
      "epoch: 2 step: 106, loss is 0.0764547\n",
      "epoch: 2 step: 107, loss is 0.069345295\n",
      "epoch: 2 step: 108, loss is 0.076747894\n",
      "epoch: 2 step: 109, loss is 0.05487895\n",
      "epoch: 2 step: 110, loss is 0.06677455\n",
      "epoch: 2 step: 111, loss is 0.063263714\n",
      "epoch: 2 step: 112, loss is 0.06265849\n",
      "epoch: 2 step: 113, loss is 0.06869465\n",
      "epoch: 2 step: 114, loss is 0.062654674\n",
      "epoch: 2 step: 115, loss is 0.071738064\n",
      "epoch: 2 step: 116, loss is 0.060008526\n",
      "epoch: 2 step: 117, loss is 0.063076556\n",
      "epoch: 2 step: 118, loss is 0.07272506\n",
      "epoch: 2 step: 119, loss is 0.073434174\n",
      "epoch: 2 step: 120, loss is 0.06787449\n",
      "epoch: 2 step: 121, loss is 0.071982205\n",
      "epoch: 2 step: 122, loss is 0.072110474\n",
      "epoch: 2 step: 123, loss is 0.06785804\n",
      "epoch: 2 step: 124, loss is 0.07032734\n",
      "epoch: 2 step: 125, loss is 0.07630801\n",
      "epoch: 2 step: 126, loss is 0.067628145\n",
      "epoch: 2 step: 127, loss is 0.062280178\n",
      "epoch: 2 step: 128, loss is 0.08361977\n",
      "epoch: 2 step: 129, loss is 0.07652503\n",
      "epoch: 2 step: 130, loss is 0.079330504\n",
      "epoch: 2 step: 131, loss is 0.04823798\n",
      "epoch: 2 step: 132, loss is 0.053771913\n",
      "epoch: 2 step: 133, loss is 0.053901136\n",
      "epoch: 2 step: 134, loss is 0.059911907\n",
      "epoch: 2 step: 135, loss is 0.06263715\n",
      "epoch: 2 step: 136, loss is 0.06328368\n",
      "epoch: 2 step: 137, loss is 0.06686658\n",
      "epoch: 2 step: 138, loss is 0.062101424\n",
      "epoch: 2 step: 139, loss is 0.059740365\n",
      "epoch: 2 step: 140, loss is 0.069631875\n",
      "epoch: 2 step: 141, loss is 0.07106477\n",
      "epoch: 2 step: 142, loss is 0.03877616\n",
      "epoch: 2 step: 143, loss is 0.044843197\n",
      "epoch: 2 step: 144, loss is 0.066685915\n",
      "epoch: 2 step: 145, loss is 0.03840524\n",
      "epoch: 2 step: 146, loss is 0.0844636\n",
      "epoch: 2 step: 147, loss is 0.05918145\n",
      "epoch: 2 step: 148, loss is 0.06999427\n",
      "epoch: 2 step: 149, loss is 0.0686183\n",
      "epoch: 2 step: 150, loss is 0.068893254\n",
      "epoch: 2 step: 151, loss is 0.061784506\n",
      "epoch: 2 step: 152, loss is 0.060052693\n",
      "epoch: 2 step: 153, loss is 0.06777078\n",
      "epoch: 2 step: 154, loss is 0.05860001\n",
      "epoch: 2 step: 155, loss is 0.059117794\n",
      "epoch: 2 step: 156, loss is 0.07273716\n",
      "epoch: 2 step: 157, loss is 0.054900885\n",
      "epoch: 2 step: 158, loss is 0.06022483\n",
      "epoch: 2 step: 159, loss is 0.05359888\n",
      "epoch: 2 step: 160, loss is 0.06968182\n",
      "epoch: 2 step: 161, loss is 0.07178193\n",
      "epoch: 2 step: 162, loss is 0.065650225\n",
      "epoch: 2 step: 163, loss is 0.067160845\n",
      "epoch: 2 step: 164, loss is 0.06751603\n",
      "epoch: 2 step: 165, loss is 0.072083175\n",
      "epoch: 2 step: 166, loss is 0.08856195\n",
      "epoch: 2 step: 167, loss is 0.07278603\n",
      "epoch: 2 step: 168, loss is 0.06268507\n",
      "epoch: 2 step: 169, loss is 0.08577484\n",
      "epoch: 2 step: 170, loss is 0.06295496\n",
      "epoch: 2 step: 171, loss is 0.06886846\n",
      "epoch: 2 step: 172, loss is 0.08561164\n",
      "epoch: 2 step: 173, loss is 0.07190508\n",
      "epoch: 2 step: 174, loss is 0.113578975\n",
      "epoch: 2 step: 175, loss is 0.06846434\n",
      "epoch: 2 step: 176, loss is 0.084611714\n",
      "epoch: 2 step: 177, loss is 0.08020854\n",
      "epoch: 2 step: 178, loss is 0.08930272\n",
      "epoch: 2 step: 179, loss is 0.08232957\n",
      "epoch: 2 step: 180, loss is 0.07775754\n",
      "epoch: 2 step: 181, loss is 0.0733521\n",
      "epoch: 2 step: 182, loss is 0.08043879\n",
      "epoch: 2 step: 183, loss is 0.06860453\n",
      "epoch: 2 step: 184, loss is 0.06402391\n",
      "epoch: 2 step: 185, loss is 0.0989607\n",
      "epoch: 2 step: 186, loss is 0.07857585\n",
      "epoch: 2 step: 187, loss is 0.08446282\n",
      "epoch: 2 step: 188, loss is 0.086622536\n",
      "epoch: 2 step: 189, loss is 0.073055506\n",
      "epoch: 2 step: 190, loss is 0.07598996\n",
      "epoch: 2 step: 191, loss is 0.07437879\n",
      "epoch: 2 step: 192, loss is 0.0651589\n",
      "epoch: 2 step: 193, loss is 0.08514935\n",
      "epoch: 2 step: 194, loss is 0.08135909\n",
      "epoch: 2 step: 195, loss is 0.059990704\n",
      "epoch: 2 step: 196, loss is 0.08605772\n",
      "epoch: 2 step: 197, loss is 0.074703515\n",
      "epoch: 2 step: 198, loss is 0.07533115\n",
      "epoch: 2 step: 199, loss is 0.08123845\n",
      "epoch: 2 step: 200, loss is 0.08487111\n",
      "epoch: 2 step: 201, loss is 0.09084278\n",
      "epoch: 2 step: 202, loss is 0.07770616\n",
      "epoch: 2 step: 203, loss is 0.07581043\n",
      "epoch: 2 step: 204, loss is 0.071069956\n",
      "epoch: 2 step: 205, loss is 0.08813715\n",
      "epoch: 2 step: 206, loss is 0.10574311\n",
      "epoch: 2 step: 207, loss is 0.0672369\n",
      "epoch: 2 step: 208, loss is 0.0722062\n",
      "epoch: 2 step: 209, loss is 0.094573736\n",
      "epoch: 2 step: 210, loss is 0.096521795\n",
      "epoch: 2 step: 211, loss is -0.090010166\n",
      "epoch: 2 step: 212, loss is 0.06270808\n",
      "epoch: 2 step: 213, loss is 0.06576538\n",
      "epoch: 2 step: 214, loss is 0.059054434\n",
      "epoch: 2 step: 215, loss is 0.060751915\n",
      "epoch: 2 step: 216, loss is 0.051653862\n",
      "epoch: 2 step: 217, loss is 0.06956178\n",
      "epoch: 2 step: 218, loss is 0.0703122\n",
      "epoch: 2 step: 219, loss is 0.059410095\n",
      "epoch: 2 step: 220, loss is 0.069843054\n",
      "epoch: 2 step: 221, loss is 0.053672314\n",
      "epoch: 2 step: 222, loss is 0.068924904\n",
      "epoch: 2 step: 223, loss is 0.058785737\n",
      "epoch: 2 step: 224, loss is 0.05889505\n",
      "epoch: 2 step: 225, loss is 0.07321006\n",
      "epoch: 2 step: 226, loss is 0.06891894\n",
      "epoch: 2 step: 227, loss is 0.069367886\n",
      "epoch: 2 step: 228, loss is 0.05819434\n",
      "epoch: 2 step: 229, loss is 0.06274217\n",
      "epoch: 2 step: 230, loss is 0.07211298\n",
      "epoch: 2 step: 231, loss is 0.050867856\n",
      "epoch: 2 step: 232, loss is 0.069334805\n",
      "epoch: 2 step: 233, loss is 0.050079882\n",
      "epoch: 2 step: 234, loss is 0.045461833\n",
      "epoch: 2 step: 235, loss is 0.06471324\n",
      "epoch: 2 step: 236, loss is 0.05350113\n",
      "epoch: 2 step: 237, loss is 0.07747382\n",
      "epoch: 2 step: 238, loss is 0.058742702\n",
      "epoch: 2 step: 239, loss is 0.055180073\n",
      "epoch: 2 step: 240, loss is 0.072677195\n",
      "epoch: 2 step: 241, loss is 0.054621518\n",
      "epoch: 2 step: 242, loss is 0.058148086\n",
      "epoch: 2 step: 243, loss is 0.055485785\n",
      "epoch: 2 step: 244, loss is 0.06867504\n",
      "epoch: 2 step: 245, loss is 0.0650323\n",
      "epoch: 2 step: 246, loss is 0.056991637\n",
      "epoch: 2 step: 247, loss is 0.069499254\n",
      "epoch: 2 step: 248, loss is 0.06905413\n",
      "epoch: 2 step: 249, loss is 0.06434244\n",
      "epoch: 2 step: 250, loss is 0.06668967\n",
      "epoch: 2 step: 251, loss is 0.05821085\n",
      "epoch: 2 step: 252, loss is 0.07513064\n",
      "epoch: 2 step: 253, loss is 0.070343494\n",
      "epoch: 2 step: 254, loss is 0.06549406\n",
      "epoch: 2 step: 255, loss is 0.06178212\n",
      "epoch: 2 step: 256, loss is 0.045553863\n",
      "epoch: 2 step: 257, loss is 0.06685573\n",
      "epoch: 2 step: 258, loss is 0.076646864\n",
      "epoch: 2 step: 259, loss is 0.0695796\n",
      "epoch: 2 step: 260, loss is 0.083593845\n",
      "epoch: 2 step: 261, loss is 0.061382473\n",
      "epoch: 2 step: 262, loss is 0.06833553\n",
      "epoch: 2 step: 263, loss is 0.069975615\n",
      "epoch: 2 step: 264, loss is 0.072702885\n",
      "epoch: 2 step: 265, loss is 0.04991609\n",
      "epoch: 2 step: 266, loss is 0.05181271\n",
      "epoch: 2 step: 267, loss is 0.053117514\n",
      "epoch: 2 step: 268, loss is 0.064263344\n",
      "epoch: 2 step: 269, loss is 0.06626147\n",
      "epoch: 2 step: 270, loss is 0.06601733\n",
      "epoch: 2 step: 271, loss is 0.07599181\n",
      "epoch: 2 step: 272, loss is 0.07293701\n",
      "epoch: 2 step: 273, loss is 0.0539425\n",
      "epoch: 2 step: 274, loss is 0.062470734\n",
      "epoch: 2 step: 275, loss is 0.047261417\n",
      "epoch: 2 step: 276, loss is 0.070013106\n",
      "epoch: 2 step: 277, loss is 0.062532246\n",
      "epoch: 2 step: 278, loss is 0.043295145\n",
      "epoch: 2 step: 279, loss is 0.070245504\n",
      "epoch: 2 step: 280, loss is 0.07126969\n",
      "epoch: 2 step: 281, loss is 0.074857056\n",
      "epoch: 2 step: 282, loss is 0.06047249\n",
      "epoch: 2 step: 283, loss is 0.049555242\n",
      "epoch: 2 step: 284, loss is 0.08436149\n",
      "epoch: 2 step: 285, loss is 0.05841595\n",
      "epoch: 2 step: 286, loss is 0.071979344\n",
      "epoch: 2 step: 287, loss is 0.07758337\n",
      "epoch: 2 step: 288, loss is 0.07977694\n",
      "epoch: 2 step: 289, loss is 0.079058886\n",
      "epoch: 2 step: 290, loss is 0.079203665\n",
      "epoch: 2 step: 291, loss is 0.0901227\n",
      "epoch: 2 step: 292, loss is 0.076705754\n",
      "epoch: 2 step: 293, loss is 0.037935436\n",
      "epoch: 2 step: 294, loss is 0.101436794\n",
      "epoch: 2 step: 295, loss is 0.061359107\n",
      "epoch: 2 step: 296, loss is 0.091557324\n",
      "epoch: 2 step: 297, loss is 0.0866611\n",
      "epoch: 2 step: 298, loss is 0.100362\n",
      "epoch: 2 step: 299, loss is 0.088574946\n",
      "epoch: 2 step: 300, loss is 0.09435314\n",
      "epoch: 2 step: 301, loss is 0.088037014\n",
      "epoch: 2 step: 302, loss is 0.09182745\n",
      "epoch: 2 step: 303, loss is 0.08844185\n",
      "epoch: 2 step: 304, loss is 0.09020877\n",
      "epoch: 2 step: 305, loss is 0.08248472\n",
      "epoch: 2 step: 306, loss is 0.09994286\n",
      "epoch: 2 step: 307, loss is 0.0924899\n",
      "epoch: 2 step: 308, loss is 0.026643693\n",
      "epoch: 2 step: 309, loss is 0.08551532\n",
      "epoch: 2 step: 310, loss is 0.105011046\n",
      "epoch: 2 step: 311, loss is 0.17727345\n",
      "epoch: 2 step: 312, loss is 0.38824922\n",
      "epoch: 2 step: 313, loss is 0.079385936\n",
      "epoch: 2 step: 314, loss is 0.08457184\n",
      "epoch: 2 step: 315, loss is 0.07633752\n",
      "epoch: 2 step: 316, loss is 0.03410214\n",
      "epoch: 2 step: 317, loss is 0.08769125\n",
      "epoch: 2 step: 318, loss is 0.0714193\n",
      "epoch: 2 step: 319, loss is 0.08143425\n",
      "epoch: 2 step: 320, loss is 0.06627625\n",
      "epoch: 2 step: 321, loss is 0.08308911\n",
      "epoch: 2 step: 322, loss is 0.09166241\n",
      "epoch: 2 step: 323, loss is 0.09682804\n",
      "epoch: 2 step: 324, loss is 0.0951398\n",
      "epoch: 2 step: 325, loss is 0.07501316\n",
      "epoch: 2 step: 326, loss is 0.08499533\n",
      "epoch: 2 step: 327, loss is 0.08960652\n",
      "epoch: 2 step: 328, loss is 0.091811\n",
      "epoch: 2 step: 329, loss is 0.08337432\n",
      "epoch: 2 step: 330, loss is 0.09682256\n",
      "epoch: 2 step: 331, loss is 0.08982754\n",
      "epoch: 2 step: 332, loss is 0.088713944\n",
      "epoch: 2 step: 333, loss is 0.08867502\n",
      "epoch: 2 step: 334, loss is 0.08293396\n",
      "epoch: 2 step: 335, loss is 0.0912264\n",
      "epoch: 2 step: 336, loss is 0.07329059\n",
      "epoch: 2 step: 337, loss is 0.08198875\n",
      "epoch: 2 step: 338, loss is 0.08102822\n",
      "epoch: 2 step: 339, loss is 0.090868294\n",
      "epoch: 2 step: 340, loss is 0.0819183\n",
      "epoch: 2 step: 341, loss is 0.0855155\n",
      "epoch: 2 step: 342, loss is 0.07632971\n",
      "epoch: 2 step: 343, loss is 0.081308365\n",
      "epoch: 2 step: 344, loss is 0.095570266\n",
      "epoch: 2 step: 345, loss is 0.098455906\n",
      "epoch: 2 step: 346, loss is 0.08719391\n",
      "epoch: 2 step: 347, loss is 0.07955766\n",
      "epoch: 2 step: 348, loss is 0.057177603\n",
      "epoch: 2 step: 349, loss is 0.07560235\n",
      "epoch: 2 step: 350, loss is 0.0677802\n",
      "epoch: 2 step: 351, loss is 0.07910675\n",
      "epoch: 2 step: 352, loss is 0.086523235\n",
      "epoch: 2 step: 353, loss is 0.06984919\n",
      "epoch: 2 step: 354, loss is 0.08897275\n",
      "epoch: 2 step: 355, loss is 0.083495855\n",
      "epoch: 2 step: 356, loss is 0.092219174\n",
      "epoch: 2 step: 357, loss is 0.08060193\n",
      "epoch: 2 step: 358, loss is 0.08959526\n",
      "epoch: 2 step: 359, loss is 0.057802677\n",
      "epoch: 2 step: 360, loss is 0.090957165\n",
      "epoch: 2 step: 361, loss is 0.08519387\n",
      "epoch: 2 step: 362, loss is 0.08614808\n",
      "epoch: 2 step: 363, loss is 0.093387306\n",
      "epoch: 2 step: 364, loss is 0.079013646\n",
      "epoch: 2 step: 365, loss is 0.055275142\n",
      "epoch: 2 step: 366, loss is 0.08749843\n",
      "epoch: 2 step: 367, loss is 0.09117025\n",
      "epoch: 2 step: 368, loss is 0.09541941\n",
      "epoch: 2 step: 369, loss is 0.07514143\n",
      "epoch: 2 step: 370, loss is 0.07205218\n",
      "epoch: 2 step: 371, loss is 0.08968419\n",
      "epoch: 2 step: 372, loss is 0.09641379\n",
      "epoch: 2 step: 373, loss is 0.08972412\n",
      "epoch: 2 step: 374, loss is 0.089569986\n",
      "epoch: 2 step: 375, loss is 0.09335685\n",
      "epoch: 2 step: 376, loss is 0.102511704\n",
      "epoch: 2 step: 377, loss is 0.078121126\n",
      "epoch: 2 step: 378, loss is 0.092231095\n",
      "epoch: 2 step: 379, loss is 0.09230012\n",
      "epoch: 2 step: 380, loss is 0.08918029\n",
      "epoch: 2 step: 381, loss is 0.082244396\n",
      "epoch: 2 step: 382, loss is 0.07399589\n",
      "epoch: 2 step: 383, loss is 0.09364581\n",
      "epoch: 2 step: 384, loss is 0.10399175\n",
      "epoch: 2 step: 385, loss is 0.08212596\n",
      "epoch: 2 step: 386, loss is 0.10003108\n",
      "epoch: 2 step: 387, loss is 0.083480656\n",
      "epoch: 2 step: 388, loss is 0.09776449\n",
      "epoch: 2 step: 389, loss is 0.098783016\n",
      "epoch: 2 step: 390, loss is 0.09122068\n",
      "epoch: 2 step: 391, loss is 0.094917834\n",
      "epoch: 2 step: 392, loss is 0.08539963\n",
      "epoch: 2 step: 393, loss is 0.08263749\n",
      "epoch: 2 step: 394, loss is 0.08846617\n",
      "epoch: 2 step: 395, loss is 0.08475423\n",
      "epoch: 2 step: 396, loss is 0.07189864\n",
      "epoch: 2 step: 397, loss is 0.0710578\n",
      "epoch: 2 step: 398, loss is 0.078841865\n",
      "epoch: 2 step: 399, loss is 0.075830996\n",
      "epoch: 2 step: 400, loss is 0.06400806\n",
      "epoch: 2 step: 401, loss is 0.02965641\n",
      "epoch: 2 step: 402, loss is 0.07256144\n",
      "epoch: 2 step: 403, loss is 0.07633108\n",
      "epoch: 2 step: 404, loss is 0.06415719\n",
      "epoch: 2 step: 405, loss is 0.07136631\n",
      "epoch: 2 step: 406, loss is 0.07265878\n",
      "epoch: 2 step: 407, loss is 0.07376605\n",
      "epoch: 2 step: 408, loss is 0.07626891\n",
      "epoch: 2 step: 409, loss is 0.08157611\n",
      "epoch: 2 step: 410, loss is 0.06473559\n",
      "epoch: 2 step: 411, loss is 0.084914386\n",
      "epoch: 2 step: 412, loss is 0.073268354\n",
      "epoch: 2 step: 413, loss is 0.07301682\n",
      "epoch: 2 step: 414, loss is 0.05798149\n",
      "epoch: 2 step: 415, loss is 0.053382874\n",
      "epoch: 2 step: 416, loss is 0.06969792\n",
      "epoch: 2 step: 417, loss is 0.057787955\n",
      "epoch: 2 step: 418, loss is 0.06346673\n",
      "epoch: 2 step: 419, loss is 0.066782236\n",
      "epoch: 2 step: 420, loss is 0.05152607\n",
      "epoch: 2 step: 421, loss is 0.07775116\n",
      "epoch: 2 step: 422, loss is 0.07385248\n",
      "epoch: 2 step: 423, loss is 0.068516195\n",
      "epoch: 2 step: 424, loss is 0.13982606\n",
      "epoch: 2 step: 425, loss is 0.07266039\n",
      "epoch: 2 step: 426, loss is 0.09507483\n",
      "epoch: 2 step: 427, loss is 0.08297223\n",
      "epoch: 2 step: 428, loss is 0.085386276\n",
      "epoch: 2 step: 429, loss is 0.09961599\n",
      "epoch: 2 step: 430, loss is 0.08765823\n",
      "epoch: 2 step: 431, loss is 0.09345865\n",
      "epoch: 2 step: 432, loss is 0.10263723\n",
      "epoch: 2 step: 433, loss is 0.07983637\n",
      "epoch: 2 step: 434, loss is 0.080516815\n",
      "epoch: 2 step: 435, loss is 0.08855981\n",
      "epoch: 2 step: 436, loss is 0.085666\n",
      "epoch: 2 step: 437, loss is 0.083321035\n",
      "epoch: 2 step: 438, loss is 0.07564396\n",
      "epoch: 2 step: 439, loss is 0.084830105\n",
      "epoch: 2 step: 440, loss is 0.09390783\n",
      "epoch: 2 step: 441, loss is 0.08468193\n",
      "epoch: 2 step: 442, loss is 0.087916076\n",
      "epoch: 2 step: 443, loss is 0.084566176\n",
      "epoch: 2 step: 444, loss is 0.09194332\n",
      "epoch: 2 step: 445, loss is 0.09571266\n",
      "epoch: 2 step: 446, loss is 0.14033818\n",
      "epoch: 2 step: 447, loss is 0.07740694\n",
      "epoch: 2 step: 448, loss is 0.07358557\n",
      "epoch: 2 step: 449, loss is 0.06859207\n",
      "epoch: 2 step: 450, loss is 0.058202803\n",
      "epoch: 2 step: 451, loss is 0.077246666\n",
      "epoch: 2 step: 452, loss is 0.06872058\n",
      "epoch: 2 step: 453, loss is 0.08112675\n",
      "epoch: 2 step: 454, loss is 0.060923398\n",
      "epoch: 2 step: 455, loss is 0.06255555\n",
      "epoch: 2 step: 456, loss is 0.08295876\n",
      "epoch: 2 step: 457, loss is 0.06996995\n",
      "epoch: 2 step: 458, loss is 0.097144544\n",
      "epoch: 2 step: 459, loss is 0.0863722\n",
      "epoch: 2 step: 460, loss is 0.07918602\n",
      "epoch: 2 step: 461, loss is 0.10326475\n",
      "epoch: 2 step: 462, loss is 0.08781427\n",
      "epoch: 2 step: 463, loss is 0.03255272\n",
      "epoch: 2 step: 464, loss is 0.09788704\n",
      "epoch: 2 step: 465, loss is 0.091831446\n",
      "epoch: 2 step: 466, loss is 0.09755117\n",
      "epoch: 2 step: 467, loss is 0.10222775\n",
      "epoch: 2 step: 468, loss is 1.8722556\n",
      "epoch: 2 step: 469, loss is 0.09268218\n",
      "epoch: 2 step: 470, loss is 0.09258586\n",
      "epoch: 2 step: 471, loss is 0.097878635\n",
      "epoch: 2 step: 472, loss is 0.11116344\n",
      "epoch: 2 step: 473, loss is 0.088573396\n",
      "epoch: 2 step: 474, loss is 0.08066964\n",
      "epoch: 2 step: 475, loss is 0.08957648\n",
      "epoch: 2 step: 476, loss is 0.08142114\n",
      "epoch: 2 step: 477, loss is 0.10023099\n",
      "epoch: 2 step: 478, loss is 0.07794261\n",
      "epoch: 2 step: 479, loss is 0.09194964\n",
      "epoch: 2 step: 480, loss is 0.09328246\n",
      "epoch: 2 step: 481, loss is 0.09026784\n",
      "epoch: 2 step: 482, loss is 0.09180111\n",
      "epoch: 2 step: 483, loss is 0.09785777\n",
      "epoch: 2 step: 484, loss is 0.11291504\n",
      "epoch: 2 step: 485, loss is 0.1050846\n",
      "epoch: 2 step: 486, loss is 0.08930105\n",
      "epoch: 2 step: 487, loss is 0.09567571\n",
      "epoch: 2 step: 488, loss is 0.094076395\n",
      "epoch: 2 step: 489, loss is 0.09534931\n",
      "epoch: 2 step: 490, loss is 0.08853483\n",
      "epoch: 2 step: 491, loss is 0.10771078\n",
      "epoch: 2 step: 492, loss is 0.10596663\n",
      "epoch: 2 step: 493, loss is 0.07761526\n",
      "epoch: 2 step: 494, loss is 0.07627612\n",
      "epoch: 2 step: 495, loss is 0.10823679\n",
      "epoch: 2 step: 496, loss is 0.08849841\n",
      "epoch: 2 step: 497, loss is 0.095677376\n",
      "epoch: 2 step: 498, loss is 0.0968073\n",
      "epoch: 2 step: 499, loss is 0.09846878\n",
      "epoch: 2 step: 500, loss is 0.09380883\n",
      "epoch: 2 step: 501, loss is 0.071849644\n",
      "epoch: 2 step: 502, loss is 0.06654507\n",
      "epoch: 2 step: 503, loss is 0.09404302\n",
      "epoch: 2 step: 504, loss is 0.08339167\n",
      "epoch: 2 step: 505, loss is 0.1004172\n",
      "epoch: 2 step: 506, loss is 0.098320544\n",
      "epoch: 2 step: 507, loss is 0.0899207\n",
      "epoch: 2 step: 508, loss is 0.1029675\n",
      "epoch: 2 step: 509, loss is 0.01575023\n",
      "epoch: 2 step: 510, loss is 0.068960845\n",
      "epoch: 2 step: 511, loss is 0.10026485\n",
      "epoch: 2 step: 512, loss is 0.12301427\n",
      "epoch: 2 step: 513, loss is 0.10197824\n",
      "epoch: 2 step: 514, loss is 0.13041669\n",
      "epoch: 2 step: 515, loss is 0.09265691\n",
      "epoch: 2 step: 516, loss is 0.08152205\n",
      "epoch: 2 step: 517, loss is 0.093584955\n",
      "epoch: 2 step: 518, loss is 0.08931512\n",
      "epoch: 2 step: 519, loss is 0.09284848\n",
      "epoch: 2 step: 520, loss is 0.0782935\n",
      "epoch: 2 step: 521, loss is 0.09278947\n",
      "epoch: 2 step: 522, loss is 0.077094495\n",
      "epoch: 2 step: 523, loss is 0.103108704\n",
      "epoch: 2 step: 524, loss is 0.089467466\n",
      "epoch: 2 step: 525, loss is 0.06563902\n",
      "epoch: 2 step: 526, loss is 0.1012246\n",
      "epoch: 2 step: 527, loss is 0.12092823\n",
      "epoch: 2 step: 528, loss is 0.09128952\n",
      "epoch: 2 step: 529, loss is 0.078774154\n",
      "epoch: 2 step: 530, loss is 0.08653015\n",
      "epoch: 2 step: 531, loss is 0.06934279\n",
      "epoch: 2 step: 532, loss is 0.12910032\n",
      "epoch: 2 step: 533, loss is 0.08972573\n",
      "epoch: 2 step: 534, loss is 0.100617826\n",
      "epoch: 2 step: 535, loss is 0.09308523\n",
      "epoch: 2 step: 536, loss is 0.102959454\n",
      "epoch: 2 step: 537, loss is 0.097031295\n",
      "epoch: 2 step: 538, loss is 0.06722361\n",
      "epoch: 2 step: 539, loss is 0.09149003\n",
      "epoch: 2 step: 540, loss is 0.065546215\n",
      "epoch: 2 step: 541, loss is 0.08943194\n",
      "epoch: 2 step: 542, loss is 0.050691426\n",
      "epoch: 2 step: 543, loss is 0.10101527\n",
      "epoch: 2 step: 544, loss is 0.085758686\n",
      "epoch: 2 step: 545, loss is 0.083379745\n",
      "epoch: 2 step: 546, loss is 0.070438325\n",
      "epoch: 2 step: 547, loss is 0.07739705\n",
      "epoch: 2 step: 548, loss is 0.080494106\n",
      "epoch: 2 step: 549, loss is 0.08209878\n",
      "epoch: 2 step: 550, loss is 0.10406369\n",
      "epoch: 2 step: 551, loss is 0.06615001\n",
      "epoch: 2 step: 552, loss is 0.076871336\n",
      "epoch: 2 step: 553, loss is 0.0855543\n",
      "epoch: 2 step: 554, loss is 0.09335178\n",
      "epoch: 2 step: 555, loss is 0.0918625\n",
      "epoch: 2 step: 556, loss is 0.08226609\n",
      "epoch: 2 step: 557, loss is 0.09412938\n",
      "epoch: 2 step: 558, loss is 0.10894233\n",
      "epoch: 2 step: 559, loss is 0.075800896\n",
      "epoch: 2 step: 560, loss is 0.0897215\n",
      "epoch: 2 step: 561, loss is 0.09487671\n",
      "epoch: 2 step: 562, loss is 0.0814082\n",
      "epoch: 2 step: 563, loss is 0.08425957\n",
      "epoch: 2 step: 564, loss is -0.019377112\n",
      "epoch: 2 step: 565, loss is 0.09815711\n",
      "epoch: 2 step: 566, loss is 0.08340931\n",
      "epoch: 2 step: 567, loss is 0.08043623\n",
      "epoch: 2 step: 568, loss is 0.08544254\n",
      "epoch: 2 step: 569, loss is 0.06821132\n",
      "epoch: 2 step: 570, loss is 0.08918947\n",
      "epoch: 2 step: 571, loss is 0.08566928\n",
      "epoch: 2 step: 572, loss is 0.087679684\n",
      "epoch: 2 step: 573, loss is 0.08762008\n",
      "epoch: 2 step: 574, loss is 0.08650249\n",
      "epoch: 2 step: 575, loss is 0.0936988\n",
      "epoch: 2 step: 576, loss is 0.086239815\n",
      "epoch: 2 step: 577, loss is 0.08449966\n",
      "epoch: 2 step: 578, loss is 0.09071869\n",
      "epoch: 2 step: 579, loss is 0.07956451\n",
      "epoch: 2 step: 580, loss is 0.086815834\n",
      "epoch: 2 step: 581, loss is 0.08755106\n",
      "epoch: 2 step: 582, loss is 0.08557767\n",
      "epoch: 2 step: 583, loss is 0.07956904\n",
      "epoch: 2 step: 584, loss is 0.07794672\n",
      "epoch: 2 step: 585, loss is 0.083857715\n",
      "epoch: 2 step: 586, loss is 0.08684522\n",
      "epoch: 2 step: 587, loss is 0.089482546\n",
      "epoch: 2 step: 588, loss is 0.07912898\n",
      "epoch: 2 step: 589, loss is 0.07913351\n",
      "epoch: 2 step: 590, loss is 0.0698964\n",
      "epoch: 2 step: 591, loss is 0.07592124\n",
      "epoch: 2 step: 592, loss is 0.09070462\n",
      "epoch: 2 step: 593, loss is 0.065170586\n",
      "epoch: 2 step: 594, loss is 0.07789594\n",
      "epoch: 2 step: 595, loss is 0.084697425\n",
      "epoch: 2 step: 596, loss is 0.05642563\n",
      "epoch: 2 step: 597, loss is 0.07672024\n",
      "epoch: 2 step: 598, loss is 0.094148874\n",
      "epoch: 2 step: 599, loss is 0.08406925\n",
      "epoch: 2 step: 600, loss is 0.090164006\n",
      "epoch: 2 step: 601, loss is 0.092297554\n",
      "epoch: 2 step: 602, loss is 0.084306896\n",
      "epoch: 2 step: 603, loss is 0.08172244\n",
      "epoch: 2 step: 604, loss is 0.07677728\n",
      "epoch: 2 step: 605, loss is 0.07638234\n",
      "epoch: 2 step: 606, loss is 0.08914405\n",
      "epoch: 2 step: 607, loss is 0.08349532\n",
      "epoch: 2 step: 608, loss is 0.08941287\n",
      "epoch: 2 step: 609, loss is 0.08748627\n",
      "epoch: 2 step: 610, loss is 0.09753269\n",
      "epoch: 2 step: 611, loss is 0.08579999\n",
      "epoch: 2 step: 612, loss is 0.08250171\n",
      "epoch: 2 step: 613, loss is 0.08428508\n",
      "epoch: 2 step: 614, loss is 0.08069211\n",
      "epoch: 2 step: 615, loss is 0.08823079\n",
      "epoch: 2 step: 616, loss is 0.08570057\n",
      "epoch: 2 step: 617, loss is 0.08170128\n",
      "epoch: 2 step: 618, loss is 0.084928036\n",
      "epoch: 2 step: 619, loss is 0.08940774\n",
      "epoch: 2 step: 620, loss is 0.08788186\n",
      "epoch: 2 step: 621, loss is 0.08786148\n",
      "epoch: 2 step: 622, loss is 0.08896828\n",
      "epoch: 2 step: 623, loss is 0.09006375\n",
      "epoch: 2 step: 624, loss is 0.0919199\n",
      "epoch: 2 step: 625, loss is 0.08704901\n",
      "epoch: 2 step: 626, loss is 0.09142047\n",
      "epoch: 2 step: 627, loss is 0.07352656\n",
      "epoch: 2 step: 628, loss is 0.10232639\n",
      "epoch: 2 step: 629, loss is 0.08824402\n",
      "epoch: 2 step: 630, loss is 0.08073038\n",
      "epoch: 2 step: 631, loss is 0.08132553\n",
      "epoch: 2 step: 632, loss is 0.08800346\n",
      "epoch: 2 step: 633, loss is 0.084546804\n",
      "epoch: 2 step: 634, loss is 0.08005124\n",
      "epoch: 2 step: 635, loss is 0.087022126\n",
      "epoch: 2 step: 636, loss is 0.08703613\n",
      "epoch: 2 step: 637, loss is 0.09835315\n",
      "epoch: 2 step: 638, loss is 0.08736926\n",
      "epoch: 2 step: 639, loss is 0.08837801\n",
      "epoch: 2 step: 640, loss is 0.07461041\n",
      "epoch: 2 step: 641, loss is 0.10664958\n",
      "epoch: 2 step: 642, loss is 0.06765479\n",
      "epoch: 2 step: 643, loss is 0.08942509\n",
      "epoch: 2 step: 644, loss is 0.10650486\n",
      "epoch: 2 step: 645, loss is 0.07194215\n",
      "epoch: 2 step: 646, loss is 0.08186728\n",
      "epoch: 2 step: 647, loss is 0.082990944\n",
      "epoch: 2 step: 648, loss is 0.08387232\n",
      "epoch: 2 step: 649, loss is 0.08273029\n",
      "epoch: 2 step: 650, loss is 0.08856875\n",
      "epoch: 2 step: 651, loss is 0.083070755\n",
      "epoch: 2 step: 652, loss is 0.0811522\n",
      "epoch: 2 step: 653, loss is 0.09808165\n",
      "epoch: 2 step: 654, loss is 0.074269116\n",
      "epoch: 2 step: 655, loss is 0.08184594\n",
      "epoch: 2 step: 656, loss is 0.068086386\n",
      "epoch: 2 step: 657, loss is 0.07584596\n",
      "epoch: 2 step: 658, loss is 0.08909124\n",
      "epoch: 2 step: 659, loss is 0.24206918\n",
      "epoch: 2 step: 660, loss is 0.07520747\n",
      "epoch: 2 step: 661, loss is 0.07631564\n",
      "epoch: 2 step: 662, loss is 0.08199948\n",
      "epoch: 2 step: 663, loss is 0.09139514\n",
      "epoch: 2 step: 664, loss is 0.07172465\n",
      "epoch: 2 step: 665, loss is 0.0698244\n",
      "epoch: 2 step: 666, loss is 0.08247161\n",
      "epoch: 2 step: 667, loss is 0.07956821\n",
      "epoch: 2 step: 668, loss is 0.0821287\n",
      "epoch: 2 step: 669, loss is 0.0741083\n",
      "epoch: 2 step: 670, loss is 0.075115204\n",
      "epoch: 2 step: 671, loss is 0.12455958\n",
      "epoch: 2 step: 672, loss is 0.080295086\n",
      "epoch: 2 step: 673, loss is 0.08000165\n",
      "epoch: 2 step: 674, loss is 0.09690982\n",
      "epoch: 2 step: 675, loss is 0.08144957\n",
      "epoch: 2 step: 676, loss is 0.13671023\n",
      "epoch: 2 step: 677, loss is 0.07312077\n",
      "epoch: 2 step: 678, loss is 0.08104056\n",
      "epoch: 2 step: 679, loss is 0.086277306\n",
      "epoch: 2 step: 680, loss is 0.076476574\n",
      "epoch: 2 step: 681, loss is 0.09961486\n",
      "epoch: 2 step: 682, loss is 0.08407402\n",
      "epoch: 2 step: 683, loss is 0.08500165\n",
      "epoch: 2 step: 684, loss is 0.079963386\n",
      "epoch: 2 step: 685, loss is 0.084735096\n",
      "epoch: 2 step: 686, loss is 0.08188224\n",
      "epoch: 2 step: 687, loss is 0.09294289\n",
      "epoch: 2 step: 688, loss is 0.08307701\n",
      "epoch: 2 step: 689, loss is 0.086571515\n",
      "epoch: 2 step: 690, loss is 0.082333386\n",
      "epoch: 2 step: 691, loss is 0.07873529\n",
      "epoch: 2 step: 692, loss is 0.07864934\n",
      "epoch: 2 step: 693, loss is 0.08216554\n",
      "epoch: 2 step: 694, loss is 0.09573954\n",
      "epoch: 2 step: 695, loss is 0.082936585\n",
      "epoch: 2 step: 696, loss is 0.0845359\n",
      "epoch: 2 step: 697, loss is 0.07916379\n",
      "epoch: 2 step: 698, loss is 0.08460188\n",
      "epoch: 2 step: 699, loss is 0.08469695\n",
      "epoch: 2 step: 700, loss is 0.070314646\n",
      "epoch: 2 step: 701, loss is 0.078852355\n",
      "epoch: 2 step: 702, loss is 0.09246987\n",
      "epoch: 2 step: 703, loss is 0.07625896\n",
      "epoch: 2 step: 704, loss is 0.087852776\n",
      "epoch: 2 step: 705, loss is 0.08418852\n",
      "epoch: 2 step: 706, loss is 0.08843261\n",
      "epoch: 2 step: 707, loss is 0.08000612\n",
      "epoch: 2 step: 708, loss is 0.081738174\n",
      "epoch: 2 step: 709, loss is 0.09315711\n",
      "epoch: 2 step: 710, loss is 0.086951554\n",
      "epoch: 2 step: 711, loss is 0.08124429\n",
      "epoch: 2 step: 712, loss is 0.08101457\n",
      "epoch: 2 step: 713, loss is 0.07833594\n",
      "epoch: 2 step: 714, loss is 0.076857805\n",
      "epoch: 2 step: 715, loss is 0.09824371\n",
      "epoch: 2 step: 716, loss is 0.08622265\n",
      "epoch: 2 step: 717, loss is 0.08996314\n",
      "epoch: 2 step: 718, loss is 0.08360416\n",
      "epoch: 2 step: 719, loss is 0.08978391\n",
      "epoch: 2 step: 720, loss is 0.088512\n",
      "epoch: 2 step: 721, loss is 0.0856545\n",
      "epoch: 2 step: 722, loss is 0.07703704\n",
      "epoch: 2 step: 723, loss is 0.08935934\n",
      "epoch: 2 step: 724, loss is 0.083120644\n",
      "epoch: 2 step: 725, loss is 0.085779965\n",
      "epoch: 2 step: 726, loss is 0.08661789\n",
      "epoch: 2 step: 727, loss is 0.083205044\n",
      "epoch: 2 step: 728, loss is 0.08865261\n",
      "epoch: 2 step: 729, loss is 0.090183675\n",
      "epoch: 2 step: 730, loss is 0.094916046\n",
      "epoch: 2 step: 731, loss is 0.08115584\n",
      "epoch: 2 step: 732, loss is 0.090482056\n",
      "epoch: 2 step: 733, loss is 0.07421821\n",
      "epoch: 2 step: 734, loss is 0.08073926\n",
      "epoch: 2 step: 735, loss is 0.09254521\n",
      "epoch: 2 step: 736, loss is 0.095401764\n",
      "epoch: 2 step: 737, loss is 0.07902497\n",
      "epoch: 2 step: 738, loss is 0.09153813\n",
      "epoch: 2 step: 739, loss is 0.087314665\n",
      "epoch: 2 step: 740, loss is 0.085615456\n",
      "epoch: 2 step: 741, loss is 0.08573896\n",
      "epoch: 2 step: 742, loss is 0.09070104\n",
      "epoch: 2 step: 743, loss is 0.090019524\n",
      "epoch: 2 step: 744, loss is 0.08715516\n",
      "epoch: 2 step: 745, loss is -0.01697886\n",
      "epoch: 2 step: 746, loss is 0.09749323\n",
      "epoch: 2 step: 747, loss is 0.07777208\n",
      "epoch: 2 step: 748, loss is 0.092933\n",
      "epoch: 2 step: 749, loss is 0.09367037\n",
      "epoch: 2 step: 750, loss is 0.12577134\n",
      "epoch: 2 step: 751, loss is 0.08558172\n",
      "epoch: 2 step: 752, loss is 0.10237026\n",
      "epoch: 2 step: 753, loss is 0.089708626\n",
      "epoch: 2 step: 754, loss is 0.0905903\n",
      "epoch: 2 step: 755, loss is 0.087859154\n",
      "epoch: 2 step: 756, loss is 0.07330257\n",
      "epoch: 2 step: 757, loss is 0.06767982\n",
      "epoch: 2 step: 758, loss is 0.0788793\n",
      "epoch: 2 step: 759, loss is 0.09220171\n",
      "epoch: 2 step: 760, loss is 0.08273554\n",
      "epoch: 2 step: 761, loss is 0.06991863\n",
      "epoch: 2 step: 762, loss is 0.09099549\n",
      "epoch: 2 step: 763, loss is 0.08808452\n",
      "epoch: 2 step: 764, loss is 0.08369416\n",
      "epoch: 2 step: 765, loss is 0.09433776\n",
      "epoch: 2 step: 766, loss is 0.07945943\n",
      "epoch: 2 step: 767, loss is 0.077329636\n",
      "epoch: 2 step: 768, loss is 0.08903289\n",
      "epoch: 2 step: 769, loss is 0.08642125\n",
      "epoch: 2 step: 770, loss is 0.09454316\n",
      "epoch: 2 step: 771, loss is 0.08273572\n",
      "epoch: 2 step: 772, loss is 0.086749375\n",
      "epoch: 2 step: 773, loss is 0.10195112\n",
      "epoch: 2 step: 774, loss is 0.0886485\n",
      "epoch: 2 step: 775, loss is 0.10672325\n",
      "epoch: 3 step: 1, loss is 0.09301901\n",
      "epoch: 3 step: 2, loss is 0.080108166\n",
      "epoch: 3 step: 3, loss is 0.1010437\n",
      "epoch: 3 step: 4, loss is 0.089231074\n",
      "epoch: 3 step: 5, loss is 0.09201485\n",
      "epoch: 3 step: 6, loss is 0.09059572\n",
      "epoch: 3 step: 7, loss is 0.08764869\n",
      "epoch: 3 step: 8, loss is 0.08243227\n",
      "epoch: 3 step: 9, loss is 0.084371865\n",
      "epoch: 3 step: 10, loss is 0.08929688\n",
      "epoch: 3 step: 11, loss is 0.09134054\n",
      "epoch: 3 step: 12, loss is 0.082303464\n",
      "epoch: 3 step: 13, loss is 0.08700675\n",
      "epoch: 3 step: 14, loss is 0.08332378\n",
      "epoch: 3 step: 15, loss is 0.08603269\n",
      "epoch: 3 step: 16, loss is 0.10325104\n",
      "epoch: 3 step: 17, loss is 0.085743666\n",
      "epoch: 3 step: 18, loss is 0.07366133\n",
      "epoch: 3 step: 19, loss is 0.0843454\n",
      "epoch: 3 step: 20, loss is 0.08926052\n",
      "epoch: 3 step: 21, loss is 0.10030031\n",
      "epoch: 3 step: 22, loss is 0.07761532\n",
      "epoch: 3 step: 23, loss is 0.08385944\n",
      "epoch: 3 step: 24, loss is 0.08315164\n",
      "epoch: 3 step: 25, loss is 0.53335893\n",
      "epoch: 3 step: 26, loss is 0.043254852\n",
      "epoch: 3 step: 27, loss is 0.08918208\n",
      "epoch: 3 step: 28, loss is 0.08547479\n",
      "epoch: 3 step: 29, loss is 0.09281987\n",
      "epoch: 3 step: 30, loss is 0.0822652\n",
      "epoch: 3 step: 31, loss is 0.08971238\n",
      "epoch: 3 step: 32, loss is -0.023648381\n",
      "epoch: 3 step: 33, loss is 0.0919373\n",
      "epoch: 3 step: 34, loss is 0.07162833\n",
      "epoch: 3 step: 35, loss is 0.068170846\n",
      "epoch: 3 step: 36, loss is 0.08096218\n",
      "epoch: 3 step: 37, loss is 0.080782235\n",
      "epoch: 3 step: 38, loss is 0.092053115\n",
      "epoch: 3 step: 39, loss is 0.076476574\n",
      "epoch: 3 step: 40, loss is 0.06981403\n",
      "epoch: 3 step: 41, loss is 0.08874273\n",
      "epoch: 3 step: 42, loss is 0.07043141\n",
      "epoch: 3 step: 43, loss is 0.086342335\n",
      "epoch: 3 step: 44, loss is 0.07141781\n",
      "epoch: 3 step: 45, loss is 0.08970642\n",
      "epoch: 3 step: 46, loss is 0.07752198\n",
      "epoch: 3 step: 47, loss is 0.087688744\n",
      "epoch: 3 step: 48, loss is 0.08572096\n",
      "epoch: 3 step: 49, loss is 0.0883773\n",
      "epoch: 3 step: 50, loss is 0.07817793\n",
      "epoch: 3 step: 51, loss is 0.09492856\n",
      "epoch: 3 step: 52, loss is 0.089280605\n",
      "epoch: 3 step: 53, loss is -0.11309111\n",
      "epoch: 3 step: 54, loss is 0.0835076\n",
      "epoch: 3 step: 55, loss is 0.07331467\n",
      "epoch: 3 step: 56, loss is 0.08281487\n",
      "epoch: 3 step: 57, loss is 0.08715153\n",
      "epoch: 3 step: 58, loss is 0.07642406\n",
      "epoch: 3 step: 59, loss is 0.08712739\n",
      "epoch: 3 step: 60, loss is 0.084177494\n",
      "epoch: 3 step: 61, loss is 0.08829218\n",
      "epoch: 3 step: 62, loss is 0.068069935\n",
      "epoch: 3 step: 63, loss is 0.08414602\n",
      "epoch: 3 step: 64, loss is 0.10009432\n",
      "epoch: 3 step: 65, loss is 0.06089592\n",
      "epoch: 3 step: 66, loss is 0.08557397\n",
      "epoch: 3 step: 67, loss is 0.0919525\n",
      "epoch: 3 step: 68, loss is 0.08669275\n",
      "epoch: 3 step: 69, loss is 0.08184236\n",
      "epoch: 3 step: 70, loss is 0.08461064\n",
      "epoch: 3 step: 71, loss is 0.06865531\n",
      "epoch: 3 step: 72, loss is 0.0865944\n",
      "epoch: 3 step: 73, loss is 0.08728266\n",
      "epoch: 3 step: 74, loss is 0.08149797\n",
      "epoch: 3 step: 75, loss is 0.08238059\n",
      "epoch: 3 step: 76, loss is 0.08561736\n",
      "epoch: 3 step: 77, loss is 0.02684784\n",
      "epoch: 3 step: 78, loss is 0.086832225\n",
      "epoch: 3 step: 79, loss is 0.09119272\n",
      "epoch: 3 step: 80, loss is 0.09654254\n",
      "epoch: 3 step: 81, loss is 0.08687496\n",
      "epoch: 3 step: 82, loss is 0.09924489\n",
      "epoch: 3 step: 83, loss is 0.09517771\n",
      "epoch: 3 step: 84, loss is 0.09393674\n",
      "epoch: 3 step: 85, loss is 0.09256649\n",
      "epoch: 3 step: 86, loss is 0.08623546\n",
      "epoch: 3 step: 87, loss is 0.09374124\n",
      "epoch: 3 step: 88, loss is 0.09581596\n",
      "epoch: 3 step: 89, loss is 0.089931786\n",
      "epoch: 3 step: 90, loss is 0.10446757\n",
      "epoch: 3 step: 91, loss is 0.08751124\n",
      "epoch: 3 step: 92, loss is 0.080987215\n",
      "epoch: 3 step: 93, loss is 0.09152633\n",
      "epoch: 3 step: 94, loss is 0.087542295\n",
      "epoch: 3 step: 95, loss is 0.08675665\n",
      "epoch: 3 step: 96, loss is 0.08324474\n",
      "epoch: 3 step: 97, loss is 0.08758956\n",
      "epoch: 3 step: 98, loss is 0.09399849\n",
      "epoch: 3 step: 99, loss is 0.09459591\n",
      "epoch: 3 step: 100, loss is 0.07191008\n",
      "epoch: 3 step: 101, loss is 0.09543055\n",
      "epoch: 3 step: 102, loss is 0.083409786\n",
      "epoch: 3 step: 103, loss is 0.085811615\n",
      "epoch: 3 step: 104, loss is 0.09161544\n",
      "epoch: 3 step: 105, loss is 0.094441\n",
      "epoch: 3 step: 106, loss is 0.092359364\n",
      "epoch: 3 step: 107, loss is 0.1056869\n",
      "epoch: 3 step: 108, loss is 0.09902686\n",
      "epoch: 3 step: 109, loss is 0.07817024\n",
      "epoch: 3 step: 110, loss is 0.09624559\n",
      "epoch: 3 step: 111, loss is 0.09725839\n",
      "epoch: 3 step: 112, loss is 0.08588463\n",
      "epoch: 3 step: 113, loss is 0.099050224\n",
      "epoch: 3 step: 114, loss is 0.094772756\n",
      "epoch: 3 step: 115, loss is 0.09458017\n",
      "epoch: 3 step: 116, loss is 0.09029341\n",
      "epoch: 3 step: 117, loss is 0.08533496\n",
      "epoch: 3 step: 118, loss is 0.095499694\n",
      "epoch: 3 step: 119, loss is 0.09076452\n",
      "epoch: 3 step: 120, loss is 0.08029771\n",
      "epoch: 3 step: 121, loss is 0.09052938\n",
      "epoch: 3 step: 122, loss is 0.072104275\n",
      "epoch: 3 step: 123, loss is 0.08965975\n",
      "epoch: 3 step: 124, loss is 0.09275538\n",
      "epoch: 3 step: 125, loss is 0.09354001\n",
      "epoch: 3 step: 126, loss is 0.1004917\n",
      "epoch: 3 step: 127, loss is 0.088054895\n",
      "epoch: 3 step: 128, loss is 0.08676863\n",
      "epoch: 3 step: 129, loss is 0.09575027\n",
      "epoch: 3 step: 130, loss is 0.09008199\n",
      "epoch: 3 step: 131, loss is 0.08702761\n",
      "epoch: 3 step: 132, loss is 0.08826256\n",
      "epoch: 3 step: 133, loss is 0.06588006\n",
      "epoch: 3 step: 134, loss is 0.095137894\n",
      "epoch: 3 step: 135, loss is 0.085241795\n",
      "epoch: 3 step: 136, loss is 0.075687945\n",
      "epoch: 3 step: 137, loss is 0.08511919\n",
      "epoch: 3 step: 138, loss is 0.079990566\n",
      "epoch: 3 step: 139, loss is 0.07913226\n",
      "epoch: 3 step: 140, loss is 0.09439653\n",
      "epoch: 3 step: 141, loss is 0.078175485\n",
      "epoch: 3 step: 142, loss is 0.100779235\n",
      "epoch: 3 step: 143, loss is 0.074355066\n",
      "epoch: 3 step: 144, loss is 0.10980129\n",
      "epoch: 3 step: 145, loss is 0.09581137\n",
      "epoch: 3 step: 146, loss is 0.0820443\n",
      "epoch: 3 step: 147, loss is 0.08295447\n",
      "epoch: 3 step: 148, loss is 0.1042068\n",
      "epoch: 3 step: 149, loss is 0.09132081\n",
      "epoch: 3 step: 150, loss is -0.082730174\n",
      "epoch: 3 step: 151, loss is 0.09239936\n",
      "epoch: 3 step: 152, loss is 0.09225863\n",
      "epoch: 3 step: 153, loss is 0.095850945\n",
      "epoch: 3 step: 154, loss is 0.095814705\n",
      "epoch: 3 step: 155, loss is 0.06090665\n",
      "epoch: 3 step: 156, loss is 0.09336233\n",
      "epoch: 3 step: 157, loss is 0.077088356\n",
      "epoch: 3 step: 158, loss is 0.09471393\n",
      "epoch: 3 step: 159, loss is 0.08341068\n",
      "epoch: 3 step: 160, loss is 0.089396\n",
      "epoch: 3 step: 161, loss is 0.094515145\n",
      "epoch: 3 step: 162, loss is 0.09822774\n",
      "epoch: 3 step: 163, loss is 0.86884135\n",
      "epoch: 3 step: 164, loss is 0.08617854\n",
      "epoch: 3 step: 165, loss is 0.08447319\n",
      "epoch: 3 step: 166, loss is 0.069783986\n",
      "epoch: 3 step: 167, loss is 0.07574719\n",
      "epoch: 3 step: 168, loss is 0.12999445\n",
      "epoch: 3 step: 169, loss is 0.087331\n",
      "epoch: 3 step: 170, loss is 0.086420715\n",
      "epoch: 3 step: 171, loss is 0.08180815\n",
      "epoch: 3 step: 172, loss is 0.07900554\n",
      "epoch: 3 step: 173, loss is 0.0796662\n",
      "epoch: 3 step: 174, loss is 0.083455265\n",
      "epoch: 3 step: 175, loss is 0.08457118\n",
      "epoch: 3 step: 176, loss is 0.08474082\n",
      "epoch: 3 step: 177, loss is 0.0639289\n",
      "epoch: 3 step: 178, loss is 0.09370828\n",
      "epoch: 3 step: 179, loss is 0.08444357\n",
      "epoch: 3 step: 180, loss is 0.08998352\n",
      "epoch: 3 step: 181, loss is 0.08751422\n",
      "epoch: 3 step: 182, loss is 0.08942342\n",
      "epoch: 3 step: 183, loss is 0.08990055\n",
      "epoch: 3 step: 184, loss is 0.085829735\n",
      "epoch: 3 step: 185, loss is 0.09015632\n",
      "epoch: 3 step: 186, loss is 0.091805995\n",
      "epoch: 3 step: 187, loss is 0.07541698\n",
      "epoch: 3 step: 188, loss is 0.07762641\n",
      "epoch: 3 step: 189, loss is 0.095290005\n",
      "epoch: 3 step: 190, loss is 0.07738763\n",
      "epoch: 3 step: 191, loss is 0.095333576\n",
      "epoch: 3 step: 192, loss is 0.07881957\n",
      "epoch: 3 step: 193, loss is 0.08845705\n",
      "epoch: 3 step: 194, loss is 0.09349424\n",
      "epoch: 3 step: 195, loss is 0.08057952\n",
      "epoch: 3 step: 196, loss is 0.09001231\n",
      "epoch: 3 step: 197, loss is 0.071168244\n",
      "epoch: 3 step: 198, loss is -0.10110319\n",
      "epoch: 3 step: 199, loss is 0.081036985\n",
      "epoch: 3 step: 200, loss is 0.08302903\n",
      "epoch: 3 step: 201, loss is 0.083717346\n",
      "epoch: 3 step: 202, loss is 0.086149156\n",
      "epoch: 3 step: 203, loss is 0.08649713\n",
      "epoch: 3 step: 204, loss is 0.074883044\n",
      "epoch: 3 step: 205, loss is 0.089588165\n",
      "epoch: 3 step: 206, loss is 0.0715062\n",
      "epoch: 3 step: 207, loss is 0.081056595\n",
      "epoch: 3 step: 208, loss is 0.071196735\n",
      "epoch: 3 step: 209, loss is 0.07458264\n",
      "epoch: 3 step: 210, loss is 0.10708231\n",
      "epoch: 3 step: 211, loss is 0.08304411\n",
      "epoch: 3 step: 212, loss is 0.080860436\n",
      "epoch: 3 step: 213, loss is 0.07856995\n",
      "epoch: 3 step: 214, loss is 0.07122642\n",
      "epoch: 3 step: 215, loss is 0.077358425\n",
      "epoch: 3 step: 216, loss is 0.0759573\n",
      "epoch: 3 step: 217, loss is 0.07908398\n",
      "epoch: 3 step: 218, loss is 0.08134478\n",
      "epoch: 3 step: 219, loss is 0.06629944\n",
      "epoch: 3 step: 220, loss is 0.07873297\n",
      "epoch: 3 step: 221, loss is 0.07024288\n",
      "epoch: 3 step: 222, loss is 0.08654016\n",
      "epoch: 3 step: 223, loss is 0.088597536\n",
      "epoch: 3 step: 224, loss is 0.08067876\n",
      "epoch: 3 step: 225, loss is 0.08164078\n",
      "epoch: 3 step: 226, loss is 0.08741361\n",
      "epoch: 3 step: 227, loss is 0.085985124\n",
      "epoch: 3 step: 228, loss is 0.091362\n",
      "epoch: 3 step: 229, loss is 0.08281207\n",
      "epoch: 3 step: 230, loss is 0.07634956\n",
      "epoch: 3 step: 231, loss is 0.07081205\n",
      "epoch: 3 step: 232, loss is 0.07095569\n",
      "epoch: 3 step: 233, loss is 0.08480197\n",
      "epoch: 3 step: 234, loss is 0.07110113\n",
      "epoch: 3 step: 235, loss is 0.06714302\n",
      "epoch: 3 step: 236, loss is 0.08308506\n",
      "epoch: 3 step: 237, loss is 0.08252472\n",
      "epoch: 3 step: 238, loss is 0.082992494\n",
      "epoch: 3 step: 239, loss is 0.08338028\n",
      "epoch: 3 step: 240, loss is 0.10916144\n",
      "epoch: 3 step: 241, loss is 0.089089096\n",
      "epoch: 3 step: 242, loss is 0.08303708\n",
      "epoch: 3 step: 243, loss is 0.08250266\n",
      "epoch: 3 step: 244, loss is 0.0830614\n",
      "epoch: 3 step: 245, loss is 0.010935307\n",
      "epoch: 3 step: 246, loss is 0.09271163\n",
      "epoch: 3 step: 247, loss is 0.086714566\n",
      "epoch: 3 step: 248, loss is 0.089586556\n",
      "epoch: 3 step: 249, loss is 0.08660978\n",
      "epoch: 3 step: 250, loss is 0.10506314\n",
      "epoch: 3 step: 251, loss is 0.10363215\n",
      "epoch: 3 step: 252, loss is 0.088180244\n",
      "epoch: 3 step: 253, loss is 0.09313077\n",
      "epoch: 3 step: 254, loss is 0.10284704\n",
      "epoch: 3 step: 255, loss is 0.08296877\n",
      "epoch: 3 step: 256, loss is 0.088537455\n",
      "epoch: 3 step: 257, loss is 0.08582693\n",
      "epoch: 3 step: 258, loss is 0.083309114\n",
      "epoch: 3 step: 259, loss is 0.098427474\n",
      "epoch: 3 step: 260, loss is 0.09180361\n",
      "epoch: 3 step: 261, loss is 0.09015536\n",
      "epoch: 3 step: 262, loss is 0.098030806\n",
      "epoch: 3 step: 263, loss is 0.14657617\n",
      "epoch: 3 step: 264, loss is 0.07986665\n",
      "epoch: 3 step: 265, loss is 0.09610051\n",
      "epoch: 3 step: 266, loss is 0.09433937\n",
      "epoch: 3 step: 267, loss is 0.08796865\n",
      "epoch: 3 step: 268, loss is 0.08986515\n",
      "epoch: 3 step: 269, loss is 0.09018296\n",
      "epoch: 3 step: 270, loss is 0.090648174\n",
      "epoch: 3 step: 271, loss is 0.077423334\n",
      "epoch: 3 step: 272, loss is 0.09611893\n",
      "epoch: 3 step: 273, loss is 0.08805251\n",
      "epoch: 3 step: 274, loss is 0.27263528\n",
      "epoch: 3 step: 275, loss is 0.086687505\n",
      "epoch: 3 step: 276, loss is 0.09677267\n",
      "epoch: 3 step: 277, loss is 0.09355384\n",
      "epoch: 3 step: 278, loss is 0.121562004\n",
      "epoch: 3 step: 279, loss is 0.09485072\n",
      "epoch: 3 step: 280, loss is 0.07935715\n",
      "epoch: 3 step: 281, loss is 0.098545074\n",
      "epoch: 3 step: 282, loss is 0.098151684\n",
      "epoch: 3 step: 283, loss is 0.049684227\n",
      "epoch: 3 step: 284, loss is 0.09478146\n",
      "epoch: 3 step: 285, loss is 0.09651542\n",
      "epoch: 3 step: 286, loss is 0.0964759\n",
      "epoch: 3 step: 287, loss is 0.10137123\n",
      "epoch: 3 step: 288, loss is 0.09690374\n",
      "epoch: 3 step: 289, loss is 0.09319562\n",
      "epoch: 3 step: 290, loss is 0.09977418\n",
      "epoch: 3 step: 291, loss is 0.09781027\n",
      "epoch: 3 step: 292, loss is 0.09184694\n",
      "epoch: 3 step: 293, loss is 0.08790058\n",
      "epoch: 3 step: 294, loss is 0.09053373\n",
      "epoch: 3 step: 295, loss is 0.10058981\n",
      "epoch: 3 step: 296, loss is 0.09793645\n",
      "epoch: 3 step: 297, loss is 0.098834336\n",
      "epoch: 3 step: 298, loss is 0.091780424\n",
      "epoch: 3 step: 299, loss is 0.08499861\n",
      "epoch: 3 step: 300, loss is 0.09411317\n",
      "epoch: 3 step: 301, loss is 0.096458435\n",
      "epoch: 3 step: 302, loss is 0.10338116\n",
      "epoch: 3 step: 303, loss is 0.10227704\n",
      "epoch: 3 step: 304, loss is 0.08760762\n",
      "epoch: 3 step: 305, loss is 0.09942013\n",
      "epoch: 3 step: 306, loss is 0.094690084\n",
      "epoch: 3 step: 307, loss is 0.10450888\n",
      "epoch: 3 step: 308, loss is 0.095920146\n",
      "epoch: 3 step: 309, loss is 0.1010229\n",
      "epoch: 3 step: 310, loss is 0.09765476\n",
      "epoch: 3 step: 311, loss is 0.10123569\n",
      "epoch: 3 step: 312, loss is 0.09991884\n",
      "epoch: 3 step: 313, loss is 0.08615047\n",
      "epoch: 3 step: 314, loss is 0.09680128\n",
      "epoch: 3 step: 315, loss is 0.09851694\n",
      "epoch: 3 step: 316, loss is 0.079891205\n",
      "epoch: 3 step: 317, loss is 0.07120204\n",
      "epoch: 3 step: 318, loss is 0.08196288\n",
      "epoch: 3 step: 319, loss is 0.07594901\n",
      "epoch: 3 step: 320, loss is 0.08663744\n",
      "epoch: 3 step: 321, loss is 0.08113307\n",
      "epoch: 3 step: 322, loss is 0.09086102\n",
      "epoch: 3 step: 323, loss is 0.08852881\n",
      "epoch: 3 step: 324, loss is 0.08688998\n",
      "epoch: 3 step: 325, loss is 0.08921176\n",
      "epoch: 3 step: 326, loss is 0.082929075\n",
      "epoch: 3 step: 327, loss is 0.09027356\n",
      "epoch: 3 step: 328, loss is 0.085573256\n",
      "epoch: 3 step: 329, loss is 0.094884396\n",
      "epoch: 3 step: 330, loss is 0.07632822\n",
      "epoch: 3 step: 331, loss is 0.092493474\n",
      "epoch: 3 step: 332, loss is 0.081635475\n",
      "epoch: 3 step: 333, loss is 0.07436448\n",
      "epoch: 3 step: 334, loss is 0.08630991\n",
      "epoch: 3 step: 335, loss is 0.09099704\n",
      "epoch: 3 step: 336, loss is 0.08725792\n",
      "epoch: 3 step: 337, loss is 0.086754024\n",
      "epoch: 3 step: 338, loss is 0.08337408\n",
      "epoch: 3 step: 339, loss is 0.08885485\n",
      "epoch: 3 step: 340, loss is 0.08622861\n",
      "epoch: 3 step: 341, loss is 0.07627612\n",
      "epoch: 3 step: 342, loss is 0.08707899\n",
      "epoch: 3 step: 343, loss is 0.081264496\n",
      "epoch: 3 step: 344, loss is 0.0858078\n",
      "epoch: 3 step: 345, loss is 0.08360034\n",
      "epoch: 3 step: 346, loss is 0.09087402\n",
      "epoch: 3 step: 347, loss is 0.09163123\n",
      "epoch: 3 step: 348, loss is 0.09289169\n",
      "epoch: 3 step: 349, loss is 0.07634646\n",
      "epoch: 3 step: 350, loss is 0.09389883\n",
      "epoch: 3 step: 351, loss is 0.0944981\n",
      "epoch: 3 step: 352, loss is 0.07208377\n",
      "epoch: 3 step: 353, loss is 0.0910275\n",
      "epoch: 3 step: 354, loss is 0.090348184\n",
      "epoch: 3 step: 355, loss is 0.08126575\n",
      "epoch: 3 step: 356, loss is 0.07393694\n",
      "epoch: 3 step: 357, loss is 0.070402086\n",
      "epoch: 3 step: 358, loss is 0.069125235\n",
      "epoch: 3 step: 359, loss is 0.07527518\n",
      "epoch: 3 step: 360, loss is 0.0758912\n",
      "epoch: 3 step: 361, loss is 0.07126784\n",
      "epoch: 3 step: 362, loss is 0.06937075\n",
      "epoch: 3 step: 363, loss is 0.0706169\n",
      "epoch: 3 step: 364, loss is 0.06335729\n",
      "epoch: 3 step: 365, loss is 0.074668586\n",
      "epoch: 3 step: 366, loss is 0.066735566\n",
      "epoch: 3 step: 367, loss is 0.07816595\n",
      "epoch: 3 step: 368, loss is 0.2063387\n",
      "epoch: 3 step: 369, loss is 0.0888083\n",
      "epoch: 3 step: 370, loss is 0.085644305\n",
      "epoch: 3 step: 371, loss is 0.06864071\n",
      "epoch: 3 step: 372, loss is 0.062293828\n",
      "epoch: 3 step: 373, loss is 0.062971294\n",
      "epoch: 3 step: 374, loss is 0.059074342\n",
      "epoch: 3 step: 375, loss is 0.06812811\n",
      "epoch: 3 step: 376, loss is 0.07317495\n",
      "epoch: 3 step: 377, loss is 0.07169551\n",
      "epoch: 3 step: 378, loss is 0.08814645\n",
      "epoch: 3 step: 379, loss is 0.06635636\n",
      "epoch: 3 step: 380, loss is 0.06930089\n",
      "epoch: 3 step: 381, loss is 0.07351065\n",
      "epoch: 3 step: 382, loss is 0.06040877\n",
      "epoch: 3 step: 383, loss is 0.08174187\n",
      "epoch: 3 step: 384, loss is 0.06332964\n",
      "epoch: 3 step: 385, loss is 0.07111448\n",
      "epoch: 3 step: 386, loss is 0.0731833\n",
      "epoch: 3 step: 387, loss is 0.064236164\n",
      "epoch: 3 step: 388, loss is 0.08601236\n",
      "epoch: 3 step: 389, loss is 0.06023431\n",
      "epoch: 3 step: 390, loss is 0.07328957\n",
      "epoch: 3 step: 391, loss is 0.079613686\n",
      "epoch: 3 step: 392, loss is 0.0683313\n",
      "epoch: 3 step: 393, loss is 0.08411646\n",
      "epoch: 3 step: 394, loss is 0.08048719\n",
      "epoch: 3 step: 395, loss is 0.076601684\n",
      "epoch: 3 step: 396, loss is 0.057670772\n",
      "epoch: 3 step: 397, loss is 0.06638694\n",
      "epoch: 3 step: 398, loss is 0.07808256\n",
      "epoch: 3 step: 399, loss is 0.07384068\n",
      "epoch: 3 step: 400, loss is 0.071023524\n",
      "epoch: 3 step: 401, loss is 0.079556644\n",
      "epoch: 3 step: 402, loss is 0.081189096\n",
      "epoch: 3 step: 403, loss is 0.08032465\n",
      "epoch: 3 step: 404, loss is 0.06987113\n",
      "epoch: 3 step: 405, loss is 0.05526358\n",
      "epoch: 3 step: 406, loss is 0.081967294\n",
      "epoch: 3 step: 407, loss is 0.07813233\n",
      "epoch: 3 step: 408, loss is 0.08249956\n",
      "epoch: 3 step: 409, loss is 0.083164155\n",
      "epoch: 3 step: 410, loss is 0.08016688\n",
      "epoch: 3 step: 411, loss is 0.07976931\n",
      "epoch: 3 step: 412, loss is 0.07403833\n",
      "epoch: 3 step: 413, loss is 0.06321293\n",
      "epoch: 3 step: 414, loss is 0.07083911\n",
      "epoch: 3 step: 415, loss is 0.08579254\n",
      "epoch: 3 step: 416, loss is 0.06931496\n",
      "epoch: 3 step: 417, loss is 0.07919127\n",
      "epoch: 3 step: 418, loss is 0.07581586\n",
      "epoch: 3 step: 419, loss is 0.07074165\n",
      "epoch: 3 step: 420, loss is 0.07588768\n",
      "epoch: 3 step: 421, loss is 0.08001804\n",
      "epoch: 3 step: 422, loss is 0.07640332\n",
      "epoch: 3 step: 423, loss is 0.07025856\n",
      "epoch: 3 step: 424, loss is 0.071992874\n",
      "epoch: 3 step: 425, loss is 0.08544773\n",
      "epoch: 3 step: 426, loss is 0.079104185\n",
      "epoch: 3 step: 427, loss is 0.06220752\n",
      "epoch: 3 step: 428, loss is 0.07203573\n",
      "epoch: 3 step: 429, loss is 0.057389677\n",
      "epoch: 3 step: 430, loss is 0.070916176\n",
      "epoch: 3 step: 431, loss is 0.07018405\n",
      "epoch: 3 step: 432, loss is 0.076874495\n",
      "epoch: 3 step: 433, loss is 0.087843716\n",
      "epoch: 3 step: 434, loss is 0.062105\n",
      "epoch: 3 step: 435, loss is 0.09061617\n",
      "epoch: 3 step: 436, loss is 0.06913328\n",
      "epoch: 3 step: 437, loss is 0.07471776\n",
      "epoch: 3 step: 438, loss is 0.07880926\n",
      "epoch: 3 step: 439, loss is 0.068800926\n",
      "epoch: 3 step: 440, loss is 0.059304\n",
      "epoch: 3 step: 441, loss is 0.07539743\n",
      "epoch: 3 step: 442, loss is 0.06297368\n",
      "epoch: 3 step: 443, loss is 0.06476754\n",
      "epoch: 3 step: 444, loss is 0.05647546\n",
      "epoch: 3 step: 445, loss is 0.070875585\n",
      "epoch: 3 step: 446, loss is 0.07213497\n",
      "epoch: 3 step: 447, loss is 0.07909471\n",
      "epoch: 3 step: 448, loss is 0.07665926\n",
      "epoch: 3 step: 449, loss is 0.08334851\n",
      "epoch: 3 step: 450, loss is 0.07645768\n",
      "epoch: 3 step: 451, loss is 0.07138866\n",
      "epoch: 3 step: 452, loss is 0.07036036\n",
      "epoch: 3 step: 453, loss is 0.07127112\n",
      "epoch: 3 step: 454, loss is 0.077850044\n",
      "epoch: 3 step: 455, loss is 0.074306905\n",
      "epoch: 3 step: 456, loss is 0.0061668754\n",
      "epoch: 3 step: 457, loss is 0.07634956\n",
      "epoch: 3 step: 458, loss is 0.082861125\n",
      "epoch: 3 step: 459, loss is 0.076378345\n",
      "epoch: 3 step: 460, loss is 0.09038526\n",
      "epoch: 3 step: 461, loss is 0.0680719\n",
      "epoch: 3 step: 462, loss is 0.078149796\n",
      "epoch: 3 step: 463, loss is 0.09698552\n",
      "epoch: 3 step: 464, loss is 0.08720225\n",
      "epoch: 3 step: 465, loss is 0.08763379\n",
      "epoch: 3 step: 466, loss is 0.08213025\n",
      "epoch: 3 step: 467, loss is 0.095280886\n",
      "epoch: 3 step: 468, loss is 0.087286234\n",
      "epoch: 3 step: 469, loss is 0.08301115\n",
      "epoch: 3 step: 470, loss is 0.08722401\n",
      "epoch: 3 step: 471, loss is 0.090759695\n",
      "epoch: 3 step: 472, loss is 0.081471205\n",
      "epoch: 3 step: 473, loss is 0.090076745\n",
      "epoch: 3 step: 474, loss is 0.077946246\n",
      "epoch: 3 step: 475, loss is 0.074193\n",
      "epoch: 3 step: 476, loss is 0.08616233\n",
      "epoch: 3 step: 477, loss is 0.0857262\n",
      "epoch: 3 step: 478, loss is 0.08465546\n",
      "epoch: 3 step: 479, loss is 0.079036176\n",
      "epoch: 3 step: 480, loss is 0.08460832\n",
      "epoch: 3 step: 481, loss is 0.08789188\n",
      "epoch: 3 step: 482, loss is 0.07624149\n",
      "epoch: 3 step: 483, loss is 0.08787304\n",
      "epoch: 3 step: 484, loss is 0.06904501\n",
      "epoch: 3 step: 485, loss is 0.08012104\n",
      "epoch: 3 step: 486, loss is 0.086863995\n",
      "epoch: 3 step: 487, loss is 0.075415134\n",
      "epoch: 3 step: 488, loss is 0.09042984\n",
      "epoch: 3 step: 489, loss is 0.07523209\n",
      "epoch: 3 step: 490, loss is 0.08629775\n",
      "epoch: 3 step: 491, loss is 0.059159935\n",
      "epoch: 3 step: 492, loss is 0.06877571\n",
      "epoch: 3 step: 493, loss is 0.07176685\n",
      "epoch: 3 step: 494, loss is 0.6980541\n",
      "epoch: 3 step: 495, loss is 0.0721851\n",
      "epoch: 3 step: 496, loss is 0.07093793\n",
      "epoch: 3 step: 497, loss is 0.067074955\n",
      "epoch: 3 step: 498, loss is 0.07225418\n",
      "epoch: 3 step: 499, loss is 0.039790094\n",
      "epoch: 3 step: 500, loss is 0.06492907\n",
      "epoch: 3 step: 501, loss is 0.078297615\n",
      "epoch: 3 step: 502, loss is 0.07415414\n",
      "epoch: 3 step: 503, loss is 0.048900843\n",
      "epoch: 3 step: 504, loss is 0.06947297\n",
      "epoch: 3 step: 505, loss is 0.06142634\n",
      "epoch: 3 step: 506, loss is 0.03015405\n",
      "epoch: 3 step: 507, loss is 0.051407814\n",
      "epoch: 3 step: 508, loss is 0.0755083\n",
      "epoch: 3 step: 509, loss is 0.0564484\n",
      "epoch: 3 step: 510, loss is 0.06995076\n",
      "epoch: 3 step: 511, loss is 0.079572916\n",
      "epoch: 3 step: 512, loss is 0.05902928\n",
      "epoch: 3 step: 513, loss is 0.086585045\n",
      "epoch: 3 step: 514, loss is 0.041120827\n",
      "epoch: 3 step: 515, loss is 0.051157176\n",
      "epoch: 3 step: 516, loss is 0.057207763\n",
      "epoch: 3 step: 517, loss is 0.06237912\n",
      "epoch: 3 step: 518, loss is 0.053803682\n",
      "epoch: 3 step: 519, loss is 0.057866096\n",
      "epoch: 3 step: 520, loss is 0.05928749\n",
      "epoch: 3 step: 521, loss is 0.07564074\n",
      "epoch: 3 step: 522, loss is 0.061279774\n",
      "epoch: 3 step: 523, loss is 0.06831741\n",
      "epoch: 3 step: 524, loss is 0.053279877\n",
      "epoch: 3 step: 525, loss is 0.08745313\n",
      "epoch: 3 step: 526, loss is 0.05836886\n",
      "epoch: 3 step: 527, loss is 0.06753111\n",
      "epoch: 3 step: 528, loss is 0.07611817\n",
      "epoch: 3 step: 529, loss is 0.055939615\n",
      "epoch: 3 step: 530, loss is 0.08711189\n",
      "epoch: 3 step: 531, loss is 0.038604736\n",
      "epoch: 3 step: 532, loss is 0.06809288\n",
      "epoch: 3 step: 533, loss is 0.06275624\n",
      "epoch: 3 step: 534, loss is 0.06082064\n",
      "epoch: 3 step: 535, loss is 0.057630777\n",
      "epoch: 3 step: 536, loss is 0.067722976\n",
      "epoch: 3 step: 537, loss is 0.0745613\n",
      "epoch: 3 step: 538, loss is 0.048289776\n",
      "epoch: 3 step: 539, loss is 0.082068145\n",
      "epoch: 3 step: 540, loss is 0.08607912\n",
      "epoch: 3 step: 541, loss is 0.076269805\n",
      "epoch: 3 step: 542, loss is 0.06384063\n",
      "epoch: 3 step: 543, loss is 0.07515973\n",
      "epoch: 3 step: 544, loss is 0.06358987\n",
      "epoch: 3 step: 545, loss is 0.07871312\n",
      "epoch: 3 step: 546, loss is 0.08126044\n",
      "epoch: 3 step: 547, loss is 0.05503398\n",
      "epoch: 3 step: 548, loss is 0.050339162\n",
      "epoch: 3 step: 549, loss is 0.053866565\n",
      "epoch: 3 step: 550, loss is 0.07025546\n",
      "epoch: 3 step: 551, loss is 0.08358091\n",
      "epoch: 3 step: 552, loss is 0.07510632\n",
      "epoch: 3 step: 553, loss is 0.079552114\n",
      "epoch: 3 step: 554, loss is 0.07240027\n",
      "epoch: 3 step: 555, loss is 0.08109599\n",
      "epoch: 3 step: 556, loss is 0.0512349\n",
      "epoch: 3 step: 557, loss is 0.05205536\n",
      "epoch: 3 step: 558, loss is 0.13806838\n",
      "epoch: 3 step: 559, loss is 0.07925135\n",
      "epoch: 3 step: 560, loss is 0.07340288\n",
      "epoch: 3 step: 561, loss is 0.078098774\n",
      "epoch: 3 step: 562, loss is 0.083818436\n",
      "epoch: 3 step: 563, loss is 0.08442432\n",
      "epoch: 3 step: 564, loss is 0.081192315\n",
      "epoch: 3 step: 565, loss is 0.08613843\n",
      "epoch: 3 step: 566, loss is 0.08967763\n",
      "epoch: 3 step: 567, loss is 0.08814651\n",
      "epoch: 3 step: 568, loss is 0.09037095\n",
      "epoch: 3 step: 569, loss is 0.07876778\n",
      "epoch: 3 step: 570, loss is 0.07589334\n",
      "epoch: 3 step: 571, loss is 0.087445915\n",
      "epoch: 3 step: 572, loss is 0.076909065\n",
      "epoch: 3 step: 573, loss is 0.08270985\n",
      "epoch: 3 step: 574, loss is 0.09473133\n",
      "epoch: 3 step: 575, loss is 0.079535306\n",
      "epoch: 3 step: 576, loss is 0.07791406\n",
      "epoch: 3 step: 577, loss is 0.081024826\n",
      "epoch: 3 step: 578, loss is 0.06454682\n",
      "epoch: 3 step: 579, loss is 0.09644294\n",
      "epoch: 3 step: 580, loss is 0.08894974\n",
      "epoch: 3 step: 581, loss is 0.08068019\n",
      "epoch: 3 step: 582, loss is 0.08239633\n",
      "epoch: 3 step: 583, loss is 0.08058721\n",
      "epoch: 3 step: 584, loss is 0.08791143\n",
      "epoch: 3 step: 585, loss is 0.09235889\n",
      "epoch: 3 step: 586, loss is 0.09236097\n",
      "epoch: 3 step: 587, loss is 0.087747574\n",
      "epoch: 3 step: 588, loss is 0.08120966\n",
      "epoch: 3 step: 589, loss is 0.080770016\n",
      "epoch: 3 step: 590, loss is 0.07879895\n",
      "epoch: 3 step: 591, loss is 0.07773608\n",
      "epoch: 3 step: 592, loss is 0.08276391\n",
      "epoch: 3 step: 593, loss is 0.076750934\n",
      "epoch: 3 step: 594, loss is 0.06868124\n",
      "epoch: 3 step: 595, loss is 0.079209805\n",
      "epoch: 3 step: 596, loss is 0.088787735\n",
      "epoch: 3 step: 597, loss is 0.092493534\n",
      "epoch: 3 step: 598, loss is 0.07978463\n",
      "epoch: 3 step: 599, loss is 0.08158225\n",
      "epoch: 3 step: 600, loss is 0.08602923\n",
      "epoch: 3 step: 601, loss is 0.086882114\n",
      "epoch: 3 step: 602, loss is 0.09691554\n",
      "epoch: 3 step: 603, loss is 0.07595414\n",
      "epoch: 3 step: 604, loss is 0.079051316\n",
      "epoch: 3 step: 605, loss is 0.09112495\n",
      "epoch: 3 step: 606, loss is 0.08700019\n",
      "epoch: 3 step: 607, loss is 0.090534925\n",
      "epoch: 3 step: 608, loss is 0.07866138\n",
      "epoch: 3 step: 609, loss is 0.08399558\n",
      "epoch: 3 step: 610, loss is 0.079417884\n",
      "epoch: 3 step: 611, loss is 0.07053417\n",
      "epoch: 3 step: 612, loss is 0.07964611\n",
      "epoch: 3 step: 613, loss is 0.07991606\n",
      "epoch: 3 step: 614, loss is 0.08558351\n",
      "epoch: 3 step: 615, loss is 0.091753244\n",
      "epoch: 3 step: 616, loss is 0.086885154\n",
      "epoch: 3 step: 617, loss is 0.121525705\n",
      "epoch: 3 step: 618, loss is 0.0836696\n",
      "epoch: 3 step: 619, loss is 0.06541735\n",
      "epoch: 3 step: 620, loss is 0.082960665\n",
      "epoch: 3 step: 621, loss is 0.076711\n",
      "epoch: 3 step: 622, loss is 0.09619665\n",
      "epoch: 3 step: 623, loss is 0.07615715\n",
      "epoch: 3 step: 624, loss is 0.07014829\n",
      "epoch: 3 step: 625, loss is 0.09352881\n",
      "epoch: 3 step: 626, loss is 0.078078985\n",
      "epoch: 3 step: 627, loss is 0.09004611\n",
      "epoch: 3 step: 628, loss is 0.0776062\n",
      "epoch: 3 step: 629, loss is 0.07243943\n",
      "epoch: 3 step: 630, loss is 0.08281797\n",
      "epoch: 3 step: 631, loss is 0.08027989\n",
      "epoch: 3 step: 632, loss is 0.07681006\n",
      "epoch: 3 step: 633, loss is 0.072779655\n",
      "epoch: 3 step: 634, loss is 0.07688993\n",
      "epoch: 3 step: 635, loss is 0.06756401\n",
      "epoch: 3 step: 636, loss is 0.07202786\n",
      "epoch: 3 step: 637, loss is 0.080744505\n",
      "epoch: 3 step: 638, loss is 0.08380002\n",
      "epoch: 3 step: 639, loss is 0.07207519\n",
      "epoch: 3 step: 640, loss is 0.079868555\n",
      "epoch: 3 step: 641, loss is 0.07787734\n",
      "epoch: 3 step: 642, loss is 0.07865161\n",
      "epoch: 3 step: 643, loss is 0.067937374\n",
      "epoch: 3 step: 644, loss is 0.056512356\n",
      "epoch: 3 step: 645, loss is 0.07461405\n",
      "epoch: 3 step: 646, loss is 0.08910465\n",
      "epoch: 3 step: 647, loss is 0.068751514\n",
      "epoch: 3 step: 648, loss is 0.07543564\n",
      "epoch: 3 step: 649, loss is 0.08229393\n",
      "epoch: 3 step: 650, loss is 0.08544332\n",
      "epoch: 3 step: 651, loss is 0.08341217\n",
      "epoch: 3 step: 652, loss is 0.08555299\n",
      "epoch: 3 step: 653, loss is 0.06826335\n",
      "epoch: 3 step: 654, loss is 0.071112394\n",
      "epoch: 3 step: 655, loss is 0.09474975\n",
      "epoch: 3 step: 656, loss is 0.08039379\n",
      "epoch: 3 step: 657, loss is 0.070623696\n",
      "epoch: 3 step: 658, loss is 0.07161641\n",
      "epoch: 3 step: 659, loss is 0.07632542\n",
      "epoch: 3 step: 660, loss is 0.06752139\n",
      "epoch: 3 step: 661, loss is 0.074159324\n",
      "epoch: 3 step: 662, loss is 0.06344038\n",
      "epoch: 3 step: 663, loss is 0.052433968\n",
      "epoch: 3 step: 664, loss is 0.07617092\n",
      "epoch: 3 step: 665, loss is 0.08367616\n",
      "epoch: 3 step: 666, loss is 0.06231284\n",
      "epoch: 3 step: 667, loss is 0.07506436\n",
      "epoch: 3 step: 668, loss is 0.06029862\n",
      "epoch: 3 step: 669, loss is 0.07875115\n",
      "epoch: 3 step: 670, loss is 0.08634871\n",
      "epoch: 3 step: 671, loss is 0.07959515\n",
      "epoch: 3 step: 672, loss is 0.07849312\n",
      "epoch: 3 step: 673, loss is 0.061971664\n",
      "epoch: 3 step: 674, loss is 0.07335281\n",
      "epoch: 3 step: 675, loss is 0.085673034\n",
      "epoch: 3 step: 676, loss is 0.09122008\n",
      "epoch: 3 step: 677, loss is 0.06907314\n",
      "epoch: 3 step: 678, loss is 0.081745684\n",
      "epoch: 3 step: 679, loss is 0.08718115\n",
      "epoch: 3 step: 680, loss is 0.08710438\n",
      "epoch: 3 step: 681, loss is 0.07797474\n",
      "epoch: 3 step: 682, loss is 0.07741314\n",
      "epoch: 3 step: 683, loss is 0.085923254\n",
      "epoch: 3 step: 684, loss is 0.18858021\n",
      "epoch: 3 step: 685, loss is 0.049838483\n",
      "epoch: 3 step: 686, loss is 0.04041052\n",
      "epoch: 3 step: 687, loss is 0.050063908\n",
      "epoch: 3 step: 688, loss is 0.056160986\n",
      "epoch: 3 step: 689, loss is 0.049348116\n",
      "epoch: 3 step: 690, loss is 0.042385638\n",
      "epoch: 3 step: 691, loss is 0.05464512\n",
      "epoch: 3 step: 692, loss is 0.07057816\n",
      "epoch: 3 step: 693, loss is 0.043940604\n",
      "epoch: 3 step: 694, loss is 0.05401045\n",
      "epoch: 3 step: 695, loss is 0.07134819\n",
      "epoch: 3 step: 696, loss is 0.10556525\n",
      "epoch: 3 step: 697, loss is 0.047975242\n",
      "epoch: 3 step: 698, loss is 0.064184666\n",
      "epoch: 3 step: 699, loss is 0.05420178\n",
      "epoch: 3 step: 700, loss is 0.06419915\n",
      "epoch: 3 step: 701, loss is 0.054617226\n",
      "epoch: 3 step: 702, loss is 0.043821633\n",
      "epoch: 3 step: 703, loss is 0.054752827\n",
      "epoch: 3 step: 704, loss is 0.06161517\n",
      "epoch: 3 step: 705, loss is 0.05614215\n",
      "epoch: 3 step: 706, loss is 0.063750446\n",
      "epoch: 3 step: 707, loss is 0.039433777\n",
      "epoch: 3 step: 708, loss is 0.07848388\n",
      "epoch: 3 step: 709, loss is 0.06789017\n",
      "epoch: 3 step: 710, loss is 0.06560439\n",
      "epoch: 3 step: 711, loss is 0.048529923\n",
      "epoch: 3 step: 712, loss is 0.04676509\n",
      "epoch: 3 step: 713, loss is 0.050104678\n",
      "epoch: 3 step: 714, loss is 0.06255466\n",
      "epoch: 3 step: 715, loss is 0.07024485\n",
      "epoch: 3 step: 716, loss is 0.06666213\n",
      "epoch: 3 step: 717, loss is 0.044742584\n",
      "epoch: 3 step: 718, loss is 0.067302704\n",
      "epoch: 3 step: 719, loss is 0.07863027\n",
      "epoch: 3 step: 720, loss is 0.03034252\n",
      "epoch: 3 step: 721, loss is 0.05306077\n",
      "epoch: 3 step: 722, loss is 0.05104059\n",
      "epoch: 3 step: 723, loss is 0.053995073\n",
      "epoch: 3 step: 724, loss is 0.07932311\n",
      "epoch: 3 step: 725, loss is 0.06519383\n",
      "epoch: 3 step: 726, loss is 0.060351074\n",
      "epoch: 3 step: 727, loss is 0.06203574\n",
      "epoch: 3 step: 728, loss is 0.052595317\n",
      "epoch: 3 step: 729, loss is 0.05175978\n",
      "epoch: 3 step: 730, loss is 0.079230845\n",
      "epoch: 3 step: 731, loss is 0.043943405\n",
      "epoch: 3 step: 732, loss is 0.074640214\n",
      "epoch: 3 step: 733, loss is 0.053495884\n",
      "epoch: 3 step: 734, loss is 0.05772668\n",
      "epoch: 3 step: 735, loss is 0.068558455\n",
      "epoch: 3 step: 736, loss is 0.07865971\n",
      "epoch: 3 step: 737, loss is 0.07685596\n",
      "epoch: 3 step: 738, loss is 0.054535806\n",
      "epoch: 3 step: 739, loss is 0.0563367\n",
      "epoch: 3 step: 740, loss is 0.06239873\n",
      "epoch: 3 step: 741, loss is 0.041213095\n",
      "epoch: 3 step: 742, loss is 0.08817607\n",
      "epoch: 3 step: 743, loss is 0.04973823\n",
      "epoch: 3 step: 744, loss is 0.064442575\n",
      "epoch: 3 step: 745, loss is 0.07684153\n",
      "epoch: 3 step: 746, loss is 0.07387757\n",
      "epoch: 3 step: 747, loss is 0.06369227\n",
      "epoch: 3 step: 748, loss is 0.064017\n",
      "epoch: 3 step: 749, loss is 0.07204622\n",
      "epoch: 3 step: 750, loss is 0.049476147\n",
      "epoch: 3 step: 751, loss is 0.037115395\n",
      "epoch: 3 step: 752, loss is 0.070682704\n",
      "epoch: 3 step: 753, loss is 0.057675898\n",
      "epoch: 3 step: 754, loss is 0.07081312\n",
      "epoch: 3 step: 755, loss is 0.0710668\n",
      "epoch: 3 step: 756, loss is 0.06040269\n",
      "epoch: 3 step: 757, loss is 0.052377522\n",
      "epoch: 3 step: 758, loss is 0.06498885\n",
      "epoch: 3 step: 759, loss is 0.053448677\n",
      "epoch: 3 step: 760, loss is 0.030300915\n",
      "epoch: 3 step: 761, loss is 0.07268125\n",
      "epoch: 3 step: 762, loss is 0.06683755\n",
      "epoch: 3 step: 763, loss is 0.07593346\n",
      "epoch: 3 step: 764, loss is 0.068024635\n",
      "epoch: 3 step: 765, loss is 0.08017892\n",
      "epoch: 3 step: 766, loss is 0.08174181\n",
      "epoch: 3 step: 767, loss is 0.06944412\n",
      "epoch: 3 step: 768, loss is 0.066044986\n",
      "epoch: 3 step: 769, loss is 0.07750863\n",
      "epoch: 3 step: 770, loss is 0.073268235\n",
      "epoch: 3 step: 771, loss is 0.060051918\n",
      "epoch: 3 step: 772, loss is 0.058387578\n",
      "epoch: 3 step: 773, loss is 0.07382268\n",
      "epoch: 3 step: 774, loss is 0.060353816\n",
      "epoch: 3 step: 775, loss is 0.07200509\n",
      "epoch: 4 step: 1, loss is 0.08099753\n",
      "epoch: 4 step: 2, loss is 0.0687713\n",
      "epoch: 4 step: 3, loss is 0.07768941\n",
      "epoch: 4 step: 4, loss is 0.07175523\n",
      "epoch: 4 step: 5, loss is 0.07184219\n",
      "epoch: 4 step: 6, loss is 0.082760036\n",
      "epoch: 4 step: 7, loss is 0.09093255\n",
      "epoch: 4 step: 8, loss is 0.08149177\n",
      "epoch: 4 step: 9, loss is 0.071563005\n",
      "epoch: 4 step: 10, loss is 0.07679564\n",
      "epoch: 4 step: 11, loss is 0.08675361\n",
      "epoch: 4 step: 12, loss is 0.0685665\n",
      "epoch: 4 step: 13, loss is 0.0677163\n",
      "epoch: 4 step: 14, loss is 0.07864261\n",
      "epoch: 4 step: 15, loss is 0.08337784\n",
      "epoch: 4 step: 16, loss is 0.06980437\n",
      "epoch: 4 step: 17, loss is -0.11218512\n",
      "epoch: 4 step: 18, loss is 0.082747936\n",
      "epoch: 4 step: 19, loss is 0.08780259\n",
      "epoch: 4 step: 20, loss is 0.070433676\n",
      "epoch: 4 step: 21, loss is 0.08767772\n",
      "epoch: 4 step: 22, loss is -0.031421065\n",
      "epoch: 4 step: 23, loss is 0.0812211\n",
      "epoch: 4 step: 24, loss is 0.08253813\n",
      "epoch: 4 step: 25, loss is 0.09055787\n",
      "epoch: 4 step: 26, loss is 0.08790439\n",
      "epoch: 4 step: 27, loss is 0.07995772\n",
      "epoch: 4 step: 28, loss is 0.111209095\n",
      "epoch: 4 step: 29, loss is 0.07845974\n",
      "epoch: 4 step: 30, loss is 0.088730514\n",
      "epoch: 4 step: 31, loss is 0.09191513\n",
      "epoch: 4 step: 32, loss is 0.09016043\n",
      "epoch: 4 step: 33, loss is 0.0762589\n",
      "epoch: 4 step: 34, loss is 0.076397\n",
      "epoch: 4 step: 35, loss is 0.07401037\n",
      "epoch: 4 step: 36, loss is 0.088721514\n",
      "epoch: 4 step: 37, loss is 0.10760218\n",
      "epoch: 4 step: 38, loss is 0.08247203\n",
      "epoch: 4 step: 39, loss is 0.077227354\n",
      "epoch: 4 step: 40, loss is 0.088775456\n",
      "epoch: 4 step: 41, loss is 0.08230239\n",
      "epoch: 4 step: 42, loss is 0.09174913\n",
      "epoch: 4 step: 43, loss is 0.081320345\n",
      "epoch: 4 step: 44, loss is 0.111662805\n",
      "epoch: 4 step: 45, loss is 0.08325845\n",
      "epoch: 4 step: 46, loss is 0.085768044\n",
      "epoch: 4 step: 47, loss is 0.07583076\n",
      "epoch: 4 step: 48, loss is 0.08575302\n",
      "epoch: 4 step: 49, loss is 0.08937794\n",
      "epoch: 4 step: 50, loss is 0.08096105\n",
      "epoch: 4 step: 51, loss is 0.08466679\n",
      "epoch: 4 step: 52, loss is 0.08600885\n",
      "epoch: 4 step: 53, loss is -0.060450435\n",
      "epoch: 4 step: 54, loss is 0.06619102\n",
      "epoch: 4 step: 55, loss is 0.08885068\n",
      "epoch: 4 step: 56, loss is 0.0733642\n",
      "epoch: 4 step: 57, loss is 0.089185536\n",
      "epoch: 4 step: 58, loss is 0.08994055\n",
      "epoch: 4 step: 59, loss is 0.08438331\n",
      "epoch: 4 step: 60, loss is 0.07639104\n",
      "epoch: 4 step: 61, loss is 0.06406444\n",
      "epoch: 4 step: 62, loss is 0.072886765\n",
      "epoch: 4 step: 63, loss is 0.07601768\n",
      "epoch: 4 step: 64, loss is 0.06910008\n",
      "epoch: 4 step: 65, loss is 0.08139199\n",
      "epoch: 4 step: 66, loss is 0.07365441\n",
      "epoch: 4 step: 67, loss is 0.115336955\n",
      "epoch: 4 step: 68, loss is 0.07491374\n",
      "epoch: 4 step: 69, loss is 0.082467556\n",
      "epoch: 4 step: 70, loss is 0.07489729\n",
      "epoch: 4 step: 71, loss is 0.07852411\n",
      "epoch: 4 step: 72, loss is 0.07269734\n",
      "epoch: 4 step: 73, loss is 0.07320613\n",
      "epoch: 4 step: 74, loss is 0.084771335\n",
      "epoch: 4 step: 75, loss is 0.08400875\n",
      "epoch: 4 step: 76, loss is 0.08882767\n",
      "epoch: 4 step: 77, loss is 0.07109642\n",
      "epoch: 4 step: 78, loss is 0.06967813\n",
      "epoch: 4 step: 79, loss is 0.08473009\n",
      "epoch: 4 step: 80, loss is 0.09204656\n",
      "epoch: 4 step: 81, loss is 0.08690411\n",
      "epoch: 4 step: 82, loss is 0.06798178\n",
      "epoch: 4 step: 83, loss is 0.07454443\n",
      "epoch: 4 step: 84, loss is 0.09305018\n",
      "epoch: 4 step: 85, loss is 0.0806458\n",
      "epoch: 4 step: 86, loss is 0.08411759\n",
      "epoch: 4 step: 87, loss is 0.08791113\n",
      "epoch: 4 step: 88, loss is 0.09591657\n",
      "epoch: 4 step: 89, loss is 0.08086616\n",
      "epoch: 4 step: 90, loss is 0.078015745\n",
      "epoch: 4 step: 91, loss is 0.080150425\n",
      "epoch: 4 step: 92, loss is 0.08228856\n",
      "epoch: 4 step: 93, loss is 0.085888445\n",
      "epoch: 4 step: 94, loss is 0.071823776\n",
      "epoch: 4 step: 95, loss is 0.06607276\n",
      "epoch: 4 step: 96, loss is 0.094858825\n",
      "epoch: 4 step: 97, loss is 0.077893555\n",
      "epoch: 4 step: 98, loss is 0.07707268\n",
      "epoch: 4 step: 99, loss is 0.07315093\n",
      "epoch: 4 step: 100, loss is 0.07592487\n",
      "epoch: 4 step: 101, loss is 0.07068825\n",
      "epoch: 4 step: 102, loss is 0.07795906\n",
      "epoch: 4 step: 103, loss is 0.07547045\n",
      "epoch: 4 step: 104, loss is 0.08803302\n",
      "epoch: 4 step: 105, loss is 0.07565951\n",
      "epoch: 4 step: 106, loss is 0.07955694\n",
      "epoch: 4 step: 107, loss is 0.08444148\n",
      "epoch: 4 step: 108, loss is 0.07776308\n",
      "epoch: 4 step: 109, loss is 0.08260894\n",
      "epoch: 4 step: 110, loss is 0.07649803\n",
      "epoch: 4 step: 111, loss is 0.068222344\n",
      "epoch: 4 step: 112, loss is 0.07404995\n",
      "epoch: 4 step: 113, loss is 0.06373954\n",
      "epoch: 4 step: 114, loss is 0.076391876\n",
      "epoch: 4 step: 115, loss is 0.07768154\n",
      "epoch: 4 step: 116, loss is 0.094537556\n",
      "epoch: 4 step: 117, loss is 0.08586669\n",
      "epoch: 4 step: 118, loss is 0.07705617\n",
      "epoch: 4 step: 119, loss is 0.0715484\n",
      "epoch: 4 step: 120, loss is 0.06404781\n",
      "epoch: 4 step: 121, loss is 0.05211276\n",
      "epoch: 4 step: 122, loss is 0.08580136\n",
      "epoch: 4 step: 123, loss is 0.07462126\n",
      "epoch: 4 step: 124, loss is 0.066313505\n",
      "epoch: 4 step: 125, loss is 0.07434493\n",
      "epoch: 4 step: 126, loss is 0.07070446\n",
      "epoch: 4 step: 127, loss is 0.080046\n",
      "epoch: 4 step: 128, loss is 0.12336558\n",
      "epoch: 4 step: 129, loss is 0.06521481\n",
      "epoch: 4 step: 130, loss is 0.07821721\n",
      "epoch: 4 step: 131, loss is 0.08109242\n",
      "epoch: 4 step: 132, loss is 0.085021794\n",
      "epoch: 4 step: 133, loss is 0.07850546\n",
      "epoch: 4 step: 134, loss is 0.082053006\n",
      "epoch: 4 step: 135, loss is 0.08108264\n",
      "epoch: 4 step: 136, loss is 0.06912756\n",
      "epoch: 4 step: 137, loss is 0.089251995\n",
      "epoch: 4 step: 138, loss is 0.069916725\n",
      "epoch: 4 step: 139, loss is 0.07998806\n",
      "epoch: 4 step: 140, loss is 0.07553339\n",
      "epoch: 4 step: 141, loss is 0.08523345\n",
      "epoch: 4 step: 142, loss is 0.07977152\n",
      "epoch: 4 step: 143, loss is 0.06850964\n",
      "epoch: 4 step: 144, loss is 0.07024312\n",
      "epoch: 4 step: 145, loss is 0.080678225\n",
      "epoch: 4 step: 146, loss is 0.0844813\n",
      "epoch: 4 step: 147, loss is 0.089099884\n",
      "epoch: 4 step: 148, loss is 0.07079631\n",
      "epoch: 4 step: 149, loss is 0.07078552\n",
      "epoch: 4 step: 150, loss is 0.063844204\n",
      "epoch: 4 step: 151, loss is 0.06883001\n",
      "epoch: 4 step: 152, loss is 0.34435982\n",
      "epoch: 4 step: 153, loss is 0.04973823\n",
      "epoch: 4 step: 154, loss is 0.057816207\n",
      "epoch: 4 step: 155, loss is 0.058178604\n",
      "epoch: 4 step: 156, loss is 0.050914466\n",
      "epoch: 4 step: 157, loss is 0.049464643\n",
      "epoch: 4 step: 158, loss is 0.046384275\n",
      "epoch: 4 step: 159, loss is 0.048398793\n",
      "epoch: 4 step: 160, loss is 0.049243867\n",
      "epoch: 4 step: 161, loss is 0.06033069\n",
      "epoch: 4 step: 162, loss is 0.097345054\n",
      "epoch: 4 step: 163, loss is 0.05767685\n",
      "epoch: 4 step: 164, loss is 0.06312132\n",
      "epoch: 4 step: 165, loss is 0.04779893\n",
      "epoch: 4 step: 166, loss is 0.053335488\n",
      "epoch: 4 step: 167, loss is 0.053474903\n",
      "epoch: 4 step: 168, loss is 0.06533927\n",
      "epoch: 4 step: 169, loss is 0.04569286\n",
      "epoch: 4 step: 170, loss is 0.06323022\n",
      "epoch: 4 step: 171, loss is 0.048365295\n",
      "epoch: 4 step: 172, loss is 0.06292087\n",
      "epoch: 4 step: 173, loss is 0.05404991\n",
      "epoch: 4 step: 174, loss is 0.04769796\n",
      "epoch: 4 step: 175, loss is 0.06356573\n",
      "epoch: 4 step: 176, loss is 0.03622943\n",
      "epoch: 4 step: 177, loss is 0.050560236\n",
      "epoch: 4 step: 178, loss is 0.053762734\n",
      "epoch: 4 step: 179, loss is 0.057596922\n",
      "epoch: 4 step: 180, loss is 0.061509788\n",
      "epoch: 4 step: 181, loss is 0.049952805\n",
      "epoch: 4 step: 182, loss is 0.049720585\n",
      "epoch: 4 step: 183, loss is 0.041360676\n",
      "epoch: 4 step: 184, loss is 0.059718132\n",
      "epoch: 4 step: 185, loss is 0.058564425\n",
      "epoch: 4 step: 186, loss is 0.069087446\n",
      "epoch: 4 step: 187, loss is 0.048359334\n",
      "epoch: 4 step: 188, loss is 0.048176765\n",
      "epoch: 4 step: 189, loss is 0.04779166\n",
      "epoch: 4 step: 190, loss is 0.059227705\n",
      "epoch: 4 step: 191, loss is 0.047533333\n",
      "epoch: 4 step: 192, loss is 0.05274129\n",
      "epoch: 4 step: 193, loss is 0.06576586\n",
      "epoch: 4 step: 194, loss is 0.052979708\n",
      "epoch: 4 step: 195, loss is 0.055335045\n",
      "epoch: 4 step: 196, loss is 0.06139785\n",
      "epoch: 4 step: 197, loss is 0.062421083\n",
      "epoch: 4 step: 198, loss is 0.063658\n",
      "epoch: 4 step: 199, loss is 0.044142783\n",
      "epoch: 4 step: 200, loss is 0.046911538\n",
      "epoch: 4 step: 201, loss is 0.052080393\n",
      "epoch: 4 step: 202, loss is 0.05491525\n",
      "epoch: 4 step: 203, loss is 0.047598064\n",
      "epoch: 4 step: 204, loss is 0.06343365\n",
      "epoch: 4 step: 205, loss is 0.06461811\n",
      "epoch: 4 step: 206, loss is 0.04860896\n",
      "epoch: 4 step: 207, loss is 0.050940216\n",
      "epoch: 4 step: 208, loss is 0.057193696\n",
      "epoch: 4 step: 209, loss is 0.057412803\n",
      "epoch: 4 step: 210, loss is 0.058762252\n",
      "epoch: 4 step: 211, loss is 0.055861413\n",
      "epoch: 4 step: 212, loss is 0.057662785\n",
      "epoch: 4 step: 213, loss is 0.078165054\n",
      "epoch: 4 step: 214, loss is 0.042517602\n",
      "epoch: 4 step: 215, loss is 0.04225111\n",
      "epoch: 4 step: 216, loss is 0.048376024\n",
      "epoch: 4 step: 217, loss is 0.059586048\n",
      "epoch: 4 step: 218, loss is 0.051315963\n",
      "epoch: 4 step: 219, loss is 0.060762167\n",
      "epoch: 4 step: 220, loss is 0.0539729\n",
      "epoch: 4 step: 221, loss is 0.06967425\n",
      "epoch: 4 step: 222, loss is 0.050090134\n",
      "epoch: 4 step: 223, loss is 0.06791681\n",
      "epoch: 4 step: 224, loss is 0.056013286\n",
      "epoch: 4 step: 225, loss is 0.06783193\n",
      "epoch: 4 step: 226, loss is 0.055398047\n",
      "epoch: 4 step: 227, loss is 0.04321623\n",
      "epoch: 4 step: 228, loss is 0.051064253\n",
      "epoch: 4 step: 229, loss is 0.064869225\n",
      "epoch: 4 step: 230, loss is 0.06129986\n",
      "epoch: 4 step: 231, loss is 0.053702354\n",
      "epoch: 4 step: 232, loss is 0.085403204\n",
      "epoch: 4 step: 233, loss is 0.0445776\n",
      "epoch: 4 step: 234, loss is 0.06982964\n",
      "epoch: 4 step: 235, loss is 0.05403304\n",
      "epoch: 4 step: 236, loss is 0.06449103\n",
      "epoch: 4 step: 237, loss is 0.054652393\n",
      "epoch: 4 step: 238, loss is 0.0614236\n",
      "epoch: 4 step: 239, loss is 0.059764206\n",
      "epoch: 4 step: 240, loss is 0.060974658\n",
      "epoch: 4 step: 241, loss is 0.060532033\n",
      "epoch: 4 step: 242, loss is 0.057482064\n",
      "epoch: 4 step: 243, loss is 0.05603695\n",
      "epoch: 4 step: 244, loss is 0.05897981\n",
      "epoch: 4 step: 245, loss is 0.065657794\n",
      "epoch: 4 step: 246, loss is 0.05630088\n",
      "epoch: 4 step: 247, loss is 0.055223405\n",
      "epoch: 4 step: 248, loss is 0.05966395\n",
      "epoch: 4 step: 249, loss is 0.059156954\n",
      "epoch: 4 step: 250, loss is 0.05959326\n",
      "epoch: 4 step: 251, loss is 0.06755072\n",
      "epoch: 4 step: 252, loss is 0.07289654\n",
      "epoch: 4 step: 253, loss is 0.06871003\n",
      "epoch: 4 step: 254, loss is 0.055719376\n",
      "epoch: 4 step: 255, loss is 0.057600677\n",
      "epoch: 4 step: 256, loss is 0.070449114\n",
      "epoch: 4 step: 257, loss is 0.044362366\n",
      "epoch: 4 step: 258, loss is 0.06894112\n",
      "epoch: 4 step: 259, loss is 0.03995937\n",
      "epoch: 4 step: 260, loss is 0.065081775\n",
      "epoch: 4 step: 261, loss is 0.04810363\n",
      "epoch: 4 step: 262, loss is 0.0610978\n",
      "epoch: 4 step: 263, loss is 0.056324065\n",
      "epoch: 4 step: 264, loss is 0.06044799\n",
      "epoch: 4 step: 265, loss is 0.058900654\n",
      "epoch: 4 step: 266, loss is 0.06811863\n",
      "epoch: 4 step: 267, loss is 0.046977818\n",
      "epoch: 4 step: 268, loss is 0.068413734\n",
      "epoch: 4 step: 269, loss is 0.053677082\n",
      "epoch: 4 step: 270, loss is 0.07327205\n",
      "epoch: 4 step: 271, loss is 0.06750554\n",
      "epoch: 4 step: 272, loss is 0.061622918\n",
      "epoch: 4 step: 273, loss is 0.060899556\n",
      "epoch: 4 step: 274, loss is 0.05864495\n",
      "epoch: 4 step: 275, loss is 0.07465267\n",
      "epoch: 4 step: 276, loss is 0.0602355\n",
      "epoch: 4 step: 277, loss is 0.066348374\n",
      "epoch: 4 step: 278, loss is 0.040429294\n",
      "epoch: 4 step: 279, loss is 0.0358693\n",
      "epoch: 4 step: 280, loss is 0.024253547\n",
      "epoch: 4 step: 281, loss is 0.03163308\n",
      "epoch: 4 step: 282, loss is 0.026846409\n",
      "epoch: 4 step: 283, loss is 0.023477972\n",
      "epoch: 4 step: 284, loss is 0.031845868\n",
      "epoch: 4 step: 285, loss is 0.048204362\n",
      "epoch: 4 step: 286, loss is 0.02570635\n",
      "epoch: 4 step: 287, loss is 0.032396793\n",
      "epoch: 4 step: 288, loss is 0.028521478\n",
      "epoch: 4 step: 289, loss is 0.020376682\n",
      "epoch: 4 step: 290, loss is 0.037894726\n",
      "epoch: 4 step: 291, loss is 0.04818207\n",
      "epoch: 4 step: 292, loss is 0.036257327\n",
      "epoch: 4 step: 293, loss is 0.046384633\n",
      "epoch: 4 step: 294, loss is 0.040842593\n",
      "epoch: 4 step: 295, loss is 0.031517208\n",
      "epoch: 4 step: 296, loss is 0.062446773\n",
      "epoch: 4 step: 297, loss is 0.054626644\n",
      "epoch: 4 step: 298, loss is 0.06376147\n",
      "epoch: 4 step: 299, loss is 0.06198734\n",
      "epoch: 4 step: 300, loss is 0.037762403\n",
      "epoch: 4 step: 301, loss is 0.041232526\n",
      "epoch: 4 step: 302, loss is 0.045152187\n",
      "epoch: 4 step: 303, loss is 0.03644198\n",
      "epoch: 4 step: 304, loss is 0.025227487\n",
      "epoch: 4 step: 305, loss is 0.055182934\n",
      "epoch: 4 step: 306, loss is 0.08036208\n",
      "epoch: 4 step: 307, loss is 0.067341566\n",
      "epoch: 4 step: 308, loss is 0.045261443\n",
      "epoch: 4 step: 309, loss is 0.08050609\n",
      "epoch: 4 step: 310, loss is 0.0777877\n",
      "epoch: 4 step: 311, loss is 0.079419374\n",
      "epoch: 4 step: 312, loss is 0.067913234\n",
      "epoch: 4 step: 313, loss is 0.07251096\n",
      "epoch: 4 step: 314, loss is 0.06588966\n",
      "epoch: 4 step: 315, loss is 0.08260566\n",
      "epoch: 4 step: 316, loss is 0.06952733\n",
      "epoch: 4 step: 317, loss is 0.08506626\n",
      "epoch: 4 step: 318, loss is 0.07387036\n",
      "epoch: 4 step: 319, loss is 0.06449723\n",
      "epoch: 4 step: 320, loss is 0.08217502\n",
      "epoch: 4 step: 321, loss is 0.07231957\n",
      "epoch: 4 step: 322, loss is 0.08415872\n",
      "epoch: 4 step: 323, loss is 0.08384353\n",
      "epoch: 4 step: 324, loss is 0.075761855\n",
      "epoch: 4 step: 325, loss is 0.0823369\n",
      "epoch: 4 step: 326, loss is 0.085831165\n",
      "epoch: 4 step: 327, loss is 0.058552742\n",
      "epoch: 4 step: 328, loss is 0.076927125\n",
      "epoch: 4 step: 329, loss is 0.08270329\n",
      "epoch: 4 step: 330, loss is 0.06681442\n",
      "epoch: 4 step: 331, loss is 0.064374626\n",
      "epoch: 4 step: 332, loss is 0.05965042\n",
      "epoch: 4 step: 333, loss is 0.07092142\n",
      "epoch: 4 step: 334, loss is 0.057328284\n",
      "epoch: 4 step: 335, loss is 0.07638878\n",
      "epoch: 4 step: 336, loss is 0.080652654\n",
      "epoch: 4 step: 337, loss is 0.073630154\n",
      "epoch: 4 step: 338, loss is 0.06762242\n",
      "epoch: 4 step: 339, loss is 0.06993532\n",
      "epoch: 4 step: 340, loss is 0.07827395\n",
      "epoch: 4 step: 341, loss is 0.07197684\n",
      "epoch: 4 step: 342, loss is 0.07328147\n",
      "epoch: 4 step: 343, loss is 0.0785386\n",
      "epoch: 4 step: 344, loss is 0.04554224\n",
      "epoch: 4 step: 345, loss is 0.05932975\n",
      "epoch: 4 step: 346, loss is 0.062333822\n",
      "epoch: 4 step: 347, loss is 0.06910366\n",
      "epoch: 4 step: 348, loss is 0.068918705\n",
      "epoch: 4 step: 349, loss is 0.07015437\n",
      "epoch: 4 step: 350, loss is 0.07286239\n",
      "epoch: 4 step: 351, loss is 0.058447838\n",
      "epoch: 4 step: 352, loss is 0.06895858\n",
      "epoch: 4 step: 353, loss is 0.06657386\n",
      "epoch: 4 step: 354, loss is 0.06576443\n",
      "epoch: 4 step: 355, loss is 0.074640274\n",
      "epoch: 4 step: 356, loss is 0.068614066\n",
      "epoch: 4 step: 357, loss is 0.06305581\n",
      "epoch: 4 step: 358, loss is 0.059057772\n",
      "epoch: 4 step: 359, loss is 0.08453876\n",
      "epoch: 4 step: 360, loss is 0.07098931\n",
      "epoch: 4 step: 361, loss is 0.078259945\n",
      "epoch: 4 step: 362, loss is 0.06876564\n",
      "epoch: 4 step: 363, loss is 0.07646656\n",
      "epoch: 4 step: 364, loss is 0.061803818\n",
      "epoch: 4 step: 365, loss is 0.065316856\n",
      "epoch: 4 step: 366, loss is 0.062345028\n",
      "epoch: 4 step: 367, loss is 0.07362801\n",
      "epoch: 4 step: 368, loss is 0.06762648\n",
      "epoch: 4 step: 369, loss is 0.066612065\n",
      "epoch: 4 step: 370, loss is 0.07928592\n",
      "epoch: 4 step: 371, loss is 0.0699051\n",
      "epoch: 4 step: 372, loss is 0.07755548\n",
      "epoch: 4 step: 373, loss is 0.052487433\n",
      "epoch: 4 step: 374, loss is 0.08078176\n",
      "epoch: 4 step: 375, loss is 0.07408619\n",
      "epoch: 4 step: 376, loss is 0.06996679\n",
      "epoch: 4 step: 377, loss is 0.08327943\n",
      "epoch: 4 step: 378, loss is 0.055279553\n",
      "epoch: 4 step: 379, loss is 0.06696969\n",
      "epoch: 4 step: 380, loss is 0.067944944\n",
      "epoch: 4 step: 381, loss is 0.08265585\n",
      "epoch: 4 step: 382, loss is 0.07916641\n",
      "epoch: 4 step: 383, loss is 0.12863213\n",
      "epoch: 4 step: 384, loss is 0.0944829\n",
      "epoch: 4 step: 385, loss is 0.07619572\n",
      "epoch: 4 step: 386, loss is 0.07739085\n",
      "epoch: 4 step: 387, loss is 0.06523615\n",
      "epoch: 4 step: 388, loss is 0.08808279\n",
      "epoch: 4 step: 389, loss is 0.059181213\n",
      "epoch: 4 step: 390, loss is 0.06816387\n",
      "epoch: 4 step: 391, loss is 0.063506424\n",
      "epoch: 4 step: 392, loss is 0.05101633\n",
      "epoch: 4 step: 393, loss is 0.064065695\n",
      "epoch: 4 step: 394, loss is 0.06512666\n",
      "epoch: 4 step: 395, loss is 0.075099885\n",
      "epoch: 4 step: 396, loss is 0.07636315\n",
      "epoch: 4 step: 397, loss is 0.062395275\n",
      "epoch: 4 step: 398, loss is 0.068567574\n",
      "epoch: 4 step: 399, loss is 0.045966387\n",
      "epoch: 4 step: 400, loss is 0.053870678\n",
      "epoch: 4 step: 401, loss is 0.037302732\n",
      "epoch: 4 step: 402, loss is 0.072247684\n",
      "epoch: 4 step: 403, loss is 0.053775012\n",
      "epoch: 4 step: 404, loss is 0.0511809\n",
      "epoch: 4 step: 405, loss is 0.07007766\n",
      "epoch: 4 step: 406, loss is 0.06786823\n",
      "epoch: 4 step: 407, loss is 0.06551474\n",
      "epoch: 4 step: 408, loss is 0.042495966\n",
      "epoch: 4 step: 409, loss is 0.06703538\n",
      "epoch: 4 step: 410, loss is 0.06531316\n",
      "epoch: 4 step: 411, loss is 0.060199678\n",
      "epoch: 4 step: 412, loss is 0.057973623\n",
      "epoch: 4 step: 413, loss is 0.05913329\n",
      "epoch: 4 step: 414, loss is 0.06443167\n",
      "epoch: 4 step: 415, loss is 0.06852716\n",
      "epoch: 4 step: 416, loss is 0.047312677\n",
      "epoch: 4 step: 417, loss is 0.06334144\n",
      "epoch: 4 step: 418, loss is 0.07906932\n",
      "epoch: 4 step: 419, loss is 0.069630325\n",
      "epoch: 4 step: 420, loss is 0.06343335\n",
      "epoch: 4 step: 421, loss is 0.051697314\n",
      "epoch: 4 step: 422, loss is 0.05738783\n",
      "epoch: 4 step: 423, loss is 0.058941305\n",
      "epoch: 4 step: 424, loss is 0.05608678\n",
      "epoch: 4 step: 425, loss is 0.053168118\n",
      "epoch: 4 step: 426, loss is 0.056946218\n",
      "epoch: 4 step: 427, loss is 0.07429123\n",
      "epoch: 4 step: 428, loss is 0.051579773\n",
      "epoch: 4 step: 429, loss is 0.05583054\n",
      "epoch: 4 step: 430, loss is 0.08037704\n",
      "epoch: 4 step: 431, loss is 0.031391323\n",
      "epoch: 4 step: 432, loss is 0.060218155\n",
      "epoch: 4 step: 433, loss is 0.05660373\n",
      "epoch: 4 step: 434, loss is 0.06775612\n",
      "epoch: 4 step: 435, loss is 0.0773468\n",
      "epoch: 4 step: 436, loss is 0.07060355\n",
      "epoch: 4 step: 437, loss is 0.09190816\n",
      "epoch: 4 step: 438, loss is 0.07224244\n",
      "epoch: 4 step: 439, loss is 0.060787976\n",
      "epoch: 4 step: 440, loss is 0.06171304\n",
      "epoch: 4 step: 441, loss is 0.07147032\n",
      "epoch: 4 step: 442, loss is 0.059907973\n",
      "epoch: 4 step: 443, loss is 0.054822147\n",
      "epoch: 4 step: 444, loss is 0.064463615\n",
      "epoch: 4 step: 445, loss is 0.082785845\n",
      "epoch: 4 step: 446, loss is 0.07032317\n",
      "epoch: 4 step: 447, loss is 0.0598495\n",
      "epoch: 4 step: 448, loss is 0.075754225\n",
      "epoch: 4 step: 449, loss is 0.033535242\n",
      "epoch: 4 step: 450, loss is 0.07354182\n",
      "epoch: 4 step: 451, loss is 0.07780212\n",
      "epoch: 4 step: 452, loss is 0.086779416\n",
      "epoch: 4 step: 453, loss is 0.08117628\n",
      "epoch: 4 step: 454, loss is 0.06273097\n",
      "epoch: 4 step: 455, loss is 0.0741238\n",
      "epoch: 4 step: 456, loss is 0.022301197\n",
      "epoch: 4 step: 457, loss is 0.06935173\n",
      "epoch: 4 step: 458, loss is 0.08202386\n",
      "epoch: 4 step: 459, loss is 0.068995476\n",
      "epoch: 4 step: 460, loss is 0.056316078\n",
      "epoch: 4 step: 461, loss is 0.051500797\n",
      "epoch: 4 step: 462, loss is 0.069549024\n",
      "epoch: 4 step: 463, loss is 0.08275193\n",
      "epoch: 4 step: 464, loss is 0.07736182\n",
      "epoch: 4 step: 465, loss is 0.06434411\n",
      "epoch: 4 step: 466, loss is 0.04998833\n",
      "epoch: 4 step: 467, loss is 0.058191\n",
      "epoch: 4 step: 468, loss is 0.047766745\n",
      "epoch: 4 step: 469, loss is 0.08808082\n",
      "epoch: 4 step: 470, loss is 0.08312577\n",
      "epoch: 4 step: 471, loss is 0.07318872\n",
      "epoch: 4 step: 472, loss is 0.05941242\n",
      "epoch: 4 step: 473, loss is 0.0708583\n",
      "epoch: 4 step: 474, loss is 0.05002469\n",
      "epoch: 4 step: 475, loss is 0.056691885\n",
      "epoch: 4 step: 476, loss is 0.04771644\n",
      "epoch: 4 step: 477, loss is 0.068244636\n",
      "epoch: 4 step: 478, loss is 0.06037575\n",
      "epoch: 4 step: 479, loss is 0.061642885\n",
      "epoch: 4 step: 480, loss is 0.049965322\n",
      "epoch: 4 step: 481, loss is 0.08926982\n",
      "epoch: 4 step: 482, loss is 0.054736555\n",
      "epoch: 4 step: 483, loss is 0.060198486\n",
      "epoch: 4 step: 484, loss is 0.06471318\n",
      "epoch: 4 step: 485, loss is 0.061951935\n",
      "epoch: 4 step: 486, loss is 0.05732757\n",
      "epoch: 4 step: 487, loss is 0.07515669\n",
      "epoch: 4 step: 488, loss is 0.07155055\n",
      "epoch: 4 step: 489, loss is 0.085707664\n",
      "epoch: 4 step: 490, loss is 0.065122545\n",
      "epoch: 4 step: 491, loss is 0.030185401\n",
      "epoch: 4 step: 492, loss is 0.051002204\n",
      "epoch: 4 step: 493, loss is 0.05676508\n",
      "epoch: 4 step: 494, loss is 0.07195377\n",
      "epoch: 4 step: 495, loss is 0.051817417\n",
      "epoch: 4 step: 496, loss is 0.06936079\n",
      "epoch: 4 step: 497, loss is 0.065920174\n",
      "epoch: 4 step: 498, loss is 0.06773257\n",
      "epoch: 4 step: 499, loss is 0.060800314\n",
      "epoch: 4 step: 500, loss is 0.06236571\n",
      "epoch: 4 step: 501, loss is 0.05072522\n",
      "epoch: 4 step: 502, loss is 0.050551713\n",
      "epoch: 4 step: 503, loss is 0.07592249\n",
      "epoch: 4 step: 504, loss is 0.082476795\n",
      "epoch: 4 step: 505, loss is 0.07393664\n",
      "epoch: 4 step: 506, loss is 0.071178734\n",
      "epoch: 4 step: 507, loss is 0.059056938\n",
      "epoch: 4 step: 508, loss is 0.055277348\n",
      "epoch: 4 step: 509, loss is 0.07891917\n",
      "epoch: 4 step: 510, loss is 0.082717955\n",
      "epoch: 4 step: 511, loss is 0.090946496\n",
      "epoch: 4 step: 512, loss is 0.06479788\n",
      "epoch: 4 step: 513, loss is 0.0672462\n",
      "epoch: 4 step: 514, loss is 0.077661335\n",
      "epoch: 4 step: 515, loss is 0.06495285\n",
      "epoch: 4 step: 516, loss is 0.05872923\n",
      "epoch: 4 step: 517, loss is 0.07004881\n",
      "epoch: 4 step: 518, loss is 0.07817316\n",
      "epoch: 4 step: 519, loss is 0.06680077\n",
      "epoch: 4 step: 520, loss is 0.0743863\n",
      "epoch: 4 step: 521, loss is 0.106045425\n",
      "epoch: 4 step: 522, loss is 0.059424877\n",
      "epoch: 4 step: 523, loss is 0.07061362\n",
      "epoch: 4 step: 524, loss is 0.10432178\n",
      "epoch: 4 step: 525, loss is 0.055377245\n",
      "epoch: 4 step: 526, loss is 0.057116687\n",
      "epoch: 4 step: 527, loss is 0.047795296\n",
      "epoch: 4 step: 528, loss is 0.059883177\n",
      "epoch: 4 step: 529, loss is 0.061793983\n",
      "epoch: 4 step: 530, loss is 0.054590464\n",
      "epoch: 4 step: 531, loss is 0.051461875\n",
      "epoch: 4 step: 532, loss is 0.05740851\n",
      "epoch: 4 step: 533, loss is 0.06413531\n",
      "epoch: 4 step: 534, loss is 0.08485299\n",
      "epoch: 4 step: 535, loss is 0.07783133\n",
      "epoch: 4 step: 536, loss is 0.071514785\n",
      "epoch: 4 step: 537, loss is 0.054948628\n",
      "epoch: 4 step: 538, loss is 0.06391412\n",
      "epoch: 4 step: 539, loss is 0.056722105\n",
      "epoch: 4 step: 540, loss is 0.05709219\n",
      "epoch: 4 step: 541, loss is 0.058380365\n",
      "epoch: 4 step: 542, loss is 0.064261734\n",
      "epoch: 4 step: 543, loss is 0.06279582\n",
      "epoch: 4 step: 544, loss is 0.047063112\n",
      "epoch: 4 step: 545, loss is 0.08373231\n",
      "epoch: 4 step: 546, loss is 0.091142595\n",
      "epoch: 4 step: 547, loss is 0.07005125\n",
      "epoch: 4 step: 548, loss is 0.07292223\n",
      "epoch: 4 step: 549, loss is 0.06741053\n",
      "epoch: 4 step: 550, loss is 0.06902081\n",
      "epoch: 4 step: 551, loss is 0.059855163\n",
      "epoch: 4 step: 552, loss is 0.0650906\n",
      "epoch: 4 step: 553, loss is 0.07577038\n",
      "epoch: 4 step: 554, loss is 0.08629638\n",
      "epoch: 4 step: 555, loss is 0.07152349\n",
      "epoch: 4 step: 556, loss is 0.05931449\n",
      "epoch: 4 step: 557, loss is 0.06359023\n",
      "epoch: 4 step: 558, loss is 0.05990976\n",
      "epoch: 4 step: 559, loss is 0.08304238\n",
      "epoch: 4 step: 560, loss is 0.073615074\n",
      "epoch: 4 step: 561, loss is 0.05580336\n",
      "epoch: 4 step: 562, loss is 0.061240375\n",
      "epoch: 4 step: 563, loss is 0.05439353\n",
      "epoch: 4 step: 564, loss is 0.060205936\n",
      "epoch: 4 step: 565, loss is 0.05550927\n",
      "epoch: 4 step: 566, loss is 0.060733616\n",
      "epoch: 4 step: 567, loss is 0.061519563\n",
      "epoch: 4 step: 568, loss is 0.058965683\n",
      "epoch: 4 step: 569, loss is 0.056489646\n",
      "epoch: 4 step: 570, loss is 0.052828014\n",
      "epoch: 4 step: 571, loss is 0.061086357\n",
      "epoch: 4 step: 572, loss is 0.05369377\n",
      "epoch: 4 step: 573, loss is 0.07549906\n",
      "epoch: 4 step: 574, loss is 0.064474285\n",
      "epoch: 4 step: 575, loss is 0.064355075\n",
      "epoch: 4 step: 576, loss is 0.05742985\n",
      "epoch: 4 step: 577, loss is 0.08204955\n",
      "epoch: 4 step: 578, loss is 0.058850765\n",
      "epoch: 4 step: 579, loss is 0.0542953\n",
      "epoch: 4 step: 580, loss is 0.06292683\n",
      "epoch: 4 step: 581, loss is 0.059205472\n",
      "epoch: 4 step: 582, loss is 0.05587125\n",
      "epoch: 4 step: 583, loss is -4.3697066\n",
      "epoch: 4 step: 584, loss is 0.06827313\n",
      "epoch: 4 step: 585, loss is 0.071977854\n",
      "epoch: 4 step: 586, loss is 0.07441139\n",
      "epoch: 4 step: 587, loss is 0.043620348\n",
      "epoch: 4 step: 588, loss is 0.07218319\n",
      "epoch: 4 step: 589, loss is 0.07692784\n",
      "epoch: 4 step: 590, loss is 0.084625065\n",
      "epoch: 4 step: 591, loss is 0.06224519\n",
      "epoch: 4 step: 592, loss is 0.06381327\n",
      "epoch: 4 step: 593, loss is 0.059770882\n",
      "epoch: 4 step: 594, loss is 0.07487869\n",
      "epoch: 4 step: 595, loss is 0.068440616\n",
      "epoch: 4 step: 596, loss is 0.077448666\n",
      "epoch: 4 step: 597, loss is 0.09141737\n",
      "epoch: 4 step: 598, loss is 0.08201319\n",
      "epoch: 4 step: 599, loss is 0.06145805\n",
      "epoch: 4 step: 600, loss is 0.07274228\n",
      "epoch: 4 step: 601, loss is 0.066670895\n",
      "epoch: 4 step: 602, loss is 0.08183044\n",
      "epoch: 4 step: 603, loss is 0.08038658\n",
      "epoch: 4 step: 604, loss is 0.08395213\n",
      "epoch: 4 step: 605, loss is 0.06143874\n",
      "epoch: 4 step: 606, loss is 0.08188057\n",
      "epoch: 4 step: 607, loss is 0.064231634\n",
      "epoch: 4 step: 608, loss is 0.08360094\n",
      "epoch: 4 step: 609, loss is 0.08847666\n",
      "epoch: 4 step: 610, loss is 0.0747903\n",
      "epoch: 4 step: 611, loss is 0.07787305\n",
      "epoch: 4 step: 612, loss is 0.06304473\n",
      "epoch: 4 step: 613, loss is 0.07522899\n",
      "epoch: 4 step: 614, loss is 0.06836277\n",
      "epoch: 4 step: 615, loss is 0.07582837\n",
      "epoch: 4 step: 616, loss is 0.06650376\n",
      "epoch: 4 step: 617, loss is 0.07246536\n",
      "epoch: 4 step: 618, loss is 0.0839107\n",
      "epoch: 4 step: 619, loss is 0.0740518\n",
      "epoch: 4 step: 620, loss is 0.07810587\n",
      "epoch: 4 step: 621, loss is 0.06643695\n",
      "epoch: 4 step: 622, loss is 0.0874362\n",
      "epoch: 4 step: 623, loss is 0.058912933\n",
      "epoch: 4 step: 624, loss is 0.071917236\n",
      "epoch: 4 step: 625, loss is 0.07040912\n",
      "epoch: 4 step: 626, loss is 0.06887525\n",
      "epoch: 4 step: 627, loss is -0.0030035973\n",
      "epoch: 4 step: 628, loss is 0.08960563\n",
      "epoch: 4 step: 629, loss is 0.07267517\n",
      "epoch: 4 step: 630, loss is 0.0811615\n",
      "epoch: 4 step: 631, loss is 0.07146186\n",
      "epoch: 4 step: 632, loss is 0.0877102\n",
      "epoch: 4 step: 633, loss is 0.081365764\n",
      "epoch: 4 step: 634, loss is 0.060988486\n",
      "epoch: 4 step: 635, loss is 0.07000607\n",
      "epoch: 4 step: 636, loss is 0.07140523\n",
      "epoch: 4 step: 637, loss is 0.07607168\n",
      "epoch: 4 step: 638, loss is 0.05577439\n",
      "epoch: 4 step: 639, loss is 0.08072251\n",
      "epoch: 4 step: 640, loss is 0.068170846\n",
      "epoch: 4 step: 641, loss is 0.08057004\n",
      "epoch: 4 step: 642, loss is 0.067701995\n",
      "epoch: 4 step: 643, loss is 0.06753367\n",
      "epoch: 4 step: 644, loss is 0.0727604\n",
      "epoch: 4 step: 645, loss is 0.07969093\n",
      "epoch: 4 step: 646, loss is 0.08914393\n",
      "epoch: 4 step: 647, loss is 0.08077288\n",
      "epoch: 4 step: 648, loss is 0.082853734\n",
      "epoch: 4 step: 649, loss is 0.06802297\n",
      "epoch: 4 step: 650, loss is 0.07889718\n",
      "epoch: 4 step: 651, loss is 0.0422585\n",
      "epoch: 4 step: 652, loss is 0.11130828\n",
      "epoch: 4 step: 653, loss is 0.07203817\n",
      "epoch: 4 step: 654, loss is 0.08039349\n",
      "epoch: 4 step: 655, loss is 0.06340122\n",
      "epoch: 4 step: 656, loss is 0.0661971\n",
      "epoch: 4 step: 657, loss is 0.055721104\n",
      "epoch: 4 step: 658, loss is 0.06309205\n",
      "epoch: 4 step: 659, loss is 0.13774127\n",
      "epoch: 4 step: 660, loss is 0.06775141\n",
      "epoch: 4 step: 661, loss is 0.08847928\n",
      "epoch: 4 step: 662, loss is 0.08203727\n",
      "epoch: 4 step: 663, loss is 0.06801337\n",
      "epoch: 4 step: 664, loss is 0.07456428\n",
      "epoch: 4 step: 665, loss is 0.07004565\n",
      "epoch: 4 step: 666, loss is 0.08612639\n",
      "epoch: 4 step: 667, loss is 0.071968734\n",
      "epoch: 4 step: 668, loss is 0.060536146\n",
      "epoch: 4 step: 669, loss is 0.07452583\n",
      "epoch: 4 step: 670, loss is 0.06832999\n",
      "epoch: 4 step: 671, loss is 0.06038052\n",
      "epoch: 4 step: 672, loss is 0.07172561\n",
      "epoch: 4 step: 673, loss is 0.0637719\n",
      "epoch: 4 step: 674, loss is 0.07106\n",
      "epoch: 4 step: 675, loss is 0.07622337\n",
      "epoch: 4 step: 676, loss is 0.046257675\n",
      "epoch: 4 step: 677, loss is 0.072354615\n",
      "epoch: 4 step: 678, loss is 0.066274345\n",
      "epoch: 4 step: 679, loss is 0.07531339\n",
      "epoch: 4 step: 680, loss is 0.07304102\n",
      "epoch: 4 step: 681, loss is 0.0441311\n",
      "epoch: 4 step: 682, loss is 0.06781787\n",
      "epoch: 4 step: 683, loss is 0.06424242\n",
      "epoch: 4 step: 684, loss is 0.077764094\n",
      "epoch: 4 step: 685, loss is 0.06325632\n",
      "epoch: 4 step: 686, loss is 0.0705381\n",
      "epoch: 4 step: 687, loss is 0.0765906\n",
      "epoch: 4 step: 688, loss is 0.06382877\n",
      "epoch: 4 step: 689, loss is 0.07279682\n",
      "epoch: 4 step: 690, loss is 0.076939344\n",
      "epoch: 4 step: 691, loss is 0.06137663\n",
      "epoch: 4 step: 692, loss is 0.05752355\n",
      "epoch: 4 step: 693, loss is 0.07503611\n",
      "epoch: 4 step: 694, loss is 0.06324601\n",
      "epoch: 4 step: 695, loss is 0.045146942\n",
      "epoch: 4 step: 696, loss is 0.07710534\n",
      "epoch: 4 step: 697, loss is 0.06464124\n",
      "epoch: 4 step: 698, loss is 0.06574267\n",
      "epoch: 4 step: 699, loss is 0.071172\n",
      "epoch: 4 step: 700, loss is 0.0718233\n",
      "epoch: 4 step: 701, loss is 0.068273306\n",
      "epoch: 4 step: 702, loss is 0.053248644\n",
      "epoch: 4 step: 703, loss is 0.047890484\n",
      "epoch: 4 step: 704, loss is 0.074249566\n",
      "epoch: 4 step: 705, loss is 0.07607794\n",
      "epoch: 4 step: 706, loss is 0.060447395\n",
      "epoch: 4 step: 707, loss is 0.07930082\n",
      "epoch: 4 step: 708, loss is 0.06918794\n",
      "epoch: 4 step: 709, loss is 0.08661765\n",
      "epoch: 4 step: 710, loss is 0.06425685\n",
      "epoch: 4 step: 711, loss is 0.06265205\n",
      "epoch: 4 step: 712, loss is 0.07608563\n",
      "epoch: 4 step: 713, loss is 0.061403573\n",
      "epoch: 4 step: 714, loss is 0.074855864\n",
      "epoch: 4 step: 715, loss is 0.06729585\n",
      "epoch: 4 step: 716, loss is 0.071309865\n",
      "epoch: 4 step: 717, loss is 0.071994126\n",
      "epoch: 4 step: 718, loss is 0.07121593\n",
      "epoch: 4 step: 719, loss is 0.074700356\n",
      "epoch: 4 step: 720, loss is 0.08314437\n",
      "epoch: 4 step: 721, loss is 0.06574631\n",
      "epoch: 4 step: 722, loss is 0.0726307\n",
      "epoch: 4 step: 723, loss is 0.07342285\n",
      "epoch: 4 step: 724, loss is 0.05556184\n",
      "epoch: 4 step: 725, loss is 0.086385906\n",
      "epoch: 4 step: 726, loss is 0.07905167\n",
      "epoch: 4 step: 727, loss is 0.07296729\n",
      "epoch: 4 step: 728, loss is 0.0915966\n",
      "epoch: 4 step: 729, loss is 0.06375599\n",
      "epoch: 4 step: 730, loss is 0.091379404\n",
      "epoch: 4 step: 731, loss is 0.07116479\n",
      "epoch: 4 step: 732, loss is 0.0712325\n",
      "epoch: 4 step: 733, loss is 0.08284122\n",
      "epoch: 4 step: 734, loss is 0.08494443\n",
      "epoch: 4 step: 735, loss is 0.057207644\n",
      "epoch: 4 step: 736, loss is 0.07512194\n",
      "epoch: 4 step: 737, loss is 0.08378333\n",
      "epoch: 4 step: 738, loss is 0.083797514\n",
      "epoch: 4 step: 739, loss is 0.0076916814\n",
      "epoch: 4 step: 740, loss is 0.082225025\n",
      "epoch: 4 step: 741, loss is 0.06622952\n",
      "epoch: 4 step: 742, loss is 0.06535798\n",
      "epoch: 4 step: 743, loss is 0.0725317\n",
      "epoch: 4 step: 744, loss is 0.060599506\n",
      "epoch: 4 step: 745, loss is 0.048921645\n",
      "epoch: 4 step: 746, loss is 0.07086462\n",
      "epoch: 4 step: 747, loss is 0.058867693\n",
      "epoch: 4 step: 748, loss is 0.07519293\n",
      "epoch: 4 step: 749, loss is 0.07533556\n",
      "epoch: 4 step: 750, loss is 0.064662516\n",
      "epoch: 4 step: 751, loss is 0.05757284\n",
      "epoch: 4 step: 752, loss is 0.04349166\n",
      "epoch: 4 step: 753, loss is 0.061131477\n",
      "epoch: 4 step: 754, loss is 0.08816123\n",
      "epoch: 4 step: 755, loss is 0.058879673\n",
      "epoch: 4 step: 756, loss is 0.042886555\n",
      "epoch: 4 step: 757, loss is 0.085452616\n",
      "epoch: 4 step: 758, loss is 0.057745993\n",
      "epoch: 4 step: 759, loss is 0.06695509\n",
      "epoch: 4 step: 760, loss is 0.060498238\n",
      "epoch: 4 step: 761, loss is 0.054167032\n",
      "epoch: 4 step: 762, loss is 0.067970276\n",
      "epoch: 4 step: 763, loss is 0.05022812\n",
      "epoch: 4 step: 764, loss is 0.08305472\n",
      "epoch: 4 step: 765, loss is 0.0682047\n",
      "epoch: 4 step: 766, loss is 0.07140827\n",
      "epoch: 4 step: 767, loss is 0.052909195\n",
      "epoch: 4 step: 768, loss is 0.06863707\n",
      "epoch: 4 step: 769, loss is 0.07178885\n",
      "epoch: 4 step: 770, loss is 0.05829811\n",
      "epoch: 4 step: 771, loss is 0.07020545\n",
      "epoch: 4 step: 772, loss is 0.056861103\n",
      "epoch: 4 step: 773, loss is 0.07602602\n",
      "epoch: 4 step: 774, loss is 0.06417209\n",
      "epoch: 4 step: 775, loss is 0.06358242\n",
      "epoch: 5 step: 1, loss is 0.059945643\n",
      "epoch: 5 step: 2, loss is 0.053403556\n",
      "epoch: 5 step: 3, loss is 0.06644219\n",
      "epoch: 5 step: 4, loss is 0.049107015\n",
      "epoch: 5 step: 5, loss is -0.0022121668\n",
      "epoch: 5 step: 6, loss is 0.054124177\n",
      "epoch: 5 step: 7, loss is 0.05573362\n",
      "epoch: 5 step: 8, loss is 0.048701704\n",
      "epoch: 5 step: 9, loss is 0.10381204\n",
      "epoch: 5 step: 10, loss is 0.06792849\n",
      "epoch: 5 step: 11, loss is 0.052533627\n",
      "epoch: 5 step: 12, loss is 0.060780525\n",
      "epoch: 5 step: 13, loss is 0.07960945\n",
      "epoch: 5 step: 14, loss is 0.053850174\n",
      "epoch: 5 step: 15, loss is 0.045139074\n",
      "epoch: 5 step: 16, loss is 0.066844165\n",
      "epoch: 5 step: 17, loss is 0.062050164\n",
      "epoch: 5 step: 18, loss is 0.045049906\n",
      "epoch: 5 step: 19, loss is 0.0552029\n",
      "epoch: 5 step: 20, loss is 0.060721636\n",
      "epoch: 5 step: 21, loss is 0.060421705\n",
      "epoch: 5 step: 22, loss is 0.055039585\n",
      "epoch: 5 step: 23, loss is 0.0635733\n",
      "epoch: 5 step: 24, loss is 0.052908003\n",
      "epoch: 5 step: 25, loss is 0.069209754\n",
      "epoch: 5 step: 26, loss is 0.05723548\n",
      "epoch: 5 step: 27, loss is 0.04538089\n",
      "epoch: 5 step: 28, loss is 0.05731839\n",
      "epoch: 5 step: 29, loss is 0.03834039\n",
      "epoch: 5 step: 30, loss is 0.0669772\n",
      "epoch: 5 step: 31, loss is 0.07012242\n",
      "epoch: 5 step: 32, loss is 0.06334382\n",
      "epoch: 5 step: 33, loss is 0.061360836\n",
      "epoch: 5 step: 34, loss is 0.059688747\n",
      "epoch: 5 step: 35, loss is 0.06069994\n",
      "epoch: 5 step: 36, loss is 0.053545\n",
      "epoch: 5 step: 37, loss is 0.07673496\n",
      "epoch: 5 step: 38, loss is 0.05946797\n",
      "epoch: 5 step: 39, loss is 0.06287336\n",
      "epoch: 5 step: 40, loss is 0.05482477\n",
      "epoch: 5 step: 41, loss is 0.0626263\n",
      "epoch: 5 step: 42, loss is 0.05440277\n",
      "epoch: 5 step: 43, loss is 0.080034554\n",
      "epoch: 5 step: 44, loss is 0.06605744\n",
      "epoch: 5 step: 45, loss is 0.056749046\n",
      "epoch: 5 step: 46, loss is 0.07177144\n",
      "epoch: 5 step: 47, loss is 0.04792863\n",
      "epoch: 5 step: 48, loss is 0.036362827\n",
      "epoch: 5 step: 49, loss is 0.0450601\n",
      "epoch: 5 step: 50, loss is 0.034817636\n",
      "epoch: 5 step: 51, loss is 0.040793598\n",
      "epoch: 5 step: 52, loss is 0.043782234\n",
      "epoch: 5 step: 53, loss is 0.047378957\n",
      "epoch: 5 step: 54, loss is 0.05196905\n",
      "epoch: 5 step: 55, loss is 0.04107994\n",
      "epoch: 5 step: 56, loss is 0.03462547\n",
      "epoch: 5 step: 57, loss is 0.039550602\n",
      "epoch: 5 step: 58, loss is 0.036476433\n",
      "epoch: 5 step: 59, loss is 0.050376356\n",
      "epoch: 5 step: 60, loss is 0.03310114\n",
      "epoch: 5 step: 61, loss is 0.038588822\n",
      "epoch: 5 step: 62, loss is 0.04122764\n",
      "epoch: 5 step: 63, loss is 0.048538625\n",
      "epoch: 5 step: 64, loss is 0.054811478\n",
      "epoch: 5 step: 65, loss is 0.051246464\n",
      "epoch: 5 step: 66, loss is 0.03063637\n",
      "epoch: 5 step: 67, loss is 0.036773145\n",
      "epoch: 5 step: 68, loss is 0.0373106\n",
      "epoch: 5 step: 69, loss is 0.045844078\n",
      "epoch: 5 step: 70, loss is 0.04229355\n",
      "epoch: 5 step: 71, loss is 0.040275097\n",
      "epoch: 5 step: 72, loss is 0.054621756\n",
      "epoch: 5 step: 73, loss is 0.042904556\n",
      "epoch: 5 step: 74, loss is 0.047952116\n",
      "epoch: 5 step: 75, loss is 0.047659338\n",
      "epoch: 5 step: 76, loss is 0.04174733\n",
      "epoch: 5 step: 77, loss is 0.051492214\n",
      "epoch: 5 step: 78, loss is 0.052476645\n",
      "epoch: 5 step: 79, loss is 0.07042831\n",
      "epoch: 5 step: 80, loss is 0.05335766\n",
      "epoch: 5 step: 81, loss is 0.044715405\n",
      "epoch: 5 step: 82, loss is -0.019256473\n",
      "epoch: 5 step: 83, loss is 0.056488693\n",
      "epoch: 5 step: 84, loss is 0.064970255\n",
      "epoch: 5 step: 85, loss is 0.055567324\n",
      "epoch: 5 step: 86, loss is 0.056240976\n",
      "epoch: 5 step: 87, loss is 0.060220957\n",
      "epoch: 5 step: 88, loss is 0.06298548\n",
      "epoch: 5 step: 89, loss is 0.06396836\n",
      "epoch: 5 step: 90, loss is 0.057668924\n",
      "epoch: 5 step: 91, loss is 0.064083636\n",
      "epoch: 5 step: 92, loss is 0.06937027\n",
      "epoch: 5 step: 93, loss is 0.053215444\n",
      "epoch: 5 step: 94, loss is 0.0516904\n",
      "epoch: 5 step: 95, loss is 0.048971117\n",
      "epoch: 5 step: 96, loss is 0.07302922\n",
      "epoch: 5 step: 97, loss is 0.07627016\n",
      "epoch: 5 step: 98, loss is 0.049504995\n",
      "epoch: 5 step: 99, loss is 0.056177914\n",
      "epoch: 5 step: 100, loss is 0.058193147\n",
      "epoch: 5 step: 101, loss is 0.06707239\n",
      "epoch: 5 step: 102, loss is 0.056493104\n",
      "epoch: 5 step: 103, loss is 0.064242065\n",
      "epoch: 5 step: 104, loss is 0.047478497\n",
      "epoch: 5 step: 105, loss is 0.065677345\n",
      "epoch: 5 step: 106, loss is 0.051977456\n",
      "epoch: 5 step: 107, loss is 0.052180767\n",
      "epoch: 5 step: 108, loss is 0.048790395\n",
      "epoch: 5 step: 109, loss is 0.06736308\n",
      "epoch: 5 step: 110, loss is 0.047079384\n",
      "epoch: 5 step: 111, loss is 0.06650907\n",
      "epoch: 5 step: 112, loss is 0.044957936\n",
      "epoch: 5 step: 113, loss is 0.05207151\n",
      "epoch: 5 step: 114, loss is 0.06244737\n",
      "epoch: 5 step: 115, loss is 0.0564937\n",
      "epoch: 5 step: 116, loss is 0.04442221\n",
      "epoch: 5 step: 117, loss is 0.042477608\n",
      "epoch: 5 step: 118, loss is 0.039995193\n",
      "epoch: 5 step: 119, loss is 0.05360192\n",
      "epoch: 5 step: 120, loss is 0.0646202\n",
      "epoch: 5 step: 121, loss is 0.063364565\n",
      "epoch: 5 step: 122, loss is 0.06267601\n",
      "epoch: 5 step: 123, loss is 0.046503246\n",
      "epoch: 5 step: 124, loss is 0.06844157\n",
      "epoch: 5 step: 125, loss is 0.07816619\n",
      "epoch: 5 step: 126, loss is 0.06940144\n",
      "epoch: 5 step: 127, loss is 0.060644448\n",
      "epoch: 5 step: 128, loss is 0.26890904\n",
      "epoch: 5 step: 129, loss is 0.06792855\n",
      "epoch: 5 step: 130, loss is 0.05217862\n",
      "epoch: 5 step: 131, loss is 0.06345481\n",
      "epoch: 5 step: 132, loss is 0.06791395\n",
      "epoch: 5 step: 133, loss is 0.070804894\n",
      "epoch: 5 step: 134, loss is 0.057328463\n",
      "epoch: 5 step: 135, loss is 0.056315362\n",
      "epoch: 5 step: 136, loss is 0.066120625\n",
      "epoch: 5 step: 137, loss is 0.059952736\n",
      "epoch: 5 step: 138, loss is 0.06803423\n",
      "epoch: 5 step: 139, loss is 0.070486724\n",
      "epoch: 5 step: 140, loss is 0.06652027\n",
      "epoch: 5 step: 141, loss is 0.06428689\n",
      "epoch: 5 step: 142, loss is 0.06601864\n",
      "epoch: 5 step: 143, loss is 0.08463496\n",
      "epoch: 5 step: 144, loss is 0.064224005\n",
      "epoch: 5 step: 145, loss is 0.07709998\n",
      "epoch: 5 step: 146, loss is 0.07589108\n",
      "epoch: 5 step: 147, loss is 0.073563874\n",
      "epoch: 5 step: 148, loss is 0.07625198\n",
      "epoch: 5 step: 149, loss is 0.070346534\n",
      "epoch: 5 step: 150, loss is 0.06939834\n",
      "epoch: 5 step: 151, loss is 0.0625698\n",
      "epoch: 5 step: 152, loss is 0.051421642\n",
      "epoch: 5 step: 153, loss is 0.023020029\n",
      "epoch: 5 step: 154, loss is 0.06921095\n",
      "epoch: 5 step: 155, loss is 0.057384074\n",
      "epoch: 5 step: 156, loss is 0.06213212\n",
      "epoch: 5 step: 157, loss is 0.074863255\n",
      "epoch: 5 step: 158, loss is 0.061795473\n",
      "epoch: 5 step: 159, loss is 0.073527336\n",
      "epoch: 5 step: 160, loss is 0.06824702\n",
      "epoch: 5 step: 161, loss is 0.048001587\n",
      "epoch: 5 step: 162, loss is 0.0024686456\n",
      "epoch: 5 step: 163, loss is 0.07605523\n",
      "epoch: 5 step: 164, loss is 0.063185036\n",
      "epoch: 5 step: 165, loss is 0.065335274\n",
      "epoch: 5 step: 166, loss is 0.06290358\n",
      "epoch: 5 step: 167, loss is 0.058465242\n",
      "epoch: 5 step: 168, loss is 0.07224327\n",
      "epoch: 5 step: 169, loss is 0.062339485\n",
      "epoch: 5 step: 170, loss is 0.07031518\n",
      "epoch: 5 step: 171, loss is 0.026873648\n",
      "epoch: 5 step: 172, loss is 0.065399945\n",
      "epoch: 5 step: 173, loss is 0.066329\n",
      "epoch: 5 step: 174, loss is 0.07200211\n",
      "epoch: 5 step: 175, loss is 0.06912631\n",
      "epoch: 5 step: 176, loss is 0.063480616\n",
      "epoch: 5 step: 177, loss is 0.07166523\n",
      "epoch: 5 step: 178, loss is 0.06239003\n",
      "epoch: 5 step: 179, loss is 0.07217139\n",
      "epoch: 5 step: 180, loss is 0.071469486\n",
      "epoch: 5 step: 181, loss is 0.0730837\n",
      "epoch: 5 step: 182, loss is 0.0697906\n",
      "epoch: 5 step: 183, loss is 0.059341192\n",
      "epoch: 5 step: 184, loss is 0.058921278\n",
      "epoch: 5 step: 185, loss is 0.06496638\n",
      "epoch: 5 step: 186, loss is 0.05805081\n",
      "epoch: 5 step: 187, loss is 0.06972772\n",
      "epoch: 5 step: 188, loss is 0.0854764\n",
      "epoch: 5 step: 189, loss is 0.083786905\n",
      "epoch: 5 step: 190, loss is 0.06800419\n",
      "epoch: 5 step: 191, loss is 0.052073\n",
      "epoch: 5 step: 192, loss is 0.07429165\n",
      "epoch: 5 step: 193, loss is 0.06445217\n",
      "epoch: 5 step: 194, loss is 0.063436985\n",
      "epoch: 5 step: 195, loss is 0.07448745\n",
      "epoch: 5 step: 196, loss is 0.07842159\n",
      "epoch: 5 step: 197, loss is 0.070856094\n",
      "epoch: 5 step: 198, loss is 0.06924111\n",
      "epoch: 5 step: 199, loss is 0.07861549\n",
      "epoch: 5 step: 200, loss is 0.061948836\n",
      "epoch: 5 step: 201, loss is 0.0614354\n",
      "epoch: 5 step: 202, loss is 0.061225474\n",
      "epoch: 5 step: 203, loss is 0.073792696\n",
      "epoch: 5 step: 204, loss is 0.05256349\n",
      "epoch: 5 step: 205, loss is -0.027293563\n",
      "epoch: 5 step: 206, loss is 0.06529707\n",
      "epoch: 5 step: 207, loss is 0.063818395\n",
      "epoch: 5 step: 208, loss is 0.057204247\n",
      "epoch: 5 step: 209, loss is 0.0770424\n",
      "epoch: 5 step: 210, loss is 0.06619531\n",
      "epoch: 5 step: 211, loss is 0.058306634\n",
      "epoch: 5 step: 212, loss is 0.050045013\n",
      "epoch: 5 step: 213, loss is 0.065475404\n",
      "epoch: 5 step: 214, loss is 0.028294623\n",
      "epoch: 5 step: 215, loss is 0.06241256\n",
      "epoch: 5 step: 216, loss is 0.07554293\n",
      "epoch: 5 step: 217, loss is 0.059499323\n",
      "epoch: 5 step: 218, loss is 0.14707941\n",
      "epoch: 5 step: 219, loss is 0.06570727\n",
      "epoch: 5 step: 220, loss is 0.066171646\n",
      "epoch: 5 step: 221, loss is 0.06333226\n",
      "epoch: 5 step: 222, loss is 0.06721467\n",
      "epoch: 5 step: 223, loss is 0.06782037\n",
      "epoch: 5 step: 224, loss is -0.00026643276\n",
      "epoch: 5 step: 225, loss is 0.06654185\n",
      "epoch: 5 step: 226, loss is 0.071095645\n",
      "epoch: 5 step: 227, loss is 0.0700385\n",
      "epoch: 5 step: 228, loss is 0.060065508\n",
      "epoch: 5 step: 229, loss is 0.07374972\n",
      "epoch: 5 step: 230, loss is 0.06704515\n",
      "epoch: 5 step: 231, loss is 0.0706411\n",
      "epoch: 5 step: 232, loss is 0.063058615\n",
      "epoch: 5 step: 233, loss is 0.06430727\n",
      "epoch: 5 step: 234, loss is 0.05659753\n",
      "epoch: 5 step: 235, loss is 0.073720515\n",
      "epoch: 5 step: 236, loss is 0.078172505\n",
      "epoch: 5 step: 237, loss is 0.075608194\n",
      "epoch: 5 step: 238, loss is 0.066657126\n",
      "epoch: 5 step: 239, loss is 0.07211691\n",
      "epoch: 5 step: 240, loss is 0.07163364\n",
      "epoch: 5 step: 241, loss is 0.077423334\n",
      "epoch: 5 step: 242, loss is 0.06322473\n",
      "epoch: 5 step: 243, loss is 0.06371474\n",
      "epoch: 5 step: 244, loss is 0.07549381\n",
      "epoch: 5 step: 245, loss is 0.06445795\n",
      "epoch: 5 step: 246, loss is 0.06447464\n",
      "epoch: 5 step: 247, loss is 0.06904888\n",
      "epoch: 5 step: 248, loss is 0.06594944\n",
      "epoch: 5 step: 249, loss is 0.058024943\n",
      "epoch: 5 step: 250, loss is 0.07035762\n",
      "epoch: 5 step: 251, loss is 0.070246935\n",
      "epoch: 5 step: 252, loss is 0.06818628\n",
      "epoch: 5 step: 253, loss is 0.06799978\n",
      "epoch: 5 step: 254, loss is 0.07213855\n",
      "epoch: 5 step: 255, loss is 0.071264565\n",
      "epoch: 5 step: 256, loss is 0.063735306\n",
      "epoch: 5 step: 257, loss is 0.07175845\n",
      "epoch: 5 step: 258, loss is 0.072867155\n",
      "epoch: 5 step: 259, loss is 0.06545907\n",
      "epoch: 5 step: 260, loss is 0.078256845\n",
      "epoch: 5 step: 261, loss is 0.069065034\n",
      "epoch: 5 step: 262, loss is 0.089458704\n",
      "epoch: 5 step: 263, loss is 0.064457595\n",
      "epoch: 5 step: 264, loss is 0.07104713\n",
      "epoch: 5 step: 265, loss is 0.06628972\n",
      "epoch: 5 step: 266, loss is 0.06726819\n",
      "epoch: 5 step: 267, loss is 0.07341313\n",
      "epoch: 5 step: 268, loss is 0.073427975\n",
      "epoch: 5 step: 269, loss is 0.070759475\n",
      "epoch: 5 step: 270, loss is 0.06830424\n",
      "epoch: 5 step: 271, loss is 0.064924896\n",
      "epoch: 5 step: 272, loss is 0.067533076\n",
      "epoch: 5 step: 273, loss is 0.06821662\n",
      "epoch: 5 step: 274, loss is 0.07681805\n",
      "epoch: 5 step: 275, loss is 0.06948668\n",
      "epoch: 5 step: 276, loss is 0.06532651\n",
      "epoch: 5 step: 277, loss is 0.06844807\n",
      "epoch: 5 step: 278, loss is 0.0697611\n",
      "epoch: 5 step: 279, loss is 0.07511455\n",
      "epoch: 5 step: 280, loss is 0.086170495\n",
      "epoch: 5 step: 281, loss is 0.07549435\n",
      "epoch: 5 step: 282, loss is 0.07205695\n",
      "epoch: 5 step: 283, loss is 0.07391578\n",
      "epoch: 5 step: 284, loss is 0.061507046\n",
      "epoch: 5 step: 285, loss is 0.07620853\n",
      "epoch: 5 step: 286, loss is 0.05919218\n",
      "epoch: 5 step: 287, loss is 0.068436146\n",
      "epoch: 5 step: 288, loss is 0.09214878\n",
      "epoch: 5 step: 289, loss is 0.07028264\n",
      "epoch: 5 step: 290, loss is 0.06598765\n",
      "epoch: 5 step: 291, loss is 0.05961317\n",
      "epoch: 5 step: 292, loss is 0.07077211\n",
      "epoch: 5 step: 293, loss is 0.072583854\n",
      "epoch: 5 step: 294, loss is 0.0710724\n",
      "epoch: 5 step: 295, loss is 0.0565086\n",
      "epoch: 5 step: 296, loss is 0.08385849\n",
      "epoch: 5 step: 297, loss is 0.059412897\n",
      "epoch: 5 step: 298, loss is 0.07591748\n",
      "epoch: 5 step: 299, loss is 0.058625937\n",
      "epoch: 5 step: 300, loss is 0.056877673\n",
      "epoch: 5 step: 301, loss is 0.07169515\n",
      "epoch: 5 step: 302, loss is 0.06286687\n",
      "epoch: 5 step: 303, loss is 0.08271009\n",
      "epoch: 5 step: 304, loss is 0.08126742\n",
      "epoch: 5 step: 305, loss is 0.08539599\n",
      "epoch: 5 step: 306, loss is 0.08677602\n",
      "epoch: 5 step: 307, loss is 0.07014185\n",
      "epoch: 5 step: 308, loss is 0.07791251\n",
      "epoch: 5 step: 309, loss is 0.06692624\n",
      "epoch: 5 step: 310, loss is 0.06823492\n",
      "epoch: 5 step: 311, loss is 0.07837844\n",
      "epoch: 5 step: 312, loss is 0.06065184\n",
      "epoch: 5 step: 313, loss is 0.066391945\n",
      "epoch: 5 step: 314, loss is 0.07120538\n",
      "epoch: 5 step: 315, loss is 0.06984252\n",
      "epoch: 5 step: 316, loss is 0.023831666\n",
      "epoch: 5 step: 317, loss is 0.07832408\n",
      "epoch: 5 step: 318, loss is 0.07522088\n",
      "epoch: 5 step: 319, loss is -0.009789348\n",
      "epoch: 5 step: 320, loss is 0.07027382\n",
      "epoch: 5 step: 321, loss is 0.06445736\n",
      "epoch: 5 step: 322, loss is 0.07981205\n",
      "epoch: 5 step: 323, loss is 0.0747053\n",
      "epoch: 5 step: 324, loss is 0.0797022\n",
      "epoch: 5 step: 325, loss is 0.07219988\n",
      "epoch: 5 step: 326, loss is 0.08225745\n",
      "epoch: 5 step: 327, loss is 0.0758428\n",
      "epoch: 5 step: 328, loss is 0.072178185\n",
      "epoch: 5 step: 329, loss is 0.0760265\n",
      "epoch: 5 step: 330, loss is 0.07314998\n",
      "epoch: 5 step: 331, loss is 0.07227224\n",
      "epoch: 5 step: 332, loss is 0.08160061\n",
      "epoch: 5 step: 333, loss is 0.068045914\n",
      "epoch: 5 step: 334, loss is 0.07745379\n",
      "epoch: 5 step: 335, loss is 0.07940668\n",
      "epoch: 5 step: 336, loss is 0.07321042\n",
      "epoch: 5 step: 337, loss is 0.06666303\n",
      "epoch: 5 step: 338, loss is 0.08231181\n",
      "epoch: 5 step: 339, loss is 0.08455104\n",
      "epoch: 5 step: 340, loss is 0.07788795\n",
      "epoch: 5 step: 341, loss is 0.077901185\n",
      "epoch: 5 step: 342, loss is 0.08407861\n",
      "epoch: 5 step: 343, loss is -0.035030484\n",
      "epoch: 5 step: 344, loss is 0.0629549\n",
      "epoch: 5 step: 345, loss is 0.078648984\n",
      "epoch: 5 step: 346, loss is 0.07495338\n",
      "epoch: 5 step: 347, loss is 0.11976749\n",
      "epoch: 5 step: 348, loss is 0.072598934\n",
      "epoch: 5 step: 349, loss is 0.06916237\n",
      "epoch: 5 step: 350, loss is 0.07638568\n",
      "epoch: 5 step: 351, loss is 0.06548411\n",
      "epoch: 5 step: 352, loss is 0.088044465\n",
      "epoch: 5 step: 353, loss is 0.07808834\n",
      "epoch: 5 step: 354, loss is 0.06926602\n",
      "epoch: 5 step: 355, loss is 0.08113366\n",
      "epoch: 5 step: 356, loss is 0.07313341\n",
      "epoch: 5 step: 357, loss is 0.073122025\n",
      "epoch: 5 step: 358, loss is 0.08240032\n",
      "epoch: 5 step: 359, loss is 0.0665248\n",
      "epoch: 5 step: 360, loss is 0.081146\n",
      "epoch: 5 step: 361, loss is 0.067937136\n",
      "epoch: 5 step: 362, loss is 0.07801676\n",
      "epoch: 5 step: 363, loss is 0.08167893\n",
      "epoch: 5 step: 364, loss is 0.07787633\n",
      "epoch: 5 step: 365, loss is 0.07863778\n",
      "epoch: 5 step: 366, loss is 0.077228844\n",
      "epoch: 5 step: 367, loss is 0.0764547\n",
      "epoch: 5 step: 368, loss is 0.08980304\n",
      "epoch: 5 step: 369, loss is 0.07899594\n",
      "epoch: 5 step: 370, loss is 0.08605242\n",
      "epoch: 5 step: 371, loss is 0.06714934\n",
      "epoch: 5 step: 372, loss is 0.068534195\n",
      "epoch: 5 step: 373, loss is 0.07476491\n",
      "epoch: 5 step: 374, loss is 0.0811041\n",
      "epoch: 5 step: 375, loss is 0.073938906\n",
      "epoch: 5 step: 376, loss is 0.059779227\n",
      "epoch: 5 step: 377, loss is 0.078632176\n",
      "epoch: 5 step: 378, loss is 0.077311516\n",
      "epoch: 5 step: 379, loss is 0.07506132\n",
      "epoch: 5 step: 380, loss is 0.07737261\n",
      "epoch: 5 step: 381, loss is 0.07959396\n",
      "epoch: 5 step: 382, loss is 0.06545085\n",
      "epoch: 5 step: 383, loss is 0.10533279\n",
      "epoch: 5 step: 384, loss is 0.076269686\n",
      "epoch: 5 step: 385, loss is 0.07553345\n",
      "epoch: 5 step: 386, loss is 0.075024605\n",
      "epoch: 5 step: 387, loss is 0.073987186\n",
      "epoch: 5 step: 388, loss is 0.074560344\n",
      "epoch: 5 step: 389, loss is 0.07725167\n",
      "epoch: 5 step: 390, loss is 0.043507397\n",
      "epoch: 5 step: 391, loss is 0.07721108\n",
      "epoch: 5 step: 392, loss is 0.07066232\n",
      "epoch: 5 step: 393, loss is 0.07694751\n",
      "epoch: 5 step: 394, loss is 0.06707424\n",
      "epoch: 5 step: 395, loss is 0.08197212\n",
      "epoch: 5 step: 396, loss is 0.07929534\n",
      "epoch: 5 step: 397, loss is 0.06741929\n",
      "epoch: 5 step: 398, loss is 0.06725937\n",
      "epoch: 5 step: 399, loss is 0.07220411\n",
      "epoch: 5 step: 400, loss is 0.07053524\n",
      "epoch: 5 step: 401, loss is 0.072487056\n",
      "epoch: 5 step: 402, loss is 0.06306934\n",
      "epoch: 5 step: 403, loss is 0.07425803\n",
      "epoch: 5 step: 404, loss is 0.06825876\n",
      "epoch: 5 step: 405, loss is 0.051231682\n",
      "epoch: 5 step: 406, loss is 0.063061416\n",
      "epoch: 5 step: 407, loss is 0.06994611\n",
      "epoch: 5 step: 408, loss is 0.059512854\n",
      "epoch: 5 step: 409, loss is 0.06783992\n",
      "epoch: 5 step: 410, loss is 0.077711105\n",
      "epoch: 5 step: 411, loss is 0.06941748\n",
      "epoch: 5 step: 412, loss is 0.06312585\n",
      "epoch: 5 step: 413, loss is 0.08227533\n",
      "epoch: 5 step: 414, loss is 0.05375886\n",
      "epoch: 5 step: 415, loss is 0.06863922\n",
      "epoch: 5 step: 416, loss is 0.07313794\n",
      "epoch: 5 step: 417, loss is 0.07331592\n",
      "epoch: 5 step: 418, loss is 0.07577604\n",
      "epoch: 5 step: 419, loss is 0.068293095\n",
      "epoch: 5 step: 420, loss is 0.07044029\n",
      "epoch: 5 step: 421, loss is 0.0748772\n",
      "epoch: 5 step: 422, loss is 0.075697124\n",
      "epoch: 5 step: 423, loss is 0.07507354\n",
      "epoch: 5 step: 424, loss is 0.06865209\n",
      "epoch: 5 step: 425, loss is 0.06497699\n",
      "epoch: 5 step: 426, loss is 0.06664592\n",
      "epoch: 5 step: 427, loss is 0.0667935\n",
      "epoch: 5 step: 428, loss is 0.07843137\n",
      "epoch: 5 step: 429, loss is 0.07225829\n",
      "epoch: 5 step: 430, loss is 0.068068564\n",
      "epoch: 5 step: 431, loss is 0.0609591\n",
      "epoch: 5 step: 432, loss is 0.09506613\n",
      "epoch: 5 step: 433, loss is 0.0700984\n",
      "epoch: 5 step: 434, loss is 0.083123386\n",
      "epoch: 5 step: 435, loss is 0.068336785\n",
      "epoch: 5 step: 436, loss is 0.061226904\n",
      "epoch: 5 step: 437, loss is 0.05788356\n",
      "epoch: 5 step: 438, loss is 0.04136461\n",
      "epoch: 5 step: 439, loss is 0.07243091\n",
      "epoch: 5 step: 440, loss is 0.0628255\n",
      "epoch: 5 step: 441, loss is 0.031398058\n",
      "epoch: 5 step: 442, loss is 0.035544455\n",
      "epoch: 5 step: 443, loss is 0.034027874\n",
      "epoch: 5 step: 444, loss is 0.044153392\n",
      "epoch: 5 step: 445, loss is 0.058037102\n",
      "epoch: 5 step: 446, loss is 0.06264782\n",
      "epoch: 5 step: 447, loss is 0.036771536\n",
      "epoch: 5 step: 448, loss is 0.05701518\n",
      "epoch: 5 step: 449, loss is 0.061618626\n",
      "epoch: 5 step: 450, loss is 0.046592295\n",
      "epoch: 5 step: 451, loss is 0.03304988\n",
      "epoch: 5 step: 452, loss is 0.060703814\n",
      "epoch: 5 step: 453, loss is 0.03755659\n",
      "epoch: 5 step: 454, loss is 0.06080866\n",
      "epoch: 5 step: 455, loss is 0.056256115\n",
      "epoch: 5 step: 456, loss is 0.03662795\n",
      "epoch: 5 step: 457, loss is 0.03045535\n",
      "epoch: 5 step: 458, loss is 0.057584763\n",
      "epoch: 5 step: 459, loss is 0.0426116\n",
      "epoch: 5 step: 460, loss is 0.052798986\n",
      "epoch: 5 step: 461, loss is 0.04706478\n",
      "epoch: 5 step: 462, loss is 0.05245638\n",
      "epoch: 5 step: 463, loss is 0.075807035\n",
      "epoch: 5 step: 464, loss is 0.068734586\n",
      "epoch: 5 step: 465, loss is 0.05744487\n",
      "epoch: 5 step: 466, loss is 0.07032251\n",
      "epoch: 5 step: 467, loss is 0.07289624\n",
      "epoch: 5 step: 468, loss is 0.060845435\n",
      "epoch: 5 step: 469, loss is 0.08770728\n",
      "epoch: 5 step: 470, loss is 0.070510626\n",
      "epoch: 5 step: 471, loss is 0.058938503\n",
      "epoch: 5 step: 472, loss is -0.050954938\n",
      "epoch: 5 step: 473, loss is 0.06620377\n",
      "epoch: 5 step: 474, loss is 0.054654896\n",
      "epoch: 5 step: 475, loss is 0.04573065\n",
      "epoch: 5 step: 476, loss is 0.04243344\n",
      "epoch: 5 step: 477, loss is 0.050598204\n",
      "epoch: 5 step: 478, loss is 0.057668865\n",
      "epoch: 5 step: 479, loss is 0.06789285\n",
      "epoch: 5 step: 480, loss is 0.044934332\n",
      "epoch: 5 step: 481, loss is 0.056068122\n",
      "epoch: 5 step: 482, loss is 0.059467733\n",
      "epoch: 5 step: 483, loss is 0.060355186\n",
      "epoch: 5 step: 484, loss is 0.03781748\n",
      "epoch: 5 step: 485, loss is 0.05569321\n",
      "epoch: 5 step: 486, loss is 0.057677925\n",
      "epoch: 5 step: 487, loss is 0.047503293\n",
      "epoch: 5 step: 488, loss is 0.0587731\n",
      "epoch: 5 step: 489, loss is 0.047973692\n",
      "epoch: 5 step: 490, loss is 0.09694058\n",
      "epoch: 5 step: 491, loss is 0.047899067\n",
      "epoch: 5 step: 492, loss is 0.037145793\n",
      "epoch: 5 step: 493, loss is 0.05545932\n",
      "epoch: 5 step: 494, loss is 0.058378875\n",
      "epoch: 5 step: 495, loss is 0.05973673\n",
      "epoch: 5 step: 496, loss is 0.07092714\n",
      "epoch: 5 step: 497, loss is 0.074664295\n",
      "epoch: 5 step: 498, loss is 0.057389498\n",
      "epoch: 5 step: 499, loss is 0.061231315\n",
      "epoch: 5 step: 500, loss is 0.077952206\n",
      "epoch: 5 step: 501, loss is 0.0674184\n",
      "epoch: 5 step: 502, loss is 0.06444955\n",
      "epoch: 5 step: 503, loss is 0.081543684\n",
      "epoch: 5 step: 504, loss is 0.056353986\n",
      "epoch: 5 step: 505, loss is 0.070623875\n",
      "epoch: 5 step: 506, loss is 0.057354152\n",
      "epoch: 5 step: 507, loss is 0.0592615\n",
      "epoch: 5 step: 508, loss is 0.061736286\n",
      "epoch: 5 step: 509, loss is 0.07574201\n",
      "epoch: 5 step: 510, loss is 0.07225293\n",
      "epoch: 5 step: 511, loss is 0.07127124\n",
      "epoch: 5 step: 512, loss is 0.07636517\n",
      "epoch: 5 step: 513, loss is 0.06010294\n",
      "epoch: 5 step: 514, loss is 0.089138746\n",
      "epoch: 5 step: 515, loss is 0.06613088\n",
      "epoch: 5 step: 516, loss is 0.061877787\n",
      "epoch: 5 step: 517, loss is 0.07694739\n",
      "epoch: 5 step: 518, loss is 0.060614347\n",
      "epoch: 5 step: 519, loss is 0.064926565\n",
      "epoch: 5 step: 520, loss is 0.07511264\n",
      "epoch: 5 step: 521, loss is 0.069411516\n",
      "epoch: 5 step: 522, loss is 0.0494048\n",
      "epoch: 5 step: 523, loss is 0.053011894\n",
      "epoch: 5 step: 524, loss is 0.07612467\n",
      "epoch: 5 step: 525, loss is 0.08790344\n",
      "epoch: 5 step: 526, loss is 0.081728995\n",
      "epoch: 5 step: 527, loss is 0.06722641\n",
      "epoch: 5 step: 528, loss is 0.07015234\n",
      "epoch: 5 step: 529, loss is 0.06953734\n",
      "epoch: 5 step: 530, loss is 0.09366101\n",
      "epoch: 5 step: 531, loss is -0.0634383\n",
      "epoch: 5 step: 532, loss is 0.08078927\n",
      "epoch: 5 step: 533, loss is 0.06715745\n",
      "epoch: 5 step: 534, loss is 0.078830004\n",
      "epoch: 5 step: 535, loss is 0.042535484\n",
      "epoch: 5 step: 536, loss is 0.087429225\n",
      "epoch: 5 step: 537, loss is 0.055897057\n",
      "epoch: 5 step: 538, loss is 0.057245016\n",
      "epoch: 5 step: 539, loss is 0.077452004\n",
      "epoch: 5 step: 540, loss is 0.05990523\n",
      "epoch: 5 step: 541, loss is 0.06802511\n",
      "epoch: 5 step: 542, loss is 0.07245064\n",
      "epoch: 5 step: 543, loss is 0.08725673\n",
      "epoch: 5 step: 544, loss is 0.066153586\n",
      "epoch: 5 step: 545, loss is 0.05578822\n",
      "epoch: 5 step: 546, loss is 0.057669878\n",
      "epoch: 5 step: 547, loss is 0.072390854\n",
      "epoch: 5 step: 548, loss is 0.059220493\n",
      "epoch: 5 step: 549, loss is 0.081858575\n",
      "epoch: 5 step: 550, loss is 0.076500416\n",
      "epoch: 5 step: 551, loss is 0.06675506\n",
      "epoch: 5 step: 552, loss is 0.076894045\n",
      "epoch: 5 step: 553, loss is 0.061603367\n",
      "epoch: 5 step: 554, loss is 0.076625824\n",
      "epoch: 5 step: 555, loss is 0.046729863\n",
      "epoch: 5 step: 556, loss is 0.06255728\n",
      "epoch: 5 step: 557, loss is 0.05460787\n",
      "epoch: 5 step: 558, loss is 0.052694798\n",
      "epoch: 5 step: 559, loss is 0.090842545\n",
      "epoch: 5 step: 560, loss is 0.061936677\n",
      "epoch: 5 step: 561, loss is 0.05295086\n",
      "epoch: 5 step: 562, loss is 0.078122795\n",
      "epoch: 5 step: 563, loss is 0.058717012\n",
      "epoch: 5 step: 564, loss is 0.06771153\n",
      "epoch: 5 step: 565, loss is 0.064279735\n",
      "epoch: 5 step: 566, loss is 0.08127898\n",
      "epoch: 5 step: 567, loss is 0.066859245\n",
      "epoch: 5 step: 568, loss is 0.0856424\n",
      "epoch: 5 step: 569, loss is 0.053702056\n",
      "epoch: 5 step: 570, loss is 0.06515378\n",
      "epoch: 5 step: 571, loss is 0.068579376\n",
      "epoch: 5 step: 572, loss is 0.06201005\n",
      "epoch: 5 step: 573, loss is 0.049756825\n",
      "epoch: 5 step: 574, loss is 0.064679146\n",
      "epoch: 5 step: 575, loss is 0.066205025\n",
      "epoch: 5 step: 576, loss is 0.07963437\n",
      "epoch: 5 step: 577, loss is 0.0701372\n",
      "epoch: 5 step: 578, loss is 0.061627865\n",
      "epoch: 5 step: 579, loss is 0.06607628\n",
      "epoch: 5 step: 580, loss is 0.0708127\n",
      "epoch: 5 step: 581, loss is 0.08678347\n",
      "epoch: 5 step: 582, loss is 0.06983882\n",
      "epoch: 5 step: 583, loss is 0.081229866\n",
      "epoch: 5 step: 584, loss is 0.06633711\n",
      "epoch: 5 step: 585, loss is 0.06344944\n",
      "epoch: 5 step: 586, loss is 0.082233906\n",
      "epoch: 5 step: 587, loss is 0.066658914\n",
      "epoch: 5 step: 588, loss is 0.07970637\n",
      "epoch: 5 step: 589, loss is 0.0658831\n",
      "epoch: 5 step: 590, loss is 0.058989227\n",
      "epoch: 5 step: 591, loss is 0.083497524\n",
      "epoch: 5 step: 592, loss is 0.06921625\n",
      "epoch: 5 step: 593, loss is 0.07480788\n",
      "epoch: 5 step: 594, loss is 0.054124355\n",
      "epoch: 5 step: 595, loss is 0.050103962\n",
      "epoch: 5 step: 596, loss is 0.049725115\n",
      "epoch: 5 step: 597, loss is 0.07052189\n",
      "epoch: 5 step: 598, loss is 0.06836492\n",
      "epoch: 5 step: 599, loss is 0.0597592\n",
      "epoch: 5 step: 600, loss is 0.077079594\n",
      "epoch: 5 step: 601, loss is 0.0750311\n",
      "epoch: 5 step: 602, loss is 0.0691731\n",
      "epoch: 5 step: 603, loss is 0.07367295\n",
      "epoch: 5 step: 604, loss is 0.06689626\n",
      "epoch: 5 step: 605, loss is 0.06658602\n",
      "epoch: 5 step: 606, loss is 0.022959411\n",
      "epoch: 5 step: 607, loss is 0.07171887\n",
      "epoch: 5 step: 608, loss is 0.076385915\n",
      "epoch: 5 step: 609, loss is 0.07377952\n",
      "epoch: 5 step: 610, loss is 0.072821915\n",
      "epoch: 5 step: 611, loss is 0.065627635\n",
      "epoch: 5 step: 612, loss is 0.06509799\n",
      "epoch: 5 step: 613, loss is 0.078967094\n",
      "epoch: 5 step: 614, loss is 0.07025772\n",
      "epoch: 5 step: 615, loss is 0.065433085\n",
      "epoch: 5 step: 616, loss is 0.07277471\n",
      "epoch: 5 step: 617, loss is 0.06330013\n",
      "epoch: 5 step: 618, loss is 0.066159725\n",
      "epoch: 5 step: 619, loss is 0.070858955\n",
      "epoch: 5 step: 620, loss is 0.0790689\n",
      "epoch: 5 step: 621, loss is 0.061949253\n",
      "epoch: 5 step: 622, loss is 0.0679245\n",
      "epoch: 5 step: 623, loss is 0.07142264\n",
      "epoch: 5 step: 624, loss is 0.074572325\n",
      "epoch: 5 step: 625, loss is 0.04537368\n",
      "epoch: 5 step: 626, loss is 0.08007985\n",
      "epoch: 5 step: 627, loss is 0.070325136\n",
      "epoch: 5 step: 628, loss is 0.082723916\n",
      "epoch: 5 step: 629, loss is 0.073536575\n",
      "epoch: 5 step: 630, loss is 0.061837435\n",
      "epoch: 5 step: 631, loss is 0.07654858\n",
      "epoch: 5 step: 632, loss is 0.08335614\n",
      "epoch: 5 step: 633, loss is 0.07785058\n",
      "epoch: 5 step: 634, loss is 0.072871864\n",
      "epoch: 5 step: 635, loss is 0.07582122\n",
      "epoch: 5 step: 636, loss is 0.07297945\n",
      "epoch: 5 step: 637, loss is 0.07713574\n",
      "epoch: 5 step: 638, loss is 0.0748294\n",
      "epoch: 5 step: 639, loss is 0.07326341\n",
      "epoch: 5 step: 640, loss is 0.07160044\n",
      "epoch: 5 step: 641, loss is 0.072440326\n",
      "epoch: 5 step: 642, loss is 0.0693028\n",
      "epoch: 5 step: 643, loss is 0.08192676\n",
      "epoch: 5 step: 644, loss is 0.07448244\n",
      "epoch: 5 step: 645, loss is 0.064846754\n",
      "epoch: 5 step: 646, loss is 0.07180482\n",
      "epoch: 5 step: 647, loss is 0.06408328\n",
      "epoch: 5 step: 648, loss is 0.07088566\n",
      "epoch: 5 step: 649, loss is 0.08373588\n",
      "epoch: 5 step: 650, loss is 0.07348746\n",
      "epoch: 5 step: 651, loss is 0.085044205\n",
      "epoch: 5 step: 652, loss is 0.070899904\n",
      "epoch: 5 step: 653, loss is 0.06342536\n",
      "epoch: 5 step: 654, loss is 0.072072506\n",
      "epoch: 5 step: 655, loss is 0.077105284\n",
      "epoch: 5 step: 656, loss is 0.086080015\n",
      "epoch: 5 step: 657, loss is 0.07446045\n",
      "epoch: 5 step: 658, loss is 0.12322897\n",
      "epoch: 5 step: 659, loss is 0.074514806\n",
      "epoch: 5 step: 660, loss is 0.072184086\n",
      "epoch: 5 step: 661, loss is 0.072609484\n",
      "epoch: 5 step: 662, loss is 0.07755619\n",
      "epoch: 5 step: 663, loss is 0.07540703\n",
      "epoch: 5 step: 664, loss is 0.068403244\n",
      "epoch: 5 step: 665, loss is 0.077020705\n",
      "epoch: 5 step: 666, loss is 0.07200354\n",
      "epoch: 5 step: 667, loss is 0.07623172\n",
      "epoch: 5 step: 668, loss is 0.07819891\n",
      "epoch: 5 step: 669, loss is 0.08013439\n",
      "epoch: 5 step: 670, loss is 0.08116126\n",
      "epoch: 5 step: 671, loss is 0.06761235\n",
      "epoch: 5 step: 672, loss is 0.07476634\n",
      "epoch: 5 step: 673, loss is 0.072745025\n",
      "epoch: 5 step: 674, loss is 0.0734027\n",
      "epoch: 5 step: 675, loss is 0.08057815\n",
      "epoch: 5 step: 676, loss is 0.081346214\n",
      "epoch: 5 step: 677, loss is 0.07553524\n",
      "epoch: 5 step: 678, loss is 0.075214446\n",
      "epoch: 5 step: 679, loss is 3.269409\n",
      "epoch: 5 step: 680, loss is 0.06367284\n",
      "epoch: 5 step: 681, loss is 0.06817603\n",
      "epoch: 5 step: 682, loss is 0.0760327\n",
      "epoch: 5 step: 683, loss is 0.07521933\n",
      "epoch: 5 step: 684, loss is 0.068966925\n",
      "epoch: 5 step: 685, loss is 0.0728752\n",
      "epoch: 5 step: 686, loss is 0.06728846\n",
      "epoch: 5 step: 687, loss is 0.062163353\n",
      "epoch: 5 step: 688, loss is 0.066106796\n",
      "epoch: 5 step: 689, loss is 0.07788497\n",
      "epoch: 5 step: 690, loss is 0.070285976\n",
      "epoch: 5 step: 691, loss is 0.07986021\n",
      "epoch: 5 step: 692, loss is 0.0698784\n",
      "epoch: 5 step: 693, loss is 0.06652862\n",
      "epoch: 5 step: 694, loss is 0.07722241\n",
      "epoch: 5 step: 695, loss is 0.072751224\n",
      "epoch: 5 step: 696, loss is 0.0751645\n",
      "epoch: 5 step: 697, loss is 0.081677675\n",
      "epoch: 5 step: 698, loss is 0.060296237\n",
      "epoch: 5 step: 699, loss is 0.07306975\n",
      "epoch: 5 step: 700, loss is 0.07221365\n",
      "epoch: 5 step: 701, loss is 0.058226585\n",
      "epoch: 5 step: 702, loss is 0.06772375\n",
      "epoch: 5 step: 703, loss is 0.068259716\n",
      "epoch: 5 step: 704, loss is 0.07012898\n",
      "epoch: 5 step: 705, loss is 0.0727005\n",
      "epoch: 5 step: 706, loss is 0.06305814\n",
      "epoch: 5 step: 707, loss is 0.08101624\n",
      "epoch: 5 step: 708, loss is 0.070062935\n",
      "epoch: 5 step: 709, loss is 0.073991\n",
      "epoch: 5 step: 710, loss is 0.08011359\n",
      "epoch: 5 step: 711, loss is 0.07586986\n",
      "epoch: 5 step: 712, loss is 0.07019657\n",
      "epoch: 5 step: 713, loss is 0.070926845\n",
      "epoch: 5 step: 714, loss is 0.07265043\n",
      "epoch: 5 step: 715, loss is 0.070421875\n",
      "epoch: 5 step: 716, loss is 0.07402134\n",
      "epoch: 5 step: 717, loss is 0.07479358\n",
      "epoch: 5 step: 718, loss is 0.07451016\n",
      "epoch: 5 step: 719, loss is 0.072316825\n",
      "epoch: 5 step: 720, loss is 50.645718\n",
      "epoch: 5 step: 721, loss is 0.062136352\n",
      "epoch: 5 step: 722, loss is 0.08068228\n",
      "epoch: 5 step: 723, loss is 0.081192076\n",
      "epoch: 5 step: 724, loss is 0.08021021\n",
      "epoch: 5 step: 725, loss is 0.06975597\n",
      "epoch: 5 step: 726, loss is 0.08117604\n",
      "epoch: 5 step: 727, loss is 0.07782984\n",
      "epoch: 5 step: 728, loss is 0.06806451\n",
      "epoch: 5 step: 729, loss is 0.05869943\n",
      "epoch: 5 step: 730, loss is 0.067492425\n",
      "epoch: 5 step: 731, loss is 0.077623904\n",
      "epoch: 5 step: 732, loss is 0.07611585\n",
      "epoch: 5 step: 733, loss is 0.07605392\n",
      "epoch: 5 step: 734, loss is 0.07144481\n",
      "epoch: 5 step: 735, loss is 0.0659914\n",
      "epoch: 5 step: 736, loss is 0.07804519\n",
      "epoch: 5 step: 737, loss is 0.068843305\n",
      "epoch: 5 step: 738, loss is 0.076191485\n",
      "epoch: 5 step: 739, loss is 0.059963226\n",
      "epoch: 5 step: 740, loss is 0.07922602\n",
      "epoch: 5 step: 741, loss is 0.06772596\n",
      "epoch: 5 step: 742, loss is 0.07073766\n",
      "epoch: 5 step: 743, loss is 0.07706547\n",
      "epoch: 5 step: 744, loss is 0.06888741\n",
      "epoch: 5 step: 745, loss is 0.07309848\n",
      "epoch: 5 step: 746, loss is 0.08082801\n",
      "epoch: 5 step: 747, loss is 0.072963715\n",
      "epoch: 5 step: 748, loss is 0.069826424\n",
      "epoch: 5 step: 749, loss is 0.07897717\n",
      "epoch: 5 step: 750, loss is 0.075568855\n",
      "epoch: 5 step: 751, loss is 0.07843542\n",
      "epoch: 5 step: 752, loss is 0.055404663\n",
      "epoch: 5 step: 753, loss is 0.07156491\n",
      "epoch: 5 step: 754, loss is 0.0815649\n",
      "epoch: 5 step: 755, loss is 0.07979751\n",
      "epoch: 5 step: 756, loss is 0.06330007\n",
      "epoch: 5 step: 757, loss is 0.07107544\n",
      "epoch: 5 step: 758, loss is 0.070635796\n",
      "epoch: 5 step: 759, loss is 0.06762475\n",
      "epoch: 5 step: 760, loss is 0.076633394\n",
      "epoch: 5 step: 761, loss is 0.060512125\n",
      "epoch: 5 step: 762, loss is 0.075375795\n",
      "epoch: 5 step: 763, loss is 0.07243234\n",
      "epoch: 5 step: 764, loss is 0.06776303\n",
      "epoch: 5 step: 765, loss is 0.07670385\n",
      "epoch: 5 step: 766, loss is 0.074287236\n",
      "epoch: 5 step: 767, loss is 0.07471937\n",
      "epoch: 5 step: 768, loss is 0.054729283\n",
      "epoch: 5 step: 769, loss is 0.06852293\n",
      "epoch: 5 step: 770, loss is 0.07331753\n",
      "epoch: 5 step: 771, loss is 0.07053834\n",
      "epoch: 5 step: 772, loss is 0.07219249\n",
      "epoch: 5 step: 773, loss is 0.07931882\n",
      "epoch: 5 step: 774, loss is 0.07050568\n",
      "epoch: 5 step: 775, loss is 0.083367825\n",
      "epoch: 6 step: 1, loss is 0.08003688\n",
      "epoch: 6 step: 2, loss is 0.08072835\n",
      "epoch: 6 step: 3, loss is 0.07272607\n",
      "epoch: 6 step: 4, loss is 0.051265478\n",
      "epoch: 6 step: 5, loss is 0.07367164\n",
      "epoch: 6 step: 6, loss is 0.068404436\n",
      "epoch: 6 step: 7, loss is 0.077316046\n",
      "epoch: 6 step: 8, loss is 0.07382393\n",
      "epoch: 6 step: 9, loss is 0.08968896\n",
      "epoch: 6 step: 10, loss is 0.069957435\n",
      "epoch: 6 step: 11, loss is 0.06492454\n",
      "epoch: 6 step: 12, loss is 0.07568914\n",
      "epoch: 6 step: 13, loss is 0.05831462\n",
      "epoch: 6 step: 14, loss is 0.058047056\n",
      "epoch: 6 step: 15, loss is 0.0789752\n",
      "epoch: 6 step: 16, loss is 0.07023239\n",
      "epoch: 6 step: 17, loss is 0.07472372\n",
      "epoch: 6 step: 18, loss is 0.069156826\n",
      "epoch: 6 step: 19, loss is 0.07128757\n",
      "epoch: 6 step: 20, loss is 0.07718688\n",
      "epoch: 6 step: 21, loss is 0.07671118\n",
      "epoch: 6 step: 22, loss is 0.07313913\n",
      "epoch: 6 step: 23, loss is 0.07523757\n",
      "epoch: 6 step: 24, loss is 0.0694167\n",
      "epoch: 6 step: 25, loss is 0.06343633\n",
      "epoch: 6 step: 26, loss is 0.07258147\n",
      "epoch: 6 step: 27, loss is 0.06247425\n",
      "epoch: 6 step: 28, loss is 0.072458684\n",
      "epoch: 6 step: 29, loss is 0.07429391\n",
      "epoch: 6 step: 30, loss is 0.07698327\n",
      "epoch: 6 step: 31, loss is 0.07224321\n",
      "epoch: 6 step: 32, loss is 0.06618166\n",
      "epoch: 6 step: 33, loss is 0.08792496\n",
      "epoch: 6 step: 34, loss is 0.078431785\n",
      "epoch: 6 step: 35, loss is 0.06726056\n",
      "epoch: 6 step: 36, loss is 0.07218844\n",
      "epoch: 6 step: 37, loss is 0.077377856\n",
      "epoch: 6 step: 38, loss is 0.081186235\n",
      "epoch: 6 step: 39, loss is 0.07382345\n",
      "epoch: 6 step: 40, loss is 0.07037014\n",
      "epoch: 6 step: 41, loss is 0.05779189\n",
      "epoch: 6 step: 42, loss is 0.07482022\n",
      "epoch: 6 step: 43, loss is 0.07762009\n",
      "epoch: 6 step: 44, loss is 0.08054143\n",
      "epoch: 6 step: 45, loss is 0.07252264\n",
      "epoch: 6 step: 46, loss is 0.079250276\n",
      "epoch: 6 step: 47, loss is 0.08139855\n",
      "epoch: 6 step: 48, loss is 0.0747115\n",
      "epoch: 6 step: 49, loss is 0.063126564\n",
      "epoch: 6 step: 50, loss is 0.07248145\n",
      "epoch: 6 step: 51, loss is 0.07144481\n",
      "epoch: 6 step: 52, loss is 0.07529783\n",
      "epoch: 6 step: 53, loss is 0.07640284\n",
      "epoch: 6 step: 54, loss is 0.07243508\n",
      "epoch: 6 step: 55, loss is 0.075377285\n",
      "epoch: 6 step: 56, loss is 0.06955582\n",
      "epoch: 6 step: 57, loss is 0.07289058\n",
      "epoch: 6 step: 58, loss is 0.064986885\n",
      "epoch: 6 step: 59, loss is 0.06342775\n",
      "epoch: 6 step: 60, loss is 0.06926459\n",
      "epoch: 6 step: 61, loss is 0.06626564\n",
      "epoch: 6 step: 62, loss is 0.07529974\n",
      "epoch: 6 step: 63, loss is 0.078902066\n",
      "epoch: 6 step: 64, loss is 0.07327968\n",
      "epoch: 6 step: 65, loss is 0.06956071\n",
      "epoch: 6 step: 66, loss is 0.07489854\n",
      "epoch: 6 step: 67, loss is 0.07337165\n",
      "epoch: 6 step: 68, loss is 0.07398844\n",
      "epoch: 6 step: 69, loss is 0.06444979\n",
      "epoch: 6 step: 70, loss is 0.06567043\n",
      "epoch: 6 step: 71, loss is 0.07023746\n",
      "epoch: 6 step: 72, loss is 0.071344376\n",
      "epoch: 6 step: 73, loss is 0.06764406\n",
      "epoch: 6 step: 74, loss is 0.07096857\n",
      "epoch: 6 step: 75, loss is 0.071516216\n",
      "epoch: 6 step: 76, loss is 0.07447106\n",
      "epoch: 6 step: 77, loss is 0.084050596\n",
      "epoch: 6 step: 78, loss is 0.07761377\n",
      "epoch: 6 step: 79, loss is 0.11156875\n",
      "epoch: 6 step: 80, loss is 0.06804228\n",
      "epoch: 6 step: 81, loss is 0.06676549\n",
      "epoch: 6 step: 82, loss is 0.069951\n",
      "epoch: 6 step: 83, loss is -0.020693421\n",
      "epoch: 6 step: 84, loss is 0.069689095\n",
      "epoch: 6 step: 85, loss is 0.07095307\n",
      "epoch: 6 step: 86, loss is 0.0756585\n",
      "epoch: 6 step: 87, loss is 0.07044715\n",
      "epoch: 6 step: 88, loss is 0.0678218\n",
      "epoch: 6 step: 89, loss is 0.08230877\n",
      "epoch: 6 step: 90, loss is 0.06719321\n",
      "epoch: 6 step: 91, loss is 0.07422799\n",
      "epoch: 6 step: 92, loss is 0.07625753\n",
      "epoch: 6 step: 93, loss is 0.07129854\n",
      "epoch: 6 step: 94, loss is 0.05781603\n",
      "epoch: 6 step: 95, loss is 0.07324505\n",
      "epoch: 6 step: 96, loss is 0.070182025\n",
      "epoch: 6 step: 97, loss is 0.068436325\n",
      "epoch: 6 step: 98, loss is 0.06766194\n",
      "epoch: 6 step: 99, loss is 0.07570022\n",
      "epoch: 6 step: 100, loss is 0.06445509\n",
      "epoch: 6 step: 101, loss is 0.06776142\n",
      "epoch: 6 step: 102, loss is 0.06612116\n",
      "epoch: 6 step: 103, loss is 0.06677818\n",
      "epoch: 6 step: 104, loss is 0.07113105\n",
      "epoch: 6 step: 105, loss is 0.07725936\n",
      "epoch: 6 step: 106, loss is 0.06890041\n",
      "epoch: 6 step: 107, loss is 0.07652569\n",
      "epoch: 6 step: 108, loss is 0.06511903\n",
      "epoch: 6 step: 109, loss is 0.0689829\n",
      "epoch: 6 step: 110, loss is 0.07044023\n",
      "epoch: 6 step: 111, loss is 0.07464355\n",
      "epoch: 6 step: 112, loss is 0.07002598\n",
      "epoch: 6 step: 113, loss is 0.07355499\n",
      "epoch: 6 step: 114, loss is 0.065231085\n",
      "epoch: 6 step: 115, loss is 0.081926346\n",
      "epoch: 6 step: 116, loss is 0.073349655\n",
      "epoch: 6 step: 117, loss is 0.06480628\n",
      "epoch: 6 step: 118, loss is 0.06778008\n",
      "epoch: 6 step: 119, loss is 0.067513764\n",
      "epoch: 6 step: 120, loss is 0.06607348\n",
      "epoch: 6 step: 121, loss is 0.06817365\n",
      "epoch: 6 step: 122, loss is 0.089313984\n",
      "epoch: 6 step: 123, loss is 0.08475304\n",
      "epoch: 6 step: 124, loss is 0.07235241\n",
      "epoch: 6 step: 125, loss is 0.0659073\n",
      "epoch: 6 step: 126, loss is 0.0831089\n",
      "epoch: 6 step: 127, loss is 0.07662487\n",
      "epoch: 6 step: 128, loss is 0.08460981\n",
      "epoch: 6 step: 129, loss is 0.07835609\n",
      "epoch: 6 step: 130, loss is 0.07010239\n",
      "epoch: 6 step: 131, loss is 0.06803131\n",
      "epoch: 6 step: 132, loss is 0.0666911\n",
      "epoch: 6 step: 133, loss is 0.07218665\n",
      "epoch: 6 step: 134, loss is 0.06432694\n",
      "epoch: 6 step: 135, loss is 0.06582618\n",
      "epoch: 6 step: 136, loss is 0.07807273\n",
      "epoch: 6 step: 137, loss is 0.07507688\n",
      "epoch: 6 step: 138, loss is 0.0725739\n",
      "epoch: 6 step: 139, loss is 0.0709579\n",
      "epoch: 6 step: 140, loss is 0.07350379\n",
      "epoch: 6 step: 141, loss is 0.07765609\n",
      "epoch: 6 step: 142, loss is 0.08171272\n",
      "epoch: 6 step: 143, loss is 0.067679465\n",
      "epoch: 6 step: 144, loss is 0.0729959\n",
      "epoch: 6 step: 145, loss is 0.06343025\n",
      "epoch: 6 step: 146, loss is 0.06943852\n",
      "epoch: 6 step: 147, loss is 0.07586002\n",
      "epoch: 6 step: 148, loss is 0.076895\n",
      "epoch: 6 step: 149, loss is 0.07664895\n",
      "epoch: 6 step: 150, loss is 0.06844854\n",
      "epoch: 6 step: 151, loss is 0.107577324\n",
      "epoch: 6 step: 152, loss is 0.076328516\n",
      "epoch: 6 step: 153, loss is 0.07674813\n",
      "epoch: 6 step: 154, loss is 0.08359736\n",
      "epoch: 6 step: 155, loss is -0.008980036\n",
      "epoch: 6 step: 156, loss is 0.068212986\n",
      "epoch: 6 step: 157, loss is 0.053088427\n",
      "epoch: 6 step: 158, loss is 0.080197334\n",
      "epoch: 6 step: 159, loss is 0.062444866\n",
      "epoch: 6 step: 160, loss is 0.05943948\n",
      "epoch: 6 step: 161, loss is 0.05749196\n",
      "epoch: 6 step: 162, loss is 0.07982904\n",
      "epoch: 6 step: 163, loss is 0.07168007\n",
      "epoch: 6 step: 164, loss is 0.091338575\n",
      "epoch: 6 step: 165, loss is 0.055057704\n",
      "epoch: 6 step: 166, loss is 0.070216715\n",
      "epoch: 6 step: 167, loss is 0.07601148\n",
      "epoch: 6 step: 168, loss is 0.0758093\n",
      "epoch: 6 step: 169, loss is 0.078169286\n",
      "epoch: 6 step: 170, loss is 0.07869977\n",
      "epoch: 6 step: 171, loss is 0.068053186\n",
      "epoch: 6 step: 172, loss is 0.06562644\n",
      "epoch: 6 step: 173, loss is 0.060708225\n",
      "epoch: 6 step: 174, loss is 0.051829576\n",
      "epoch: 6 step: 175, loss is 0.068995774\n",
      "epoch: 6 step: 176, loss is 0.081769764\n",
      "epoch: 6 step: 177, loss is 0.06942606\n",
      "epoch: 6 step: 178, loss is 0.06699538\n",
      "epoch: 6 step: 179, loss is 0.06958151\n",
      "epoch: 6 step: 180, loss is 0.07107687\n",
      "epoch: 6 step: 181, loss is 0.08164334\n",
      "epoch: 6 step: 182, loss is 0.05155182\n",
      "epoch: 6 step: 183, loss is 0.08100766\n",
      "epoch: 6 step: 184, loss is 0.07129663\n",
      "epoch: 6 step: 185, loss is 0.087831795\n",
      "epoch: 6 step: 186, loss is 0.06884414\n",
      "epoch: 6 step: 187, loss is 0.07544702\n",
      "epoch: 6 step: 188, loss is 0.066271305\n",
      "epoch: 6 step: 189, loss is 0.063925326\n",
      "epoch: 6 step: 190, loss is 0.07195026\n",
      "epoch: 6 step: 191, loss is 0.08068055\n",
      "epoch: 6 step: 192, loss is 0.06995183\n",
      "epoch: 6 step: 193, loss is 0.076173246\n",
      "epoch: 6 step: 194, loss is 0.07813668\n",
      "epoch: 6 step: 195, loss is 0.08711213\n",
      "epoch: 6 step: 196, loss is 0.06700951\n",
      "epoch: 6 step: 197, loss is 0.078632355\n",
      "epoch: 6 step: 198, loss is 0.06972712\n",
      "epoch: 6 step: 199, loss is 0.071172714\n",
      "epoch: 6 step: 200, loss is 0.07130796\n",
      "epoch: 6 step: 201, loss is 0.06813735\n",
      "epoch: 6 step: 202, loss is 0.08256483\n",
      "epoch: 6 step: 203, loss is 0.06494802\n",
      "epoch: 6 step: 204, loss is 0.07760543\n",
      "epoch: 6 step: 205, loss is 0.12992197\n",
      "epoch: 6 step: 206, loss is 0.066441596\n",
      "epoch: 6 step: 207, loss is 0.060752094\n",
      "epoch: 6 step: 208, loss is 0.073538125\n",
      "epoch: 6 step: 209, loss is 0.07328081\n",
      "epoch: 6 step: 210, loss is 0.07106084\n",
      "epoch: 6 step: 211, loss is 0.07510036\n",
      "epoch: 6 step: 212, loss is 0.06428462\n",
      "epoch: 6 step: 213, loss is 0.086866796\n",
      "epoch: 6 step: 214, loss is 0.06402302\n",
      "epoch: 6 step: 215, loss is 0.08335352\n",
      "epoch: 6 step: 216, loss is 0.063144684\n",
      "epoch: 6 step: 217, loss is 0.065894604\n",
      "epoch: 6 step: 218, loss is 0.081641495\n",
      "epoch: 6 step: 219, loss is 0.07580757\n",
      "epoch: 6 step: 220, loss is 0.06940478\n",
      "epoch: 6 step: 221, loss is 0.07943916\n",
      "epoch: 6 step: 222, loss is 0.08456737\n",
      "epoch: 6 step: 223, loss is 0.07460576\n",
      "epoch: 6 step: 224, loss is 0.07061821\n",
      "epoch: 6 step: 225, loss is -0.09603357\n",
      "epoch: 6 step: 226, loss is 0.08008289\n",
      "epoch: 6 step: 227, loss is 0.07151371\n",
      "epoch: 6 step: 228, loss is 0.08097482\n",
      "epoch: 6 step: 229, loss is 0.03599149\n",
      "epoch: 6 step: 230, loss is 0.06980866\n",
      "epoch: 6 step: 231, loss is 0.081315696\n",
      "epoch: 6 step: 232, loss is 0.079312146\n",
      "epoch: 6 step: 233, loss is 0.09415418\n",
      "epoch: 6 step: 234, loss is 0.069331646\n",
      "epoch: 6 step: 235, loss is 0.0751493\n",
      "epoch: 6 step: 236, loss is 0.08262867\n",
      "epoch: 6 step: 237, loss is 0.08022517\n",
      "epoch: 6 step: 238, loss is 0.07791543\n",
      "epoch: 6 step: 239, loss is 0.0754239\n",
      "epoch: 6 step: 240, loss is 0.070525825\n",
      "epoch: 6 step: 241, loss is 0.076218665\n",
      "epoch: 6 step: 242, loss is 0.06861812\n",
      "epoch: 6 step: 243, loss is 0.09023398\n",
      "epoch: 6 step: 244, loss is 0.07218909\n",
      "epoch: 6 step: 245, loss is 0.09132463\n",
      "epoch: 6 step: 246, loss is 0.08591241\n",
      "epoch: 6 step: 247, loss is 0.07767105\n",
      "epoch: 6 step: 248, loss is 0.08412117\n",
      "epoch: 6 step: 249, loss is 0.08176613\n",
      "epoch: 6 step: 250, loss is 0.08093315\n",
      "epoch: 6 step: 251, loss is 0.06866288\n",
      "epoch: 6 step: 252, loss is 0.07557911\n",
      "epoch: 6 step: 253, loss is 0.07454687\n",
      "epoch: 6 step: 254, loss is 0.07468635\n",
      "epoch: 6 step: 255, loss is 0.06925732\n",
      "epoch: 6 step: 256, loss is 0.087537706\n",
      "epoch: 6 step: 257, loss is 0.06788778\n",
      "epoch: 6 step: 258, loss is 0.07773894\n",
      "epoch: 6 step: 259, loss is 0.07951683\n",
      "epoch: 6 step: 260, loss is 0.08801532\n",
      "epoch: 6 step: 261, loss is 0.070442855\n",
      "epoch: 6 step: 262, loss is 0.06862563\n",
      "epoch: 6 step: 263, loss is 0.07257813\n",
      "epoch: 6 step: 264, loss is 0.0731532\n",
      "epoch: 6 step: 265, loss is 0.091122806\n",
      "epoch: 6 step: 266, loss is 0.07522684\n",
      "epoch: 6 step: 267, loss is 0.08338851\n",
      "epoch: 6 step: 268, loss is 0.0701465\n",
      "epoch: 6 step: 269, loss is 0.0876711\n",
      "epoch: 6 step: 270, loss is 0.074834764\n",
      "epoch: 6 step: 271, loss is 0.07557827\n",
      "epoch: 6 step: 272, loss is 0.06699473\n",
      "epoch: 6 step: 273, loss is 0.0720256\n",
      "epoch: 6 step: 274, loss is 0.078446686\n",
      "epoch: 6 step: 275, loss is 0.07244879\n",
      "epoch: 6 step: 276, loss is 0.080118895\n",
      "epoch: 6 step: 277, loss is 0.079929054\n",
      "epoch: 6 step: 278, loss is 0.06442994\n",
      "epoch: 6 step: 279, loss is 0.07627267\n",
      "epoch: 6 step: 280, loss is 0.07963437\n",
      "epoch: 6 step: 281, loss is 0.06627256\n",
      "epoch: 6 step: 282, loss is 0.07200813\n",
      "epoch: 6 step: 283, loss is 0.086839736\n",
      "epoch: 6 step: 284, loss is 0.07878131\n",
      "epoch: 6 step: 285, loss is 0.076498985\n",
      "epoch: 6 step: 286, loss is 0.07895738\n",
      "epoch: 6 step: 287, loss is 0.07777053\n",
      "epoch: 6 step: 288, loss is 0.13550752\n",
      "epoch: 6 step: 289, loss is 0.058050334\n",
      "epoch: 6 step: 290, loss is 0.07001352\n",
      "epoch: 6 step: 291, loss is 0.06645733\n",
      "epoch: 6 step: 292, loss is 0.076728284\n",
      "epoch: 6 step: 293, loss is 0.07356405\n",
      "epoch: 6 step: 294, loss is 0.052721977\n",
      "epoch: 6 step: 295, loss is 0.083434165\n",
      "epoch: 6 step: 296, loss is 0.06998044\n",
      "epoch: 6 step: 297, loss is 0.058849752\n",
      "epoch: 6 step: 298, loss is 0.063919604\n",
      "epoch: 6 step: 299, loss is 0.07075262\n",
      "epoch: 6 step: 300, loss is 0.067252636\n",
      "epoch: 6 step: 301, loss is 0.11488539\n",
      "epoch: 6 step: 302, loss is 0.05204028\n",
      "epoch: 6 step: 303, loss is 0.0683319\n",
      "epoch: 6 step: 304, loss is 0.074995816\n",
      "epoch: 6 step: 305, loss is 0.07121354\n",
      "epoch: 6 step: 306, loss is 0.058425844\n",
      "epoch: 6 step: 307, loss is 0.06710273\n",
      "epoch: 6 step: 308, loss is 0.07748622\n",
      "epoch: 6 step: 309, loss is 0.0056762695\n",
      "epoch: 6 step: 310, loss is 0.08104724\n",
      "epoch: 6 step: 311, loss is 0.0799036\n",
      "epoch: 6 step: 312, loss is 0.07977384\n",
      "epoch: 6 step: 313, loss is 0.25978565\n",
      "epoch: 6 step: 314, loss is 0.08423895\n",
      "epoch: 6 step: 315, loss is 0.07237655\n",
      "epoch: 6 step: 316, loss is 0.07930565\n",
      "epoch: 6 step: 317, loss is 0.07565212\n",
      "epoch: 6 step: 318, loss is 0.07682514\n",
      "epoch: 6 step: 319, loss is 0.08636004\n",
      "epoch: 6 step: 320, loss is 0.077540934\n",
      "epoch: 6 step: 321, loss is 0.07257414\n",
      "epoch: 6 step: 322, loss is 0.07377595\n",
      "epoch: 6 step: 323, loss is 0.0782637\n",
      "epoch: 6 step: 324, loss is 0.079282105\n",
      "epoch: 6 step: 325, loss is 0.06872612\n",
      "epoch: 6 step: 326, loss is 0.07937747\n",
      "epoch: 6 step: 327, loss is 0.08372921\n",
      "epoch: 6 step: 328, loss is 0.07283646\n",
      "epoch: 6 step: 329, loss is 0.08433628\n",
      "epoch: 6 step: 330, loss is 0.07867831\n",
      "epoch: 6 step: 331, loss is 0.07986599\n",
      "epoch: 6 step: 332, loss is 0.06460118\n",
      "epoch: 6 step: 333, loss is 0.07021457\n",
      "epoch: 6 step: 334, loss is 0.07115048\n",
      "epoch: 6 step: 335, loss is 0.07533693\n",
      "epoch: 6 step: 336, loss is 0.07083589\n",
      "epoch: 6 step: 337, loss is 0.07480407\n",
      "epoch: 6 step: 338, loss is 0.09889251\n",
      "epoch: 6 step: 339, loss is 0.08416158\n",
      "epoch: 6 step: 340, loss is 0.07478118\n",
      "epoch: 6 step: 341, loss is 0.08440018\n",
      "epoch: 6 step: 342, loss is 0.082650185\n",
      "epoch: 6 step: 343, loss is 0.07380265\n",
      "epoch: 6 step: 344, loss is 0.072900474\n",
      "epoch: 6 step: 345, loss is 0.073601305\n",
      "epoch: 6 step: 346, loss is 0.07031959\n",
      "epoch: 6 step: 347, loss is 0.0777061\n",
      "epoch: 6 step: 348, loss is 0.07688403\n",
      "epoch: 6 step: 349, loss is 0.08219653\n",
      "epoch: 6 step: 350, loss is 0.06850499\n",
      "epoch: 6 step: 351, loss is 0.078265965\n",
      "epoch: 6 step: 352, loss is 0.07877183\n",
      "epoch: 6 step: 353, loss is 0.08179742\n",
      "epoch: 6 step: 354, loss is 0.0818792\n",
      "epoch: 6 step: 355, loss is 0.08161503\n",
      "epoch: 6 step: 356, loss is 0.07953662\n",
      "epoch: 6 step: 357, loss is 0.07731104\n",
      "epoch: 6 step: 358, loss is 0.07583958\n",
      "epoch: 6 step: 359, loss is 0.06397885\n",
      "epoch: 6 step: 360, loss is 0.07276851\n",
      "epoch: 6 step: 361, loss is 0.07627958\n",
      "epoch: 6 step: 362, loss is 0.07484102\n",
      "epoch: 6 step: 363, loss is 0.07055813\n",
      "epoch: 6 step: 364, loss is 0.06370878\n",
      "epoch: 6 step: 365, loss is 0.06562036\n",
      "epoch: 6 step: 366, loss is 0.07638234\n",
      "epoch: 6 step: 367, loss is 0.07861215\n",
      "epoch: 6 step: 368, loss is 0.057243884\n",
      "epoch: 6 step: 369, loss is 0.098931134\n",
      "epoch: 6 step: 370, loss is 0.06082076\n",
      "epoch: 6 step: 371, loss is 0.061276138\n",
      "epoch: 6 step: 372, loss is 0.06198603\n",
      "epoch: 6 step: 373, loss is 0.06989026\n",
      "epoch: 6 step: 374, loss is 0.07691932\n",
      "epoch: 6 step: 375, loss is 0.07129097\n",
      "epoch: 6 step: 376, loss is 0.064932406\n",
      "epoch: 6 step: 377, loss is 0.06727457\n",
      "epoch: 6 step: 378, loss is 0.07230711\n",
      "epoch: 6 step: 379, loss is 0.09187573\n",
      "epoch: 6 step: 380, loss is 0.07896024\n",
      "epoch: 6 step: 381, loss is 0.06710482\n",
      "epoch: 6 step: 382, loss is 0.06883186\n",
      "epoch: 6 step: 383, loss is 0.0689044\n",
      "epoch: 6 step: 384, loss is 0.07839507\n",
      "epoch: 6 step: 385, loss is 0.07302159\n",
      "epoch: 6 step: 386, loss is 0.08003837\n",
      "epoch: 6 step: 387, loss is 0.07442933\n",
      "epoch: 6 step: 388, loss is 0.09411925\n",
      "epoch: 6 step: 389, loss is 0.066921055\n",
      "epoch: 6 step: 390, loss is 0.06691021\n",
      "epoch: 6 step: 391, loss is 0.06879181\n",
      "epoch: 6 step: 392, loss is 0.0826931\n",
      "epoch: 6 step: 393, loss is 0.06380874\n",
      "epoch: 6 step: 394, loss is 0.063097775\n",
      "epoch: 6 step: 395, loss is 0.063058555\n",
      "epoch: 6 step: 396, loss is 0.05933094\n",
      "epoch: 6 step: 397, loss is 0.08057475\n",
      "epoch: 6 step: 398, loss is 0.06139356\n",
      "epoch: 6 step: 399, loss is 0.09499711\n",
      "epoch: 6 step: 400, loss is 0.06511444\n",
      "epoch: 6 step: 401, loss is 0.08824253\n",
      "epoch: 6 step: 402, loss is 0.074496746\n",
      "epoch: 6 step: 403, loss is 0.072244585\n",
      "epoch: 6 step: 404, loss is 0.07827312\n",
      "epoch: 6 step: 405, loss is 0.07931566\n",
      "epoch: 6 step: 406, loss is 0.08321613\n",
      "epoch: 6 step: 407, loss is 0.07517868\n",
      "epoch: 6 step: 408, loss is 0.08381957\n",
      "epoch: 6 step: 409, loss is 0.064165294\n",
      "epoch: 6 step: 410, loss is 0.08270949\n",
      "epoch: 6 step: 411, loss is 0.06406355\n",
      "epoch: 6 step: 412, loss is 0.07722473\n",
      "epoch: 6 step: 413, loss is 0.07070547\n",
      "epoch: 6 step: 414, loss is 0.08029604\n",
      "epoch: 6 step: 415, loss is 0.09714502\n",
      "epoch: 6 step: 416, loss is 0.07571602\n",
      "epoch: 6 step: 417, loss is 0.09204513\n",
      "epoch: 6 step: 418, loss is 0.07978773\n",
      "epoch: 6 step: 419, loss is 0.06367636\n",
      "epoch: 6 step: 420, loss is 0.07316375\n",
      "epoch: 6 step: 421, loss is 0.07055044\n",
      "epoch: 6 step: 422, loss is 0.07664579\n",
      "epoch: 6 step: 423, loss is 0.07766056\n",
      "epoch: 6 step: 424, loss is 0.08227438\n",
      "epoch: 6 step: 425, loss is 0.07887238\n",
      "epoch: 6 step: 426, loss is 0.06793517\n",
      "epoch: 6 step: 427, loss is 0.06791824\n",
      "epoch: 6 step: 428, loss is 0.08639097\n",
      "epoch: 6 step: 429, loss is 0.07757974\n",
      "epoch: 6 step: 430, loss is 0.05186534\n",
      "epoch: 6 step: 431, loss is -0.09247208\n",
      "epoch: 6 step: 432, loss is 0.091991425\n",
      "epoch: 6 step: 433, loss is 0.066128075\n",
      "epoch: 6 step: 434, loss is 0.0782395\n",
      "epoch: 6 step: 435, loss is 0.082758665\n",
      "epoch: 6 step: 436, loss is 0.080002606\n",
      "epoch: 6 step: 437, loss is 0.0818069\n",
      "epoch: 6 step: 438, loss is 0.07868272\n",
      "epoch: 6 step: 439, loss is 0.07789534\n",
      "epoch: 6 step: 440, loss is 0.08750695\n",
      "epoch: 6 step: 441, loss is 0.08828205\n",
      "epoch: 6 step: 442, loss is 0.07687092\n",
      "epoch: 6 step: 443, loss is 0.08055061\n",
      "epoch: 6 step: 444, loss is 0.07806224\n",
      "epoch: 6 step: 445, loss is 0.08799285\n",
      "epoch: 6 step: 446, loss is 0.07816988\n",
      "epoch: 6 step: 447, loss is 0.084992886\n",
      "epoch: 6 step: 448, loss is 0.07588333\n",
      "epoch: 6 step: 449, loss is 0.09017259\n",
      "epoch: 6 step: 450, loss is 0.085166216\n",
      "epoch: 6 step: 451, loss is 0.07646561\n",
      "epoch: 6 step: 452, loss is 0.07496506\n",
      "epoch: 6 step: 453, loss is 0.07910067\n",
      "epoch: 6 step: 454, loss is 0.07517713\n",
      "epoch: 6 step: 455, loss is 0.082351446\n",
      "epoch: 6 step: 456, loss is 0.080434024\n",
      "epoch: 6 step: 457, loss is 0.084022045\n",
      "epoch: 6 step: 458, loss is 0.07566351\n",
      "epoch: 6 step: 459, loss is 0.071573675\n",
      "epoch: 6 step: 460, loss is 0.08449596\n",
      "epoch: 6 step: 461, loss is 0.09019297\n",
      "epoch: 6 step: 462, loss is 0.08074349\n",
      "epoch: 6 step: 463, loss is 0.08200246\n",
      "epoch: 6 step: 464, loss is 0.08040857\n",
      "epoch: 6 step: 465, loss is 0.082868576\n",
      "epoch: 6 step: 466, loss is 0.0901804\n",
      "epoch: 6 step: 467, loss is 0.091807365\n",
      "epoch: 6 step: 468, loss is 0.08786011\n",
      "epoch: 6 step: 469, loss is 0.08037013\n",
      "epoch: 6 step: 470, loss is 0.086716115\n",
      "epoch: 6 step: 471, loss is 0.07963294\n",
      "epoch: 6 step: 472, loss is 0.09122568\n",
      "epoch: 6 step: 473, loss is 0.079441845\n",
      "epoch: 6 step: 474, loss is 0.090045154\n",
      "epoch: 6 step: 475, loss is 0.08251077\n",
      "epoch: 6 step: 476, loss is 0.085585296\n",
      "epoch: 6 step: 477, loss is 0.08508116\n",
      "epoch: 6 step: 478, loss is 0.081175625\n",
      "epoch: 6 step: 479, loss is 0.08270335\n",
      "epoch: 6 step: 480, loss is 0.07387233\n",
      "epoch: 6 step: 481, loss is 0.08112818\n",
      "epoch: 6 step: 482, loss is 0.0862115\n",
      "epoch: 6 step: 483, loss is 0.07267398\n",
      "epoch: 6 step: 484, loss is 0.094293535\n",
      "epoch: 6 step: 485, loss is 0.08678037\n",
      "epoch: 6 step: 486, loss is 0.07922155\n",
      "epoch: 6 step: 487, loss is 0.08513361\n",
      "epoch: 6 step: 488, loss is 0.08074659\n",
      "epoch: 6 step: 489, loss is 0.080057144\n",
      "epoch: 6 step: 490, loss is 0.08034927\n",
      "epoch: 6 step: 491, loss is 0.07891065\n",
      "epoch: 6 step: 492, loss is 0.08029503\n",
      "epoch: 6 step: 493, loss is 0.08102894\n",
      "epoch: 6 step: 494, loss is 0.078419745\n",
      "epoch: 6 step: 495, loss is -0.15814936\n",
      "epoch: 6 step: 496, loss is 0.06826562\n",
      "epoch: 6 step: 497, loss is 0.08649784\n",
      "epoch: 6 step: 498, loss is 0.07638216\n",
      "epoch: 6 step: 499, loss is 0.07927036\n",
      "epoch: 6 step: 500, loss is 0.08382648\n",
      "epoch: 6 step: 501, loss is 0.06423116\n",
      "epoch: 6 step: 502, loss is 0.08076525\n",
      "epoch: 6 step: 503, loss is 0.073711395\n",
      "epoch: 6 step: 504, loss is 0.07372171\n",
      "epoch: 6 step: 505, loss is 0.07698566\n",
      "epoch: 6 step: 506, loss is 0.068819046\n",
      "epoch: 6 step: 507, loss is 0.08948606\n",
      "epoch: 6 step: 508, loss is 0.072375536\n",
      "epoch: 6 step: 509, loss is 0.07360858\n",
      "epoch: 6 step: 510, loss is 0.063887775\n",
      "epoch: 6 step: 511, loss is 0.0782097\n",
      "epoch: 6 step: 512, loss is 0.07386726\n",
      "epoch: 6 step: 513, loss is 0.08889872\n",
      "epoch: 6 step: 514, loss is 0.06672287\n",
      "epoch: 6 step: 515, loss is 0.08031559\n",
      "epoch: 6 step: 516, loss is 0.08352703\n",
      "epoch: 6 step: 517, loss is 0.083495855\n",
      "epoch: 6 step: 518, loss is 0.07927132\n",
      "epoch: 6 step: 519, loss is 0.07522893\n",
      "epoch: 6 step: 520, loss is 0.06883854\n",
      "epoch: 6 step: 521, loss is 0.08175373\n",
      "epoch: 6 step: 522, loss is 0.06803471\n",
      "epoch: 6 step: 523, loss is 0.065686226\n",
      "epoch: 6 step: 524, loss is 0.07616216\n",
      "epoch: 6 step: 525, loss is 0.080593765\n",
      "epoch: 6 step: 526, loss is 0.08086127\n",
      "epoch: 6 step: 527, loss is 0.072887\n",
      "epoch: 6 step: 528, loss is 0.07561064\n",
      "epoch: 6 step: 529, loss is 0.0871765\n",
      "epoch: 6 step: 530, loss is 0.0743162\n",
      "epoch: 6 step: 531, loss is 0.07328534\n",
      "epoch: 6 step: 532, loss is 0.07731128\n",
      "epoch: 6 step: 533, loss is 0.07285243\n",
      "epoch: 6 step: 534, loss is 0.08464259\n",
      "epoch: 6 step: 535, loss is 0.08125776\n",
      "epoch: 6 step: 536, loss is 0.07962489\n",
      "epoch: 6 step: 537, loss is 0.068256676\n",
      "epoch: 6 step: 538, loss is 0.077543795\n",
      "epoch: 6 step: 539, loss is 0.073369265\n",
      "epoch: 6 step: 540, loss is 0.075581074\n",
      "epoch: 6 step: 541, loss is 0.072362006\n",
      "epoch: 6 step: 542, loss is 0.07187635\n",
      "epoch: 6 step: 543, loss is 0.070361614\n",
      "epoch: 6 step: 544, loss is 0.08074826\n",
      "epoch: 6 step: 545, loss is 0.098867714\n",
      "epoch: 6 step: 546, loss is 0.07554072\n",
      "epoch: 6 step: 547, loss is 0.072681904\n",
      "epoch: 6 step: 548, loss is 0.06825286\n",
      "epoch: 6 step: 549, loss is 0.06967139\n",
      "epoch: 6 step: 550, loss is 0.07975626\n",
      "epoch: 6 step: 551, loss is 0.07226151\n",
      "epoch: 6 step: 552, loss is 0.069683135\n",
      "epoch: 6 step: 553, loss is 0.07898933\n",
      "epoch: 6 step: 554, loss is 0.08645016\n",
      "epoch: 6 step: 555, loss is 0.081273556\n",
      "epoch: 6 step: 556, loss is 0.07336956\n",
      "epoch: 6 step: 557, loss is 0.07898837\n",
      "epoch: 6 step: 558, loss is 0.074151754\n",
      "epoch: 6 step: 559, loss is 0.08627623\n",
      "epoch: 6 step: 560, loss is 0.08271915\n",
      "epoch: 6 step: 561, loss is 0.073426306\n",
      "epoch: 6 step: 562, loss is 0.080504715\n",
      "epoch: 6 step: 563, loss is 0.07593471\n",
      "epoch: 6 step: 564, loss is 0.08569795\n",
      "epoch: 6 step: 565, loss is 0.071976244\n",
      "epoch: 6 step: 566, loss is 0.0824337\n",
      "epoch: 6 step: 567, loss is 0.089725316\n",
      "epoch: 6 step: 568, loss is 0.06846732\n",
      "epoch: 6 step: 569, loss is 0.06885338\n",
      "epoch: 6 step: 570, loss is 0.081475735\n",
      "epoch: 6 step: 571, loss is 0.0736475\n",
      "epoch: 6 step: 572, loss is 0.0677883\n",
      "epoch: 6 step: 573, loss is 0.06574756\n",
      "epoch: 6 step: 574, loss is 0.076316535\n",
      "epoch: 6 step: 575, loss is 0.08054966\n",
      "epoch: 6 step: 576, loss is 0.07239121\n",
      "epoch: 6 step: 577, loss is 0.077172935\n",
      "epoch: 6 step: 578, loss is 0.0689587\n",
      "epoch: 6 step: 579, loss is 0.06824654\n",
      "epoch: 6 step: 580, loss is 0.07330257\n",
      "epoch: 6 step: 581, loss is 0.063272655\n",
      "epoch: 6 step: 582, loss is 0.08528519\n",
      "epoch: 6 step: 583, loss is 0.086239815\n",
      "epoch: 6 step: 584, loss is 0.07019758\n",
      "epoch: 6 step: 585, loss is 0.06809777\n",
      "epoch: 6 step: 586, loss is 0.07054186\n",
      "epoch: 6 step: 587, loss is 0.075062275\n",
      "epoch: 6 step: 588, loss is 0.07118338\n",
      "epoch: 6 step: 589, loss is 0.06895566\n",
      "epoch: 6 step: 590, loss is 0.06665355\n",
      "epoch: 6 step: 591, loss is 0.069093525\n",
      "epoch: 6 step: 592, loss is 0.07671577\n",
      "epoch: 6 step: 593, loss is 0.078859806\n",
      "epoch: 6 step: 594, loss is 0.089052856\n",
      "epoch: 6 step: 595, loss is 0.05828172\n",
      "epoch: 6 step: 596, loss is 0.07511073\n",
      "epoch: 6 step: 597, loss is 0.07090062\n",
      "epoch: 6 step: 598, loss is 0.068702936\n",
      "epoch: 6 step: 599, loss is 0.11748457\n",
      "epoch: 6 step: 600, loss is 0.060874045\n",
      "epoch: 6 step: 601, loss is 0.07115203\n",
      "epoch: 6 step: 602, loss is 0.0744521\n",
      "epoch: 6 step: 603, loss is 0.058945715\n",
      "epoch: 6 step: 604, loss is 0.070812166\n",
      "epoch: 6 step: 605, loss is 0.0801599\n",
      "epoch: 6 step: 606, loss is 0.078728855\n",
      "epoch: 6 step: 607, loss is 0.07021445\n",
      "epoch: 6 step: 608, loss is 0.07093197\n",
      "epoch: 6 step: 609, loss is 0.07112414\n",
      "epoch: 6 step: 610, loss is 0.07402992\n",
      "epoch: 6 step: 611, loss is 0.08602643\n",
      "epoch: 6 step: 612, loss is 0.08103752\n",
      "epoch: 6 step: 613, loss is 0.13461477\n",
      "epoch: 6 step: 614, loss is 0.07688755\n",
      "epoch: 6 step: 615, loss is 0.09551352\n",
      "epoch: 6 step: 616, loss is 1.9938836\n",
      "epoch: 6 step: 617, loss is 0.08423716\n",
      "epoch: 6 step: 618, loss is 0.076542675\n",
      "epoch: 6 step: 619, loss is 0.07932979\n",
      "epoch: 6 step: 620, loss is 0.07930857\n",
      "epoch: 6 step: 621, loss is 0.07677668\n",
      "epoch: 6 step: 622, loss is 0.088252604\n",
      "epoch: 6 step: 623, loss is 0.076022804\n",
      "epoch: 6 step: 624, loss is 0.07224792\n",
      "epoch: 6 step: 625, loss is 0.09090996\n",
      "epoch: 6 step: 626, loss is 0.08102375\n",
      "epoch: 6 step: 627, loss is 0.080705464\n",
      "epoch: 6 step: 628, loss is 0.08035618\n",
      "epoch: 6 step: 629, loss is 0.07913047\n",
      "epoch: 6 step: 630, loss is 0.08192134\n",
      "epoch: 6 step: 631, loss is 0.079392135\n",
      "epoch: 6 step: 632, loss is 0.07871443\n",
      "epoch: 6 step: 633, loss is 0.07782078\n",
      "epoch: 6 step: 634, loss is 0.085552216\n",
      "epoch: 6 step: 635, loss is 0.082199275\n",
      "epoch: 6 step: 636, loss is 0.0770936\n",
      "epoch: 6 step: 637, loss is 0.071465254\n",
      "epoch: 6 step: 638, loss is 0.06390959\n",
      "epoch: 6 step: 639, loss is 0.075917244\n",
      "epoch: 6 step: 640, loss is 0.088569164\n",
      "epoch: 6 step: 641, loss is -0.014763951\n",
      "epoch: 6 step: 642, loss is 0.07740289\n",
      "epoch: 6 step: 643, loss is 0.08593631\n",
      "epoch: 6 step: 644, loss is 0.07236248\n",
      "epoch: 6 step: 645, loss is 0.0712533\n",
      "epoch: 6 step: 646, loss is 0.07417107\n",
      "epoch: 6 step: 647, loss is 0.083735526\n",
      "epoch: 6 step: 648, loss is 0.08485228\n",
      "epoch: 6 step: 649, loss is 0.0064397454\n",
      "epoch: 6 step: 650, loss is 0.08690947\n",
      "epoch: 6 step: 651, loss is 0.089351654\n",
      "epoch: 6 step: 652, loss is 0.07686478\n",
      "epoch: 6 step: 653, loss is 0.06524682\n",
      "epoch: 6 step: 654, loss is 0.07657933\n",
      "epoch: 6 step: 655, loss is 0.0790782\n",
      "epoch: 6 step: 656, loss is 0.082268715\n",
      "epoch: 6 step: 657, loss is 0.078131676\n",
      "epoch: 6 step: 658, loss is 0.065549195\n",
      "epoch: 6 step: 659, loss is 0.08269119\n",
      "epoch: 6 step: 660, loss is 0.07302445\n",
      "epoch: 6 step: 661, loss is 0.07798934\n",
      "epoch: 6 step: 662, loss is 0.07608181\n",
      "epoch: 6 step: 663, loss is 0.08028561\n",
      "epoch: 6 step: 664, loss is 0.06453061\n",
      "epoch: 6 step: 665, loss is 0.08575195\n",
      "epoch: 6 step: 666, loss is 0.07732415\n",
      "epoch: 6 step: 667, loss is 0.07578677\n",
      "epoch: 6 step: 668, loss is 0.07812041\n",
      "epoch: 6 step: 669, loss is 0.07652587\n",
      "epoch: 6 step: 670, loss is 0.0851233\n",
      "epoch: 6 step: 671, loss is 0.10962516\n",
      "epoch: 6 step: 672, loss is 0.07841498\n",
      "epoch: 6 step: 673, loss is 0.070378125\n",
      "epoch: 6 step: 674, loss is 0.07756835\n",
      "epoch: 6 step: 675, loss is 0.068969786\n",
      "epoch: 6 step: 676, loss is 0.085068226\n",
      "epoch: 6 step: 677, loss is 0.07174498\n",
      "epoch: 6 step: 678, loss is 0.072223485\n",
      "epoch: 6 step: 679, loss is 0.07767004\n",
      "epoch: 6 step: 680, loss is 0.08373684\n",
      "epoch: 6 step: 681, loss is 0.07609326\n",
      "epoch: 6 step: 682, loss is 0.08281511\n",
      "epoch: 6 step: 683, loss is 0.09727073\n",
      "epoch: 6 step: 684, loss is 0.108891964\n",
      "epoch: 6 step: 685, loss is 0.08174634\n",
      "epoch: 6 step: 686, loss is 0.07987118\n",
      "epoch: 6 step: 687, loss is 0.08142853\n",
      "epoch: 6 step: 688, loss is 0.081590176\n",
      "epoch: 6 step: 689, loss is 0.08808833\n",
      "epoch: 6 step: 690, loss is 0.082372844\n",
      "epoch: 6 step: 691, loss is 0.07861489\n",
      "epoch: 6 step: 692, loss is 0.08510929\n",
      "epoch: 6 step: 693, loss is 0.081388295\n",
      "epoch: 6 step: 694, loss is 0.078449786\n",
      "epoch: 6 step: 695, loss is 0.042798698\n",
      "epoch: 6 step: 696, loss is 0.077628136\n",
      "epoch: 6 step: 697, loss is 0.07831526\n",
      "epoch: 6 step: 698, loss is 0.066272676\n",
      "epoch: 6 step: 699, loss is 0.07202482\n",
      "epoch: 6 step: 700, loss is 0.080538034\n",
      "epoch: 6 step: 701, loss is 0.08562988\n",
      "epoch: 6 step: 702, loss is 0.07028848\n",
      "epoch: 6 step: 703, loss is 0.0884943\n",
      "epoch: 6 step: 704, loss is 0.07365513\n",
      "epoch: 6 step: 705, loss is 0.06962985\n",
      "epoch: 6 step: 706, loss is 0.08335233\n",
      "epoch: 6 step: 707, loss is 0.07944298\n",
      "epoch: 6 step: 708, loss is 0.085171044\n",
      "epoch: 6 step: 709, loss is 0.08319026\n",
      "epoch: 6 step: 710, loss is 0.07165557\n",
      "epoch: 6 step: 711, loss is 0.072184265\n",
      "epoch: 6 step: 712, loss is 0.08626652\n",
      "epoch: 6 step: 713, loss is 0.08219081\n",
      "epoch: 6 step: 714, loss is 0.08047432\n",
      "epoch: 6 step: 715, loss is 0.081418514\n",
      "epoch: 6 step: 716, loss is 0.08456808\n",
      "epoch: 6 step: 717, loss is 0.08952755\n",
      "epoch: 6 step: 718, loss is 0.077860594\n",
      "epoch: 6 step: 719, loss is 0.07147384\n",
      "epoch: 6 step: 720, loss is 0.08198404\n",
      "epoch: 6 step: 721, loss is 0.084025085\n",
      "epoch: 6 step: 722, loss is 0.0818305\n",
      "epoch: 6 step: 723, loss is 0.07565683\n",
      "epoch: 6 step: 724, loss is 0.080296814\n",
      "epoch: 6 step: 725, loss is 0.079019785\n",
      "epoch: 6 step: 726, loss is 0.07613641\n",
      "epoch: 6 step: 727, loss is 0.07855868\n",
      "epoch: 6 step: 728, loss is 0.08054185\n",
      "epoch: 6 step: 729, loss is 0.078242004\n",
      "epoch: 6 step: 730, loss is 0.08371711\n",
      "epoch: 6 step: 731, loss is 0.084285736\n",
      "epoch: 6 step: 732, loss is 0.09129906\n",
      "epoch: 6 step: 733, loss is 0.074187815\n",
      "epoch: 6 step: 734, loss is 0.085449696\n",
      "epoch: 6 step: 735, loss is 0.082238495\n",
      "epoch: 6 step: 736, loss is 0.09016794\n",
      "epoch: 6 step: 737, loss is 0.06491804\n",
      "epoch: 6 step: 738, loss is 0.083316326\n",
      "epoch: 6 step: 739, loss is 0.074044466\n",
      "epoch: 6 step: 740, loss is 0.079693794\n",
      "epoch: 6 step: 741, loss is 0.08116627\n",
      "epoch: 6 step: 742, loss is 0.08454418\n",
      "epoch: 6 step: 743, loss is 0.062441826\n",
      "epoch: 6 step: 744, loss is 0.06581497\n",
      "epoch: 6 step: 745, loss is 0.07323474\n",
      "epoch: 6 step: 746, loss is 0.07119304\n",
      "epoch: 6 step: 747, loss is 0.09130257\n",
      "epoch: 6 step: 748, loss is 0.092595816\n",
      "epoch: 6 step: 749, loss is 0.0832575\n",
      "epoch: 6 step: 750, loss is 0.07233733\n",
      "epoch: 6 step: 751, loss is 0.08507317\n",
      "epoch: 6 step: 752, loss is 0.08360571\n",
      "epoch: 6 step: 753, loss is 0.08064121\n",
      "epoch: 6 step: 754, loss is 0.07510179\n",
      "epoch: 6 step: 755, loss is 0.07914525\n",
      "epoch: 6 step: 756, loss is 0.06325024\n",
      "epoch: 6 step: 757, loss is 0.070810616\n",
      "epoch: 6 step: 758, loss is 0.04072267\n",
      "epoch: 6 step: 759, loss is 0.09187025\n",
      "epoch: 6 step: 760, loss is 0.08650637\n",
      "epoch: 6 step: 761, loss is 0.08180648\n",
      "epoch: 6 step: 762, loss is 0.09166676\n",
      "epoch: 6 step: 763, loss is 0.07770175\n",
      "epoch: 6 step: 764, loss is 0.08072764\n",
      "epoch: 6 step: 765, loss is 0.12503523\n",
      "epoch: 6 step: 766, loss is 0.08378506\n",
      "epoch: 6 step: 767, loss is 0.08376527\n",
      "epoch: 6 step: 768, loss is 0.08648783\n",
      "epoch: 6 step: 769, loss is 0.08267766\n",
      "epoch: 6 step: 770, loss is 0.0796926\n",
      "epoch: 6 step: 771, loss is 0.08310413\n",
      "epoch: 6 step: 772, loss is 0.09049529\n",
      "epoch: 6 step: 773, loss is 0.08364743\n",
      "epoch: 6 step: 774, loss is 0.08361691\n",
      "epoch: 6 step: 775, loss is 0.0665856\n",
      "epoch: 7 step: 1, loss is 0.07395989\n",
      "epoch: 7 step: 2, loss is 0.07813579\n",
      "epoch: 7 step: 3, loss is 0.06871587\n",
      "epoch: 7 step: 4, loss is 0.08239961\n",
      "epoch: 7 step: 5, loss is 0.084387004\n",
      "epoch: 7 step: 6, loss is 0.07654858\n",
      "epoch: 7 step: 7, loss is 0.08163619\n",
      "epoch: 7 step: 8, loss is 0.08396584\n",
      "epoch: 7 step: 9, loss is 0.08024448\n",
      "epoch: 7 step: 10, loss is 0.07266188\n",
      "epoch: 7 step: 11, loss is 0.077570915\n",
      "epoch: 7 step: 12, loss is 0.2437172\n",
      "epoch: 7 step: 13, loss is 0.08955312\n",
      "epoch: 7 step: 14, loss is 0.08478969\n",
      "epoch: 7 step: 15, loss is 0.09799045\n",
      "epoch: 7 step: 16, loss is 0.08619046\n",
      "epoch: 7 step: 17, loss is 0.089844465\n",
      "epoch: 7 step: 18, loss is 0.089583874\n",
      "epoch: 7 step: 19, loss is 0.09140676\n",
      "epoch: 7 step: 20, loss is 0.11879355\n",
      "epoch: 7 step: 21, loss is 0.08637571\n",
      "epoch: 7 step: 22, loss is 0.089039505\n",
      "epoch: 7 step: 23, loss is 0.07256079\n",
      "epoch: 7 step: 24, loss is 0.088553905\n",
      "epoch: 7 step: 25, loss is 0.0785923\n",
      "epoch: 7 step: 26, loss is 0.089468\n",
      "epoch: 7 step: 27, loss is 0.08984405\n",
      "epoch: 7 step: 28, loss is 0.08016425\n",
      "epoch: 7 step: 29, loss is 0.085772336\n",
      "epoch: 7 step: 30, loss is 0.07747221\n",
      "epoch: 7 step: 31, loss is 0.076343834\n",
      "epoch: 7 step: 32, loss is 0.09035319\n",
      "epoch: 7 step: 33, loss is 0.07992107\n",
      "epoch: 7 step: 34, loss is 0.07613063\n",
      "epoch: 7 step: 35, loss is 0.07667947\n",
      "epoch: 7 step: 36, loss is 0.08662015\n",
      "epoch: 7 step: 37, loss is 0.07989311\n",
      "epoch: 7 step: 38, loss is 0.07670432\n",
      "epoch: 7 step: 39, loss is 0.07744002\n",
      "epoch: 7 step: 40, loss is 0.07790214\n",
      "epoch: 7 step: 41, loss is 0.093810976\n",
      "epoch: 7 step: 42, loss is 0.08047563\n",
      "epoch: 7 step: 43, loss is 0.075270355\n",
      "epoch: 7 step: 44, loss is 0.06510788\n",
      "epoch: 7 step: 45, loss is 0.08168435\n",
      "epoch: 7 step: 46, loss is 0.08675265\n",
      "epoch: 7 step: 47, loss is 0.08841014\n",
      "epoch: 7 step: 48, loss is 0.080681264\n",
      "epoch: 7 step: 49, loss is 0.077219546\n",
      "epoch: 7 step: 50, loss is 0.09005308\n",
      "epoch: 7 step: 51, loss is 0.07465285\n",
      "epoch: 7 step: 52, loss is 0.08464432\n",
      "epoch: 7 step: 53, loss is 0.07162142\n",
      "epoch: 7 step: 54, loss is 0.08298063\n",
      "epoch: 7 step: 55, loss is 0.06968039\n",
      "epoch: 7 step: 56, loss is 0.09080297\n",
      "epoch: 7 step: 57, loss is 0.07746738\n",
      "epoch: 7 step: 58, loss is 0.06331438\n",
      "epoch: 7 step: 59, loss is 0.093182266\n",
      "epoch: 7 step: 60, loss is 0.08442813\n",
      "epoch: 7 step: 61, loss is 0.07944727\n",
      "epoch: 7 step: 62, loss is 0.083280504\n",
      "epoch: 7 step: 63, loss is 0.08742446\n",
      "epoch: 7 step: 64, loss is 0.08112794\n",
      "epoch: 7 step: 65, loss is 0.08304548\n",
      "epoch: 7 step: 66, loss is 0.08686274\n",
      "epoch: 7 step: 67, loss is 0.08569223\n",
      "epoch: 7 step: 68, loss is 0.08099383\n",
      "epoch: 7 step: 69, loss is 0.085974395\n",
      "epoch: 7 step: 70, loss is 0.086651504\n",
      "epoch: 7 step: 71, loss is 0.0856809\n",
      "epoch: 7 step: 72, loss is 0.08866304\n",
      "epoch: 7 step: 73, loss is 0.08317143\n",
      "epoch: 7 step: 74, loss is 0.08960551\n",
      "epoch: 7 step: 75, loss is 0.07828027\n",
      "epoch: 7 step: 76, loss is 0.08925325\n",
      "epoch: 7 step: 77, loss is 0.085948646\n",
      "epoch: 7 step: 78, loss is 0.08009416\n",
      "epoch: 7 step: 79, loss is 0.06985217\n",
      "epoch: 7 step: 80, loss is 0.08526617\n",
      "epoch: 7 step: 81, loss is 0.06269556\n",
      "epoch: 7 step: 82, loss is 0.06891459\n",
      "epoch: 7 step: 83, loss is 0.08262962\n",
      "epoch: 7 step: 84, loss is 0.08279234\n",
      "epoch: 7 step: 85, loss is 0.08703834\n",
      "epoch: 7 step: 86, loss is 0.08596927\n",
      "epoch: 7 step: 87, loss is 0.07760471\n",
      "epoch: 7 step: 88, loss is 0.09136748\n",
      "epoch: 7 step: 89, loss is 0.08700156\n",
      "epoch: 7 step: 90, loss is 0.083352566\n",
      "epoch: 7 step: 91, loss is 0.07274979\n",
      "epoch: 7 step: 92, loss is 0.084765375\n",
      "epoch: 7 step: 93, loss is 0.075959206\n",
      "epoch: 7 step: 94, loss is 0.07917994\n",
      "epoch: 7 step: 95, loss is 0.081713736\n",
      "epoch: 7 step: 96, loss is 0.057668388\n",
      "epoch: 7 step: 97, loss is 0.076717675\n",
      "epoch: 7 step: 98, loss is 0.06858778\n",
      "epoch: 7 step: 99, loss is 0.071174145\n",
      "epoch: 7 step: 100, loss is 0.077551365\n",
      "epoch: 7 step: 101, loss is 0.07869935\n",
      "epoch: 7 step: 102, loss is 0.0810346\n",
      "epoch: 7 step: 103, loss is 0.07561618\n",
      "epoch: 7 step: 104, loss is 0.10883808\n",
      "epoch: 7 step: 105, loss is 0.06431168\n",
      "epoch: 7 step: 106, loss is 0.0714342\n",
      "epoch: 7 step: 107, loss is 0.08845204\n",
      "epoch: 7 step: 108, loss is 0.077353\n",
      "epoch: 7 step: 109, loss is 0.13472325\n",
      "epoch: 7 step: 110, loss is 0.073424816\n",
      "epoch: 7 step: 111, loss is 0.083514154\n",
      "epoch: 7 step: 112, loss is 0.07925463\n",
      "epoch: 7 step: 113, loss is 0.08840412\n",
      "epoch: 7 step: 114, loss is 0.08506662\n",
      "epoch: 7 step: 115, loss is 0.0729208\n",
      "epoch: 7 step: 116, loss is 0.08249426\n",
      "epoch: 7 step: 117, loss is 0.07226533\n",
      "epoch: 7 step: 118, loss is 0.08044332\n",
      "epoch: 7 step: 119, loss is 0.069256485\n",
      "epoch: 7 step: 120, loss is 0.091418505\n",
      "epoch: 7 step: 121, loss is 0.07645178\n",
      "epoch: 7 step: 122, loss is 0.08768827\n",
      "epoch: 7 step: 123, loss is 0.08172065\n",
      "epoch: 7 step: 124, loss is 0.08734864\n",
      "epoch: 7 step: 125, loss is 0.06896114\n",
      "epoch: 7 step: 126, loss is 0.08568972\n",
      "epoch: 7 step: 127, loss is 0.06330609\n",
      "epoch: 7 step: 128, loss is 0.06314939\n",
      "epoch: 7 step: 129, loss is 0.08329803\n",
      "epoch: 7 step: 130, loss is 0.07260853\n",
      "epoch: 7 step: 131, loss is 0.08594704\n",
      "epoch: 7 step: 132, loss is 0.06791949\n",
      "epoch: 7 step: 133, loss is 0.073726654\n",
      "epoch: 7 step: 134, loss is 0.07474357\n",
      "epoch: 7 step: 135, loss is 0.08859414\n",
      "epoch: 7 step: 136, loss is 0.08888811\n",
      "epoch: 7 step: 137, loss is 0.0840351\n",
      "epoch: 7 step: 138, loss is 0.08107442\n",
      "epoch: 7 step: 139, loss is 0.07430309\n",
      "epoch: 7 step: 140, loss is 0.10333055\n",
      "epoch: 7 step: 141, loss is 0.08808684\n",
      "epoch: 7 step: 142, loss is 0.081722796\n",
      "epoch: 7 step: 143, loss is 0.060898066\n",
      "epoch: 7 step: 144, loss is 0.08384162\n",
      "epoch: 7 step: 145, loss is 0.08414292\n",
      "epoch: 7 step: 146, loss is 0.09503603\n",
      "epoch: 7 step: 147, loss is 0.07540387\n",
      "epoch: 7 step: 148, loss is 0.071764946\n",
      "epoch: 7 step: 149, loss is 0.09813756\n",
      "epoch: 7 step: 150, loss is 0.08650714\n",
      "epoch: 7 step: 151, loss is 0.070884764\n",
      "epoch: 7 step: 152, loss is 0.08796835\n",
      "epoch: 7 step: 153, loss is 0.08981466\n",
      "epoch: 7 step: 154, loss is 0.101703644\n",
      "epoch: 7 step: 155, loss is 0.09695119\n",
      "epoch: 7 step: 156, loss is 0.08627242\n",
      "epoch: 7 step: 157, loss is 0.07547933\n",
      "epoch: 7 step: 158, loss is 0.100548804\n",
      "epoch: 7 step: 159, loss is 0.08557582\n",
      "epoch: 7 step: 160, loss is 0.085298955\n",
      "epoch: 7 step: 161, loss is 0.076911926\n",
      "epoch: 7 step: 162, loss is 0.10263085\n",
      "epoch: 7 step: 163, loss is 0.08916551\n",
      "epoch: 7 step: 164, loss is 0.078627884\n",
      "epoch: 7 step: 165, loss is 0.09977418\n",
      "epoch: 7 step: 166, loss is 0.08238149\n",
      "epoch: 7 step: 167, loss is 0.09830761\n",
      "epoch: 7 step: 168, loss is 0.096223354\n",
      "epoch: 7 step: 169, loss is 0.083046734\n",
      "epoch: 7 step: 170, loss is 0.107358456\n",
      "epoch: 7 step: 171, loss is 1.0401876\n",
      "epoch: 7 step: 172, loss is 0.09295195\n",
      "epoch: 7 step: 173, loss is 0.100283325\n",
      "epoch: 7 step: 174, loss is 0.10180026\n",
      "epoch: 7 step: 175, loss is 0.09691328\n",
      "epoch: 7 step: 176, loss is 0.20495957\n",
      "epoch: 7 step: 177, loss is 0.0999434\n",
      "epoch: 7 step: 178, loss is 0.09308338\n",
      "epoch: 7 step: 179, loss is 0.09900141\n",
      "epoch: 7 step: 180, loss is 0.09820652\n",
      "epoch: 7 step: 181, loss is 0.094185054\n",
      "epoch: 7 step: 182, loss is 0.10408658\n",
      "epoch: 7 step: 183, loss is 0.09967208\n",
      "epoch: 7 step: 184, loss is 0.0996539\n",
      "epoch: 7 step: 185, loss is 0.08701056\n",
      "epoch: 7 step: 186, loss is 0.10158849\n",
      "epoch: 7 step: 187, loss is 0.09900123\n",
      "epoch: 7 step: 188, loss is 0.11221999\n",
      "epoch: 7 step: 189, loss is 0.10201663\n",
      "epoch: 7 step: 190, loss is 0.09259671\n",
      "epoch: 7 step: 191, loss is 0.100161016\n",
      "epoch: 7 step: 192, loss is 0.09930676\n",
      "epoch: 7 step: 193, loss is 0.16029626\n",
      "epoch: 7 step: 194, loss is 0.09804517\n",
      "epoch: 7 step: 195, loss is 0.098617256\n",
      "epoch: 7 step: 196, loss is 0.092466176\n",
      "epoch: 7 step: 197, loss is 0.104604304\n",
      "epoch: 7 step: 198, loss is 0.095947444\n",
      "epoch: 7 step: 199, loss is 0.09778839\n",
      "epoch: 7 step: 200, loss is 0.081515014\n",
      "epoch: 7 step: 201, loss is 0.09710306\n",
      "epoch: 7 step: 202, loss is 0.10696834\n",
      "epoch: 7 step: 203, loss is 0.09764862\n",
      "epoch: 7 step: 204, loss is 0.09844512\n",
      "epoch: 7 step: 205, loss is 0.08144361\n",
      "epoch: 7 step: 206, loss is 0.09828693\n",
      "epoch: 7 step: 207, loss is 0.0931285\n",
      "epoch: 7 step: 208, loss is 0.09872204\n",
      "epoch: 7 step: 209, loss is 0.10116601\n",
      "epoch: 7 step: 210, loss is 0.09572142\n",
      "epoch: 7 step: 211, loss is 0.09349984\n",
      "epoch: 7 step: 212, loss is 0.097539485\n",
      "epoch: 7 step: 213, loss is 0.093875885\n",
      "epoch: 7 step: 214, loss is 0.09465736\n",
      "epoch: 7 step: 215, loss is 0.0894084\n",
      "epoch: 7 step: 216, loss is 0.09248614\n",
      "epoch: 7 step: 217, loss is 0.09568834\n",
      "epoch: 7 step: 218, loss is 0.10285276\n",
      "epoch: 7 step: 219, loss is 0.0992493\n",
      "epoch: 7 step: 220, loss is 0.09489989\n",
      "epoch: 7 step: 221, loss is 0.10489732\n",
      "epoch: 7 step: 222, loss is 0.10246134\n",
      "epoch: 7 step: 223, loss is 0.090883255\n",
      "epoch: 7 step: 224, loss is 0.09529948\n",
      "epoch: 7 step: 225, loss is 0.09457803\n",
      "epoch: 7 step: 226, loss is 0.09575349\n",
      "epoch: 7 step: 227, loss is 0.099802494\n",
      "epoch: 7 step: 228, loss is 0.09783393\n",
      "epoch: 7 step: 229, loss is 0.08861977\n",
      "epoch: 7 step: 230, loss is 0.09953827\n",
      "epoch: 7 step: 231, loss is 0.08532077\n",
      "epoch: 7 step: 232, loss is 0.0994584\n",
      "epoch: 7 step: 233, loss is 0.0972777\n",
      "epoch: 7 step: 234, loss is 0.09951705\n",
      "epoch: 7 step: 235, loss is 0.116508305\n",
      "epoch: 7 step: 236, loss is 0.099865615\n",
      "epoch: 7 step: 237, loss is 0.09574586\n",
      "epoch: 7 step: 238, loss is 0.0998714\n",
      "epoch: 7 step: 239, loss is 0.10513753\n",
      "epoch: 7 step: 240, loss is 0.09566265\n",
      "epoch: 7 step: 241, loss is 0.09499192\n",
      "epoch: 7 step: 242, loss is 0.10082698\n",
      "epoch: 7 step: 243, loss is 0.09444529\n",
      "epoch: 7 step: 244, loss is 0.09524155\n",
      "epoch: 7 step: 245, loss is 0.09878653\n",
      "epoch: 7 step: 246, loss is 0.105009735\n",
      "epoch: 7 step: 247, loss is 0.09976989\n",
      "epoch: 7 step: 248, loss is 0.106328964\n",
      "epoch: 7 step: 249, loss is 0.10374278\n",
      "epoch: 7 step: 250, loss is 0.091659784\n",
      "epoch: 7 step: 251, loss is 0.09721184\n",
      "epoch: 7 step: 252, loss is 0.09298724\n",
      "epoch: 7 step: 253, loss is 0.094675004\n",
      "epoch: 7 step: 254, loss is 0.09693831\n",
      "epoch: 7 step: 255, loss is 0.101826966\n",
      "epoch: 7 step: 256, loss is 0.09626436\n",
      "epoch: 7 step: 257, loss is 0.10393506\n",
      "epoch: 7 step: 258, loss is 0.09963101\n",
      "epoch: 7 step: 259, loss is 0.09585023\n",
      "epoch: 7 step: 260, loss is 0.101793766\n",
      "epoch: 7 step: 261, loss is 0.0802024\n",
      "epoch: 7 step: 262, loss is 0.096874416\n",
      "epoch: 7 step: 263, loss is 0.100304425\n",
      "epoch: 7 step: 264, loss is 0.09618825\n",
      "epoch: 7 step: 265, loss is 0.09249276\n",
      "epoch: 7 step: 266, loss is 0.09882146\n",
      "epoch: 7 step: 267, loss is 0.099885404\n",
      "epoch: 7 step: 268, loss is 0.09994882\n",
      "epoch: 7 step: 269, loss is 0.09536791\n",
      "epoch: 7 step: 270, loss is 0.09210348\n",
      "epoch: 7 step: 271, loss is 0.0857957\n",
      "epoch: 7 step: 272, loss is 0.099892676\n",
      "epoch: 7 step: 273, loss is 0.11802083\n",
      "epoch: 7 step: 274, loss is 0.095784664\n",
      "epoch: 7 step: 275, loss is 0.09629732\n",
      "epoch: 7 step: 276, loss is 0.09462714\n",
      "epoch: 7 step: 277, loss is 0.09064722\n",
      "epoch: 7 step: 278, loss is 0.09130591\n",
      "epoch: 7 step: 279, loss is 0.08689195\n",
      "epoch: 7 step: 280, loss is 0.09155607\n",
      "epoch: 7 step: 281, loss is 0.14550883\n",
      "epoch: 7 step: 282, loss is 0.090541065\n",
      "epoch: 7 step: 283, loss is 0.09806532\n",
      "epoch: 7 step: 284, loss is 0.09470338\n",
      "epoch: 7 step: 285, loss is 0.095833\n",
      "epoch: 7 step: 286, loss is 0.09124756\n",
      "epoch: 7 step: 287, loss is 0.092645645\n",
      "epoch: 7 step: 288, loss is 0.08845502\n",
      "epoch: 7 step: 289, loss is 0.10628027\n",
      "epoch: 7 step: 290, loss is 0.09351635\n",
      "epoch: 7 step: 291, loss is 0.09288484\n",
      "epoch: 7 step: 292, loss is 0.08727437\n",
      "epoch: 7 step: 293, loss is 0.09209913\n",
      "epoch: 7 step: 294, loss is 0.11459535\n",
      "epoch: 7 step: 295, loss is 0.09397143\n",
      "epoch: 7 step: 296, loss is 0.12236673\n",
      "epoch: 7 step: 297, loss is 0.09118813\n",
      "epoch: 7 step: 298, loss is 0.09091139\n",
      "epoch: 7 step: 299, loss is 0.09612769\n",
      "epoch: 7 step: 300, loss is 0.101245224\n",
      "epoch: 7 step: 301, loss is 0.09748298\n",
      "epoch: 7 step: 302, loss is 0.08889842\n",
      "epoch: 7 step: 303, loss is 0.0964275\n",
      "epoch: 7 step: 304, loss is 0.10020828\n",
      "epoch: 7 step: 305, loss is 0.09231454\n",
      "epoch: 7 step: 306, loss is 0.092485845\n",
      "epoch: 7 step: 307, loss is 0.09509224\n",
      "epoch: 7 step: 308, loss is 0.09634209\n",
      "epoch: 7 step: 309, loss is 0.107595444\n",
      "epoch: 7 step: 310, loss is 0.099178016\n",
      "epoch: 7 step: 311, loss is 0.09300679\n",
      "epoch: 7 step: 312, loss is 0.09463996\n",
      "epoch: 7 step: 313, loss is 0.038672686\n",
      "epoch: 7 step: 314, loss is 0.099265635\n",
      "epoch: 7 step: 315, loss is 0.096761286\n",
      "epoch: 7 step: 316, loss is 0.09287161\n",
      "epoch: 7 step: 317, loss is 0.10209608\n",
      "epoch: 7 step: 318, loss is 0.094798565\n",
      "epoch: 7 step: 319, loss is 0.10065317\n",
      "epoch: 7 step: 320, loss is 0.10047126\n",
      "epoch: 7 step: 321, loss is 0.0959484\n",
      "epoch: 7 step: 322, loss is 0.12823135\n",
      "epoch: 7 step: 323, loss is 0.084768\n",
      "epoch: 7 step: 324, loss is 0.08928108\n",
      "epoch: 7 step: 325, loss is 0.08970147\n",
      "epoch: 7 step: 326, loss is 0.07658172\n",
      "epoch: 7 step: 327, loss is 0.08446878\n",
      "epoch: 7 step: 328, loss is 0.077875435\n",
      "epoch: 7 step: 329, loss is 0.080274045\n",
      "epoch: 7 step: 330, loss is 0.07868391\n",
      "epoch: 7 step: 331, loss is 0.08118874\n",
      "epoch: 7 step: 332, loss is 0.074289024\n",
      "epoch: 7 step: 333, loss is 0.09046984\n",
      "epoch: 7 step: 334, loss is 0.08207291\n",
      "epoch: 7 step: 335, loss is 0.09481859\n",
      "epoch: 7 step: 336, loss is 0.08077055\n",
      "epoch: 7 step: 337, loss is 0.08249718\n",
      "epoch: 7 step: 338, loss is 0.085733354\n",
      "epoch: 7 step: 339, loss is 0.09160286\n",
      "epoch: 7 step: 340, loss is 0.07364732\n",
      "epoch: 7 step: 341, loss is 0.09040004\n",
      "epoch: 7 step: 342, loss is 0.087533176\n",
      "epoch: 7 step: 343, loss is 0.080617964\n",
      "epoch: 7 step: 344, loss is 0.09042144\n",
      "epoch: 7 step: 345, loss is 0.078269005\n",
      "epoch: 7 step: 346, loss is 0.07813376\n",
      "epoch: 7 step: 347, loss is 0.079717815\n",
      "epoch: 7 step: 348, loss is 0.071305096\n",
      "epoch: 7 step: 349, loss is 0.07957429\n",
      "epoch: 7 step: 350, loss is 0.085175455\n",
      "epoch: 7 step: 351, loss is 0.09068948\n",
      "epoch: 7 step: 352, loss is 0.07558614\n",
      "epoch: 7 step: 353, loss is 0.08375478\n",
      "epoch: 7 step: 354, loss is 0.073174775\n",
      "epoch: 7 step: 355, loss is 0.088364065\n",
      "epoch: 7 step: 356, loss is 0.09298509\n",
      "epoch: 7 step: 357, loss is 0.10301548\n",
      "epoch: 7 step: 358, loss is 0.09927362\n",
      "epoch: 7 step: 359, loss is 0.1004799\n",
      "epoch: 7 step: 360, loss is 0.08911228\n",
      "epoch: 7 step: 361, loss is 0.091754615\n",
      "epoch: 7 step: 362, loss is 0.07684082\n",
      "epoch: 7 step: 363, loss is 0.076203406\n",
      "epoch: 7 step: 364, loss is 0.07570481\n",
      "epoch: 7 step: 365, loss is 0.0840171\n",
      "epoch: 7 step: 366, loss is 0.091458976\n",
      "epoch: 7 step: 367, loss is 0.09051877\n",
      "epoch: 7 step: 368, loss is 0.08778095\n",
      "epoch: 7 step: 369, loss is 0.109841526\n",
      "epoch: 7 step: 370, loss is 0.092315674\n",
      "epoch: 7 step: 371, loss is 0.079815686\n",
      "epoch: 7 step: 372, loss is 0.09796125\n",
      "epoch: 7 step: 373, loss is 0.086797714\n",
      "epoch: 7 step: 374, loss is 0.087298155\n",
      "epoch: 7 step: 375, loss is 0.07934779\n",
      "epoch: 7 step: 376, loss is 0.08221483\n",
      "epoch: 7 step: 377, loss is 0.07294577\n",
      "epoch: 7 step: 378, loss is 0.08799219\n",
      "epoch: 7 step: 379, loss is 0.07457137\n",
      "epoch: 7 step: 380, loss is 0.07969016\n",
      "epoch: 7 step: 381, loss is 0.09448224\n",
      "epoch: 7 step: 382, loss is 0.086774886\n",
      "epoch: 7 step: 383, loss is 0.0866614\n",
      "epoch: 7 step: 384, loss is 0.082318306\n",
      "epoch: 7 step: 385, loss is 0.08337122\n",
      "epoch: 7 step: 386, loss is 0.07684213\n",
      "epoch: 7 step: 387, loss is 0.07288009\n",
      "epoch: 7 step: 388, loss is 0.09305507\n",
      "epoch: 7 step: 389, loss is 0.07729274\n",
      "epoch: 7 step: 390, loss is 0.10091144\n",
      "epoch: 7 step: 391, loss is 0.07408428\n",
      "epoch: 7 step: 392, loss is 0.08450937\n",
      "epoch: 7 step: 393, loss is 0.07747167\n",
      "epoch: 7 step: 394, loss is 0.0802533\n",
      "epoch: 7 step: 395, loss is 0.073500216\n",
      "epoch: 7 step: 396, loss is 0.08657426\n",
      "epoch: 7 step: 397, loss is 0.09364194\n",
      "epoch: 7 step: 398, loss is 0.07262069\n",
      "epoch: 7 step: 399, loss is 0.06763941\n",
      "epoch: 7 step: 400, loss is 0.08507019\n",
      "epoch: 7 step: 401, loss is 0.070289075\n",
      "epoch: 7 step: 402, loss is 0.079886734\n",
      "epoch: 7 step: 403, loss is 0.08213663\n",
      "epoch: 7 step: 404, loss is 0.059501827\n",
      "epoch: 7 step: 405, loss is 0.093877375\n",
      "epoch: 7 step: 406, loss is 0.07075977\n",
      "epoch: 7 step: 407, loss is 0.079577446\n",
      "epoch: 7 step: 408, loss is 0.07312107\n",
      "epoch: 7 step: 409, loss is 0.074056804\n",
      "epoch: 7 step: 410, loss is 0.07013315\n",
      "epoch: 7 step: 411, loss is 0.07690525\n",
      "epoch: 7 step: 412, loss is 0.066796064\n",
      "epoch: 7 step: 413, loss is 0.07757372\n",
      "epoch: 7 step: 414, loss is 0.060412884\n",
      "epoch: 7 step: 415, loss is 0.08261174\n",
      "epoch: 7 step: 416, loss is 0.070547104\n",
      "epoch: 7 step: 417, loss is 0.070545614\n",
      "epoch: 7 step: 418, loss is 0.05866742\n",
      "epoch: 7 step: 419, loss is 0.054116428\n",
      "epoch: 7 step: 420, loss is 0.08705586\n",
      "epoch: 7 step: 421, loss is 0.09567976\n",
      "epoch: 7 step: 422, loss is 0.0812276\n",
      "epoch: 7 step: 423, loss is 0.08682257\n",
      "epoch: 7 step: 424, loss is 0.085059464\n",
      "epoch: 7 step: 425, loss is 0.07265717\n",
      "epoch: 7 step: 426, loss is 0.066306174\n",
      "epoch: 7 step: 427, loss is 0.07692504\n",
      "epoch: 7 step: 428, loss is 0.07249433\n",
      "epoch: 7 step: 429, loss is 0.0795989\n",
      "epoch: 7 step: 430, loss is 0.06458521\n",
      "epoch: 7 step: 431, loss is 0.08281565\n",
      "epoch: 7 step: 432, loss is 0.075899124\n",
      "epoch: 7 step: 433, loss is 0.027436495\n",
      "epoch: 7 step: 434, loss is 0.07816845\n",
      "epoch: 7 step: 435, loss is 0.07663673\n",
      "epoch: 7 step: 436, loss is 0.074008524\n",
      "epoch: 7 step: 437, loss is 0.081591785\n",
      "epoch: 7 step: 438, loss is 0.07362467\n",
      "epoch: 7 step: 439, loss is 0.0986681\n",
      "epoch: 7 step: 440, loss is 0.08199841\n",
      "epoch: 7 step: 441, loss is 0.07483482\n",
      "epoch: 7 step: 442, loss is 0.07992488\n",
      "epoch: 7 step: 443, loss is 0.07657415\n",
      "epoch: 7 step: 444, loss is 0.083791554\n",
      "epoch: 7 step: 445, loss is 0.08032894\n",
      "epoch: 7 step: 446, loss is 0.0767619\n",
      "epoch: 7 step: 447, loss is 0.09004861\n",
      "epoch: 7 step: 448, loss is 0.08915806\n",
      "epoch: 7 step: 449, loss is 0.07561189\n",
      "epoch: 7 step: 450, loss is 0.07366294\n",
      "epoch: 7 step: 451, loss is 0.07685882\n",
      "epoch: 7 step: 452, loss is 0.1008634\n",
      "epoch: 7 step: 453, loss is 0.0740602\n",
      "epoch: 7 step: 454, loss is 0.07808119\n",
      "epoch: 7 step: 455, loss is 0.09768009\n",
      "epoch: 7 step: 456, loss is 0.08747238\n",
      "epoch: 7 step: 457, loss is 0.08800864\n",
      "epoch: 7 step: 458, loss is 0.08280295\n",
      "epoch: 7 step: 459, loss is 0.08072287\n",
      "epoch: 7 step: 460, loss is 0.095765114\n",
      "epoch: 7 step: 461, loss is 0.087836504\n",
      "epoch: 7 step: 462, loss is 0.085906565\n",
      "epoch: 7 step: 463, loss is -0.014687896\n",
      "epoch: 7 step: 464, loss is 0.08013344\n",
      "epoch: 7 step: 465, loss is 0.07824439\n",
      "epoch: 7 step: 466, loss is 0.07225156\n",
      "epoch: 7 step: 467, loss is 0.07256007\n",
      "epoch: 7 step: 468, loss is 0.06497741\n",
      "epoch: 7 step: 469, loss is 0.07017207\n",
      "epoch: 7 step: 470, loss is 0.0783658\n",
      "epoch: 7 step: 471, loss is 0.08180237\n",
      "epoch: 7 step: 472, loss is 0.06659192\n",
      "epoch: 7 step: 473, loss is 0.079196155\n",
      "epoch: 7 step: 474, loss is 0.07184291\n",
      "epoch: 7 step: 475, loss is 0.07952279\n",
      "epoch: 7 step: 476, loss is 0.085173845\n",
      "epoch: 7 step: 477, loss is 0.076188326\n",
      "epoch: 7 step: 478, loss is 0.07418269\n",
      "epoch: 7 step: 479, loss is 0.077258766\n",
      "epoch: 7 step: 480, loss is 0.058143616\n",
      "epoch: 7 step: 481, loss is 0.093352616\n",
      "epoch: 7 step: 482, loss is 0.09986287\n",
      "epoch: 7 step: 483, loss is 0.07276982\n",
      "epoch: 7 step: 484, loss is 0.100378335\n",
      "epoch: 7 step: 485, loss is 0.09146124\n",
      "epoch: 7 step: 486, loss is 0.09735137\n",
      "epoch: 7 step: 487, loss is 0.10816312\n",
      "epoch: 7 step: 488, loss is 0.07860875\n",
      "epoch: 7 step: 489, loss is 0.084373176\n",
      "epoch: 7 step: 490, loss is 0.07804459\n",
      "epoch: 7 step: 491, loss is 0.08079976\n",
      "epoch: 7 step: 492, loss is 0.09364015\n",
      "epoch: 7 step: 493, loss is 0.073191166\n",
      "epoch: 7 step: 494, loss is 0.08492106\n",
      "epoch: 7 step: 495, loss is 0.077705085\n",
      "epoch: 7 step: 496, loss is 0.08534223\n",
      "epoch: 7 step: 497, loss is 0.081505954\n",
      "epoch: 7 step: 498, loss is 0.086335\n",
      "epoch: 7 step: 499, loss is 0.081973076\n",
      "epoch: 7 step: 500, loss is 0.085473\n",
      "epoch: 7 step: 501, loss is 0.09496498\n",
      "epoch: 7 step: 502, loss is 0.086694956\n",
      "epoch: 7 step: 503, loss is 0.082657516\n",
      "epoch: 7 step: 504, loss is 0.09091908\n",
      "epoch: 7 step: 505, loss is 0.07647532\n",
      "epoch: 7 step: 506, loss is 0.07020211\n",
      "epoch: 7 step: 507, loss is 0.07947177\n",
      "epoch: 7 step: 508, loss is 0.0852862\n",
      "epoch: 7 step: 509, loss is 0.084236145\n",
      "epoch: 7 step: 510, loss is 0.07681751\n",
      "epoch: 7 step: 511, loss is 0.07468408\n",
      "epoch: 7 step: 512, loss is 0.086761475\n",
      "epoch: 7 step: 513, loss is 0.079375446\n",
      "epoch: 7 step: 514, loss is 0.082945764\n",
      "epoch: 7 step: 515, loss is 0.07702279\n",
      "epoch: 7 step: 516, loss is 0.086072266\n",
      "epoch: 7 step: 517, loss is 0.07474834\n",
      "epoch: 7 step: 518, loss is 0.09744924\n",
      "epoch: 7 step: 519, loss is -0.0496912\n",
      "epoch: 7 step: 520, loss is 0.08790988\n",
      "epoch: 7 step: 521, loss is 0.095250905\n",
      "epoch: 7 step: 522, loss is 0.07813078\n",
      "epoch: 7 step: 523, loss is 0.07716435\n",
      "epoch: 7 step: 524, loss is 0.06926012\n",
      "epoch: 7 step: 525, loss is 0.07674199\n",
      "epoch: 7 step: 526, loss is 0.07746267\n",
      "epoch: 7 step: 527, loss is 0.07411575\n",
      "epoch: 7 step: 528, loss is 0.08357483\n",
      "epoch: 7 step: 529, loss is 0.077295125\n",
      "epoch: 7 step: 530, loss is 0.07484341\n",
      "epoch: 7 step: 531, loss is 0.071686685\n",
      "epoch: 7 step: 532, loss is 0.08718747\n",
      "epoch: 7 step: 533, loss is 0.0784623\n",
      "epoch: 7 step: 534, loss is 0.07495707\n",
      "epoch: 7 step: 535, loss is 0.084089994\n",
      "epoch: 7 step: 536, loss is 0.07800621\n",
      "epoch: 7 step: 537, loss is 0.0845688\n",
      "epoch: 7 step: 538, loss is 0.07716304\n",
      "epoch: 7 step: 539, loss is 0.08278805\n",
      "epoch: 7 step: 540, loss is 0.07734543\n",
      "epoch: 7 step: 541, loss is 0.062625885\n",
      "epoch: 7 step: 542, loss is 0.09951562\n",
      "epoch: 7 step: 543, loss is 0.08977842\n",
      "epoch: 7 step: 544, loss is 0.09740585\n",
      "epoch: 7 step: 545, loss is 0.1009295\n",
      "epoch: 7 step: 546, loss is 0.091166794\n",
      "epoch: 7 step: 547, loss is 0.09344816\n",
      "epoch: 7 step: 548, loss is 0.08809137\n",
      "epoch: 7 step: 549, loss is 0.09327167\n",
      "epoch: 7 step: 550, loss is 0.09194571\n",
      "epoch: 7 step: 551, loss is 0.085433066\n",
      "epoch: 7 step: 552, loss is 0.09231216\n",
      "epoch: 7 step: 553, loss is 0.087052345\n",
      "epoch: 7 step: 554, loss is 0.11952996\n",
      "epoch: 7 step: 555, loss is 0.05638534\n",
      "epoch: 7 step: 556, loss is 0.09021348\n",
      "epoch: 7 step: 557, loss is 0.08480626\n",
      "epoch: 7 step: 558, loss is 0.08013219\n",
      "epoch: 7 step: 559, loss is 0.083806336\n",
      "epoch: 7 step: 560, loss is 0.08371949\n",
      "epoch: 7 step: 561, loss is 0.08205575\n",
      "epoch: 7 step: 562, loss is 0.084108055\n",
      "epoch: 7 step: 563, loss is 0.079856575\n",
      "epoch: 7 step: 564, loss is 0.08989382\n",
      "epoch: 7 step: 565, loss is -0.07536733\n",
      "epoch: 7 step: 566, loss is 0.09046912\n",
      "epoch: 7 step: 567, loss is 0.08390695\n",
      "epoch: 7 step: 568, loss is 0.08999032\n",
      "epoch: 7 step: 569, loss is 0.09231943\n",
      "epoch: 7 step: 570, loss is 0.09225917\n",
      "epoch: 7 step: 571, loss is 0.08972317\n",
      "epoch: 7 step: 572, loss is 0.09062308\n",
      "epoch: 7 step: 573, loss is 0.097690344\n",
      "epoch: 7 step: 574, loss is 0.09190011\n",
      "epoch: 7 step: 575, loss is 0.09016448\n",
      "epoch: 7 step: 576, loss is 0.0873847\n",
      "epoch: 7 step: 577, loss is 0.09360367\n",
      "epoch: 7 step: 578, loss is 0.08870965\n",
      "epoch: 7 step: 579, loss is 0.090215206\n",
      "epoch: 7 step: 580, loss is 0.08468038\n",
      "epoch: 7 step: 581, loss is 0.08529395\n",
      "epoch: 7 step: 582, loss is 0.08978367\n",
      "epoch: 7 step: 583, loss is 0.08526057\n",
      "epoch: 7 step: 584, loss is 0.086930335\n",
      "epoch: 7 step: 585, loss is 0.0841158\n",
      "epoch: 7 step: 586, loss is 0.086761\n",
      "epoch: 7 step: 587, loss is 0.10036558\n",
      "epoch: 7 step: 588, loss is 0.08991295\n",
      "epoch: 7 step: 589, loss is 0.08965844\n",
      "epoch: 7 step: 590, loss is 0.09515077\n",
      "epoch: 7 step: 591, loss is 0.088695765\n",
      "epoch: 7 step: 592, loss is 0.08500928\n",
      "epoch: 7 step: 593, loss is 0.08157891\n",
      "epoch: 7 step: 594, loss is 0.093690336\n",
      "epoch: 7 step: 595, loss is 0.07317519\n",
      "epoch: 7 step: 596, loss is 0.079339206\n",
      "epoch: 7 step: 597, loss is 0.09619731\n",
      "epoch: 7 step: 598, loss is 0.078635514\n",
      "epoch: 7 step: 599, loss is 0.06945425\n",
      "epoch: 7 step: 600, loss is 0.079227865\n",
      "epoch: 7 step: 601, loss is 0.13191766\n",
      "epoch: 7 step: 602, loss is 0.092432916\n",
      "epoch: 7 step: 603, loss is 0.08207893\n",
      "epoch: 7 step: 604, loss is 0.09265381\n",
      "epoch: 7 step: 605, loss is 0.089030325\n",
      "epoch: 7 step: 606, loss is 0.092353344\n",
      "epoch: 7 step: 607, loss is 0.08926171\n",
      "epoch: 7 step: 608, loss is 0.094237804\n",
      "epoch: 7 step: 609, loss is 0.08911896\n",
      "epoch: 7 step: 610, loss is 0.096146405\n",
      "epoch: 7 step: 611, loss is 0.089785814\n",
      "epoch: 7 step: 612, loss is 0.08803922\n",
      "epoch: 7 step: 613, loss is 0.08736372\n",
      "epoch: 7 step: 614, loss is 0.07488823\n",
      "epoch: 7 step: 615, loss is 0.09037763\n",
      "epoch: 7 step: 616, loss is 0.09705758\n",
      "epoch: 7 step: 617, loss is 0.092381\n",
      "epoch: 7 step: 618, loss is 0.09033418\n",
      "epoch: 7 step: 619, loss is 0.09684199\n",
      "epoch: 7 step: 620, loss is 0.08890051\n",
      "epoch: 7 step: 621, loss is 0.089796305\n",
      "epoch: 7 step: 622, loss is 0.08937615\n",
      "epoch: 7 step: 623, loss is 0.09103745\n",
      "epoch: 7 step: 624, loss is 0.08172971\n",
      "epoch: 7 step: 625, loss is 0.12362653\n",
      "epoch: 7 step: 626, loss is 0.097940624\n",
      "epoch: 7 step: 627, loss is 0.09150934\n",
      "epoch: 7 step: 628, loss is 0.095018566\n",
      "epoch: 7 step: 629, loss is 0.092413604\n",
      "epoch: 7 step: 630, loss is 0.09339255\n",
      "epoch: 7 step: 631, loss is 0.090436876\n",
      "epoch: 7 step: 632, loss is 0.090534985\n",
      "epoch: 7 step: 633, loss is 0.09101468\n",
      "epoch: 7 step: 634, loss is 0.087926865\n",
      "epoch: 7 step: 635, loss is 0.08902472\n",
      "epoch: 7 step: 636, loss is 0.092597425\n",
      "epoch: 7 step: 637, loss is 0.09302902\n",
      "epoch: 7 step: 638, loss is 0.09542483\n",
      "epoch: 7 step: 639, loss is 0.09098309\n",
      "epoch: 7 step: 640, loss is 0.08655232\n",
      "epoch: 7 step: 641, loss is 0.3590591\n",
      "epoch: 7 step: 642, loss is 0.09336907\n",
      "epoch: 7 step: 643, loss is 0.08379388\n",
      "epoch: 7 step: 644, loss is 0.08679646\n",
      "epoch: 7 step: 645, loss is 0.09309155\n",
      "epoch: 7 step: 646, loss is 0.11794567\n",
      "epoch: 7 step: 647, loss is 0.13470197\n",
      "epoch: 7 step: 648, loss is 0.08733159\n",
      "epoch: 7 step: 649, loss is 0.08006412\n",
      "epoch: 7 step: 650, loss is 0.0900417\n",
      "epoch: 7 step: 651, loss is 0.095431805\n",
      "epoch: 7 step: 652, loss is 0.10351944\n",
      "epoch: 7 step: 653, loss is 0.09790212\n",
      "epoch: 7 step: 654, loss is 0.08675593\n",
      "epoch: 7 step: 655, loss is 0.10095024\n",
      "epoch: 7 step: 656, loss is 0.09792632\n",
      "epoch: 7 step: 657, loss is 0.08435434\n",
      "epoch: 7 step: 658, loss is 0.09060246\n",
      "epoch: 7 step: 659, loss is 0.0695222\n",
      "epoch: 7 step: 660, loss is 0.092675686\n",
      "epoch: 7 step: 661, loss is 0.08845037\n",
      "epoch: 7 step: 662, loss is 0.096609354\n",
      "epoch: 7 step: 663, loss is 0.09659892\n",
      "epoch: 7 step: 664, loss is 0.080382824\n",
      "epoch: 7 step: 665, loss is 0.09466058\n",
      "epoch: 7 step: 666, loss is -7.016984\n",
      "epoch: 7 step: 667, loss is 0.13138837\n",
      "epoch: 7 step: 668, loss is 0.093642056\n",
      "epoch: 7 step: 669, loss is 0.10737735\n",
      "epoch: 7 step: 670, loss is 0.10224134\n",
      "epoch: 7 step: 671, loss is 0.1000945\n",
      "epoch: 7 step: 672, loss is 0.09813166\n",
      "epoch: 7 step: 673, loss is 0.10299373\n",
      "epoch: 7 step: 674, loss is 0.09618968\n",
      "epoch: 7 step: 675, loss is 0.09495878\n",
      "epoch: 7 step: 676, loss is 0.10364944\n",
      "epoch: 7 step: 677, loss is 0.101653576\n",
      "epoch: 7 step: 678, loss is 0.10223287\n",
      "epoch: 7 step: 679, loss is 0.09990281\n",
      "epoch: 7 step: 680, loss is 0.10152358\n",
      "epoch: 7 step: 681, loss is 0.09217852\n",
      "epoch: 7 step: 682, loss is 0.09603715\n",
      "epoch: 7 step: 683, loss is 0.09477979\n",
      "epoch: 7 step: 684, loss is 0.10484648\n",
      "epoch: 7 step: 685, loss is 0.10180932\n",
      "epoch: 7 step: 686, loss is 0.10406357\n",
      "epoch: 7 step: 687, loss is 0.096978664\n",
      "epoch: 7 step: 688, loss is 0.10151678\n",
      "epoch: 7 step: 689, loss is 0.09946728\n",
      "epoch: 7 step: 690, loss is 0.10289723\n",
      "epoch: 7 step: 691, loss is 0.10133678\n",
      "epoch: 7 step: 692, loss is 0.110785484\n",
      "epoch: 7 step: 693, loss is 0.088106334\n",
      "epoch: 7 step: 694, loss is 0.09072775\n",
      "epoch: 7 step: 695, loss is 0.09336829\n",
      "epoch: 7 step: 696, loss is 0.098976195\n",
      "epoch: 7 step: 697, loss is 0.09889001\n",
      "epoch: 7 step: 698, loss is 0.09688097\n",
      "epoch: 7 step: 699, loss is 0.101742685\n",
      "epoch: 7 step: 700, loss is 0.1220873\n",
      "epoch: 7 step: 701, loss is 0.103110015\n",
      "epoch: 7 step: 702, loss is 0.105523586\n",
      "epoch: 7 step: 703, loss is 0.10634184\n",
      "epoch: 7 step: 704, loss is 0.09841132\n",
      "epoch: 7 step: 705, loss is 0.10599232\n",
      "epoch: 7 step: 706, loss is 0.09810001\n",
      "epoch: 7 step: 707, loss is 0.09407669\n",
      "epoch: 7 step: 708, loss is 0.10146171\n",
      "epoch: 7 step: 709, loss is 0.09951258\n",
      "epoch: 7 step: 710, loss is 0.10669631\n",
      "epoch: 7 step: 711, loss is 0.09957594\n",
      "epoch: 7 step: 712, loss is 0.09939599\n",
      "epoch: 7 step: 713, loss is 0.09953451\n",
      "epoch: 7 step: 714, loss is 0.08873731\n",
      "epoch: 7 step: 715, loss is 0.104245365\n",
      "epoch: 7 step: 716, loss is 0.09928399\n",
      "epoch: 7 step: 717, loss is 0.098570764\n",
      "epoch: 7 step: 718, loss is 0.10072857\n",
      "epoch: 7 step: 719, loss is 0.10284537\n",
      "epoch: 7 step: 720, loss is 0.12091845\n",
      "epoch: 7 step: 721, loss is 0.09580654\n",
      "epoch: 7 step: 722, loss is 0.10628301\n",
      "epoch: 7 step: 723, loss is 0.09793264\n",
      "epoch: 7 step: 724, loss is 0.10653001\n",
      "epoch: 7 step: 725, loss is 0.09743887\n",
      "epoch: 7 step: 726, loss is 0.10425037\n",
      "epoch: 7 step: 727, loss is 0.07914084\n",
      "epoch: 7 step: 728, loss is 0.10473734\n",
      "epoch: 7 step: 729, loss is 0.1070233\n",
      "epoch: 7 step: 730, loss is 0.10575038\n",
      "epoch: 7 step: 731, loss is 0.10793263\n",
      "epoch: 7 step: 732, loss is 0.0970307\n",
      "epoch: 7 step: 733, loss is 0.10973841\n",
      "epoch: 7 step: 734, loss is 0.10495645\n",
      "epoch: 7 step: 735, loss is 0.104178905\n",
      "epoch: 7 step: 736, loss is 0.096227646\n",
      "epoch: 7 step: 737, loss is 0.09607059\n",
      "epoch: 7 step: 738, loss is 0.10181975\n",
      "epoch: 7 step: 739, loss is 0.102200806\n",
      "epoch: 7 step: 740, loss is 0.10787058\n",
      "epoch: 7 step: 741, loss is 0.10781163\n",
      "epoch: 7 step: 742, loss is 0.10232574\n",
      "epoch: 7 step: 743, loss is 0.09393221\n",
      "epoch: 7 step: 744, loss is 0.100669086\n",
      "epoch: 7 step: 745, loss is 0.09387922\n",
      "epoch: 7 step: 746, loss is 0.097550094\n",
      "epoch: 7 step: 747, loss is 0.091798306\n",
      "epoch: 7 step: 748, loss is 0.11153096\n",
      "epoch: 7 step: 749, loss is 0.09847993\n",
      "epoch: 7 step: 750, loss is 0.09824282\n",
      "epoch: 7 step: 751, loss is -0.00020873547\n",
      "epoch: 7 step: 752, loss is 0.101023495\n",
      "epoch: 7 step: 753, loss is 0.11410564\n",
      "epoch: 7 step: 754, loss is 0.09560108\n",
      "epoch: 7 step: 755, loss is 0.10477972\n",
      "epoch: 7 step: 756, loss is 0.10006517\n",
      "epoch: 7 step: 757, loss is 0.104563534\n",
      "epoch: 7 step: 758, loss is 0.105959\n",
      "epoch: 7 step: 759, loss is 0.100162566\n",
      "epoch: 7 step: 760, loss is 0.104316294\n",
      "epoch: 7 step: 761, loss is 0.106913865\n",
      "epoch: 7 step: 762, loss is 0.09743905\n",
      "epoch: 7 step: 763, loss is 0.09973025\n",
      "epoch: 7 step: 764, loss is 0.09101552\n",
      "epoch: 7 step: 765, loss is 0.09660816\n",
      "epoch: 7 step: 766, loss is 0.09528762\n",
      "epoch: 7 step: 767, loss is 0.10120696\n",
      "epoch: 7 step: 768, loss is 0.094326794\n",
      "epoch: 7 step: 769, loss is 0.09399843\n",
      "epoch: 7 step: 770, loss is 0.099089324\n",
      "epoch: 7 step: 771, loss is 0.109164715\n",
      "epoch: 7 step: 772, loss is 0.093838036\n",
      "epoch: 7 step: 773, loss is 0.10794896\n",
      "epoch: 7 step: 774, loss is 0.100057065\n",
      "epoch: 7 step: 775, loss is 0.10078049\n",
      "epoch: 8 step: 1, loss is 0.09868461\n",
      "epoch: 8 step: 2, loss is 0.10595894\n",
      "epoch: 8 step: 3, loss is 0.10592812\n",
      "epoch: 8 step: 4, loss is 0.104953706\n",
      "epoch: 8 step: 5, loss is 0.09917098\n",
      "epoch: 8 step: 6, loss is 0.09853101\n",
      "epoch: 8 step: 7, loss is 0.103257835\n",
      "epoch: 8 step: 8, loss is 0.10673255\n",
      "epoch: 8 step: 9, loss is 0.10451239\n",
      "epoch: 8 step: 10, loss is 0.097049415\n",
      "epoch: 8 step: 11, loss is 0.09838575\n",
      "epoch: 8 step: 12, loss is 0.10010052\n",
      "epoch: 8 step: 13, loss is 0.09170729\n",
      "epoch: 8 step: 14, loss is 0.112993\n",
      "epoch: 8 step: 15, loss is 0.10533267\n",
      "epoch: 8 step: 16, loss is 0.10239458\n",
      "epoch: 8 step: 17, loss is 0.098046064\n",
      "epoch: 8 step: 18, loss is 0.098508835\n",
      "epoch: 8 step: 19, loss is 0.103952646\n",
      "epoch: 8 step: 20, loss is 0.10183066\n",
      "epoch: 8 step: 21, loss is 0.10230976\n",
      "epoch: 8 step: 22, loss is 0.09838945\n",
      "epoch: 8 step: 23, loss is 0.10203022\n",
      "epoch: 8 step: 24, loss is 0.09814316\n",
      "epoch: 8 step: 25, loss is 0.09838611\n",
      "epoch: 8 step: 26, loss is 0.0997684\n",
      "epoch: 8 step: 27, loss is 0.09834814\n",
      "epoch: 8 step: 28, loss is 0.10213679\n",
      "epoch: 8 step: 29, loss is 0.102671444\n",
      "epoch: 8 step: 30, loss is 0.094195366\n",
      "epoch: 8 step: 31, loss is 0.9260651\n",
      "epoch: 8 step: 32, loss is 0.105653584\n",
      "epoch: 8 step: 33, loss is 0.099906445\n",
      "epoch: 8 step: 34, loss is 0.10985637\n",
      "epoch: 8 step: 35, loss is 0.10636288\n",
      "epoch: 8 step: 36, loss is 0.10146719\n",
      "epoch: 8 step: 37, loss is 0.10654497\n",
      "epoch: 8 step: 38, loss is 0.106754065\n",
      "epoch: 8 step: 39, loss is 0.09260535\n",
      "epoch: 8 step: 40, loss is 0.044023216\n",
      "epoch: 8 step: 41, loss is 0.10456842\n",
      "epoch: 8 step: 42, loss is 0.09775144\n",
      "epoch: 8 step: 43, loss is 0.09283179\n",
      "epoch: 8 step: 44, loss is 0.097180545\n",
      "epoch: 8 step: 45, loss is 0.104856014\n",
      "epoch: 8 step: 46, loss is 0.09836358\n",
      "epoch: 8 step: 47, loss is 0.10628146\n",
      "epoch: 8 step: 48, loss is 0.09964728\n",
      "epoch: 8 step: 49, loss is 0.09911609\n",
      "epoch: 8 step: 50, loss is 0.109108984\n",
      "epoch: 8 step: 51, loss is 0.099315345\n",
      "epoch: 8 step: 52, loss is 0.08997899\n",
      "epoch: 8 step: 53, loss is 0.106229484\n",
      "epoch: 8 step: 54, loss is 0.08319068\n",
      "epoch: 8 step: 55, loss is 0.07469368\n",
      "epoch: 8 step: 56, loss is 0.09765434\n",
      "epoch: 8 step: 57, loss is 0.103191376\n",
      "epoch: 8 step: 58, loss is 0.10338205\n",
      "epoch: 8 step: 59, loss is 0.09163475\n",
      "epoch: 8 step: 60, loss is 0.110435784\n",
      "epoch: 8 step: 61, loss is 0.106723785\n",
      "epoch: 8 step: 62, loss is 0.11025697\n",
      "epoch: 8 step: 63, loss is 0.097404\n",
      "epoch: 8 step: 64, loss is 0.0983153\n",
      "epoch: 8 step: 65, loss is 0.094097614\n",
      "epoch: 8 step: 66, loss is 0.10574561\n",
      "epoch: 8 step: 67, loss is 0.10320395\n",
      "epoch: 8 step: 68, loss is 0.091774285\n",
      "epoch: 8 step: 69, loss is 0.10008001\n",
      "epoch: 8 step: 70, loss is 0.094219685\n",
      "epoch: 8 step: 71, loss is 0.100592434\n",
      "epoch: 8 step: 72, loss is 0.09646851\n",
      "epoch: 8 step: 73, loss is 0.10401493\n",
      "epoch: 8 step: 74, loss is 0.07349163\n",
      "epoch: 8 step: 75, loss is 0.10190874\n",
      "epoch: 8 step: 76, loss is 0.10758561\n",
      "epoch: 8 step: 77, loss is 0.093999684\n",
      "epoch: 8 step: 78, loss is 0.1087296\n",
      "epoch: 8 step: 79, loss is 0.104138196\n",
      "epoch: 8 step: 80, loss is 0.106381714\n",
      "epoch: 8 step: 81, loss is 0.0776937\n",
      "epoch: 8 step: 82, loss is 0.10461789\n",
      "epoch: 8 step: 83, loss is 0.111867726\n",
      "epoch: 8 step: 84, loss is 0.11109716\n",
      "epoch: 8 step: 85, loss is 0.09910679\n",
      "epoch: 8 step: 86, loss is 0.09947604\n",
      "epoch: 8 step: 87, loss is 0.10076696\n",
      "epoch: 8 step: 88, loss is 0.06723285\n",
      "epoch: 8 step: 89, loss is 0.10246831\n",
      "epoch: 8 step: 90, loss is 0.102268994\n",
      "epoch: 8 step: 91, loss is 0.10811204\n",
      "epoch: 8 step: 92, loss is 0.14654422\n",
      "epoch: 8 step: 93, loss is 0.09079546\n",
      "epoch: 8 step: 94, loss is -0.07846868\n",
      "epoch: 8 step: 95, loss is 0.10199243\n",
      "epoch: 8 step: 96, loss is 0.07813412\n",
      "epoch: 8 step: 97, loss is 0.101000786\n",
      "epoch: 8 step: 98, loss is 0.09808254\n",
      "epoch: 8 step: 99, loss is 0.09332895\n",
      "epoch: 8 step: 100, loss is 0.10647625\n",
      "epoch: 8 step: 101, loss is 0.089179695\n",
      "epoch: 8 step: 102, loss is 0.08582121\n",
      "epoch: 8 step: 103, loss is 0.09968901\n",
      "epoch: 8 step: 104, loss is 0.09179002\n",
      "epoch: 8 step: 105, loss is 0.10268849\n",
      "epoch: 8 step: 106, loss is 0.09124237\n",
      "epoch: 8 step: 107, loss is 0.103135765\n",
      "epoch: 8 step: 108, loss is 0.09423512\n",
      "epoch: 8 step: 109, loss is 0.09096986\n",
      "epoch: 8 step: 110, loss is 0.1010738\n",
      "epoch: 8 step: 111, loss is 0.08852166\n",
      "epoch: 8 step: 112, loss is 0.09155446\n",
      "epoch: 8 step: 113, loss is 0.0936386\n",
      "epoch: 8 step: 114, loss is 0.10076171\n",
      "epoch: 8 step: 115, loss is 0.096428216\n",
      "epoch: 8 step: 116, loss is 0.100296736\n",
      "epoch: 8 step: 117, loss is 0.10020447\n",
      "epoch: 8 step: 118, loss is 0.14681691\n",
      "epoch: 8 step: 119, loss is 0.09910625\n",
      "epoch: 8 step: 120, loss is 0.113310754\n",
      "epoch: 8 step: 121, loss is 0.09502822\n",
      "epoch: 8 step: 122, loss is 0.08876103\n",
      "epoch: 8 step: 123, loss is 0.095259964\n",
      "epoch: 8 step: 124, loss is 0.0928576\n",
      "epoch: 8 step: 125, loss is 0.089301884\n",
      "epoch: 8 step: 126, loss is 0.089075565\n",
      "epoch: 8 step: 127, loss is 0.092597485\n",
      "epoch: 8 step: 128, loss is 0.08756256\n",
      "epoch: 8 step: 129, loss is 0.094595134\n",
      "epoch: 8 step: 130, loss is 0.10284591\n",
      "epoch: 8 step: 131, loss is 0.10358739\n",
      "epoch: 8 step: 132, loss is 0.092659235\n",
      "epoch: 8 step: 133, loss is 0.102324784\n",
      "epoch: 8 step: 134, loss is 0.09193021\n",
      "epoch: 8 step: 135, loss is 0.09494686\n",
      "epoch: 8 step: 136, loss is 0.09790325\n",
      "epoch: 8 step: 137, loss is 0.09662253\n",
      "epoch: 8 step: 138, loss is 0.078974545\n",
      "epoch: 8 step: 139, loss is 0.10213822\n",
      "epoch: 8 step: 140, loss is 0.093783855\n",
      "epoch: 8 step: 141, loss is 0.09842318\n",
      "epoch: 8 step: 142, loss is 0.09903091\n",
      "epoch: 8 step: 143, loss is 0.097462\n",
      "epoch: 8 step: 144, loss is 0.1051352\n",
      "epoch: 8 step: 145, loss is 0.09350628\n",
      "epoch: 8 step: 146, loss is 0.09772521\n",
      "epoch: 8 step: 147, loss is 0.0865193\n",
      "epoch: 8 step: 148, loss is 0.098519325\n",
      "epoch: 8 step: 149, loss is 0.09743452\n",
      "epoch: 8 step: 150, loss is 0.10002732\n",
      "epoch: 8 step: 151, loss is 0.1019457\n",
      "epoch: 8 step: 152, loss is 0.10217935\n",
      "epoch: 8 step: 153, loss is 0.10871798\n",
      "epoch: 8 step: 154, loss is 0.1062541\n",
      "epoch: 8 step: 155, loss is 0.10887474\n",
      "epoch: 8 step: 156, loss is 0.096707284\n",
      "epoch: 8 step: 157, loss is 0.104483545\n",
      "epoch: 8 step: 158, loss is 0.095813096\n",
      "epoch: 8 step: 159, loss is 0.0802722\n",
      "epoch: 8 step: 160, loss is 0.12314743\n",
      "epoch: 8 step: 161, loss is 0.10792655\n",
      "epoch: 8 step: 162, loss is 0.10187131\n",
      "epoch: 8 step: 163, loss is 0.105937004\n",
      "epoch: 8 step: 164, loss is 0.09558469\n",
      "epoch: 8 step: 165, loss is 0.123110116\n",
      "epoch: 8 step: 166, loss is 0.07604629\n",
      "epoch: 8 step: 167, loss is 0.10539532\n",
      "epoch: 8 step: 168, loss is 0.090991795\n",
      "epoch: 8 step: 169, loss is 0.104595244\n",
      "epoch: 8 step: 170, loss is 0.097854614\n",
      "epoch: 8 step: 171, loss is 0.10531968\n",
      "epoch: 8 step: 172, loss is 0.08753592\n",
      "epoch: 8 step: 173, loss is 0.09438467\n",
      "epoch: 8 step: 174, loss is 0.10270935\n",
      "epoch: 8 step: 175, loss is 0.11853486\n",
      "epoch: 8 step: 176, loss is 0.08251685\n",
      "epoch: 8 step: 177, loss is 0.08560604\n",
      "epoch: 8 step: 178, loss is 0.09585029\n",
      "epoch: 8 step: 179, loss is 0.09392053\n",
      "epoch: 8 step: 180, loss is 0.09907216\n",
      "epoch: 8 step: 181, loss is 0.081946135\n",
      "epoch: 8 step: 182, loss is 0.09452897\n",
      "epoch: 8 step: 183, loss is 0.10354918\n",
      "epoch: 8 step: 184, loss is 0.086830616\n",
      "epoch: 8 step: 185, loss is 0.09416419\n",
      "epoch: 8 step: 186, loss is 0.08687544\n",
      "epoch: 8 step: 187, loss is 0.091944695\n",
      "epoch: 8 step: 188, loss is 0.08902025\n",
      "epoch: 8 step: 189, loss is 0.087050974\n",
      "epoch: 8 step: 190, loss is 0.10165787\n",
      "epoch: 8 step: 191, loss is 0.10077262\n",
      "epoch: 8 step: 192, loss is 0.09050894\n",
      "epoch: 8 step: 193, loss is 0.094320476\n",
      "epoch: 8 step: 194, loss is 0.08176392\n",
      "epoch: 8 step: 195, loss is 0.09630442\n",
      "epoch: 8 step: 196, loss is 0.08874822\n",
      "epoch: 8 step: 197, loss is 0.09125143\n",
      "epoch: 8 step: 198, loss is 0.09542227\n",
      "epoch: 8 step: 199, loss is 0.08335966\n",
      "epoch: 8 step: 200, loss is 0.086932\n",
      "epoch: 8 step: 201, loss is 0.0891121\n",
      "epoch: 8 step: 202, loss is 0.09481841\n",
      "epoch: 8 step: 203, loss is 0.09507865\n",
      "epoch: 8 step: 204, loss is 0.09335375\n",
      "epoch: 8 step: 205, loss is 0.08599967\n",
      "epoch: 8 step: 206, loss is 0.08798498\n",
      "epoch: 8 step: 207, loss is 0.082517326\n",
      "epoch: 8 step: 208, loss is 0.08400178\n",
      "epoch: 8 step: 209, loss is 0.106435776\n",
      "epoch: 8 step: 210, loss is 0.09162569\n",
      "epoch: 8 step: 211, loss is 0.09595919\n",
      "epoch: 8 step: 212, loss is 0.08009988\n",
      "epoch: 8 step: 213, loss is 0.08586788\n",
      "epoch: 8 step: 214, loss is 0.09814471\n",
      "epoch: 8 step: 215, loss is 0.080134094\n",
      "epoch: 8 step: 216, loss is 0.0930326\n",
      "epoch: 8 step: 217, loss is 0.0794912\n",
      "epoch: 8 step: 218, loss is 0.08942908\n",
      "epoch: 8 step: 219, loss is 0.080228865\n",
      "epoch: 8 step: 220, loss is 0.08817083\n",
      "epoch: 8 step: 221, loss is 0.07384139\n",
      "epoch: 8 step: 222, loss is 0.07912278\n",
      "epoch: 8 step: 223, loss is 0.09328562\n",
      "epoch: 8 step: 224, loss is 0.094239235\n",
      "epoch: 8 step: 225, loss is 0.09677607\n",
      "epoch: 8 step: 226, loss is 0.09471995\n",
      "epoch: 8 step: 227, loss is 0.0754413\n",
      "epoch: 8 step: 228, loss is 0.08527547\n",
      "epoch: 8 step: 229, loss is 0.09314537\n",
      "epoch: 8 step: 230, loss is 0.09434217\n",
      "epoch: 8 step: 231, loss is 0.10848427\n",
      "epoch: 8 step: 232, loss is 0.09143019\n",
      "epoch: 8 step: 233, loss is 0.06835407\n",
      "epoch: 8 step: 234, loss is 0.09988308\n",
      "epoch: 8 step: 235, loss is 0.10071349\n",
      "epoch: 8 step: 236, loss is 0.08162862\n",
      "epoch: 8 step: 237, loss is 0.09602791\n",
      "epoch: 8 step: 238, loss is 0.08310914\n",
      "epoch: 8 step: 239, loss is 0.09675789\n",
      "epoch: 8 step: 240, loss is 0.09669876\n",
      "epoch: 8 step: 241, loss is 0.08396572\n",
      "epoch: 8 step: 242, loss is 0.08924025\n",
      "epoch: 8 step: 243, loss is 0.09798622\n",
      "epoch: 8 step: 244, loss is 0.08281344\n",
      "epoch: 8 step: 245, loss is 0.097724855\n",
      "epoch: 8 step: 246, loss is 0.08489019\n",
      "epoch: 8 step: 247, loss is 0.10088378\n",
      "epoch: 8 step: 248, loss is 0.09898394\n",
      "epoch: 8 step: 249, loss is 0.09995556\n",
      "epoch: 8 step: 250, loss is 0.10120839\n",
      "epoch: 8 step: 251, loss is 0.09606528\n",
      "epoch: 8 step: 252, loss is 0.10431749\n",
      "epoch: 8 step: 253, loss is 0.099730015\n",
      "epoch: 8 step: 254, loss is 0.093420506\n",
      "epoch: 8 step: 255, loss is 0.08155662\n",
      "epoch: 8 step: 256, loss is 0.098181486\n",
      "epoch: 8 step: 257, loss is 0.081276834\n",
      "epoch: 8 step: 258, loss is 0.10279995\n",
      "epoch: 8 step: 259, loss is 0.09153581\n",
      "epoch: 8 step: 260, loss is 0.093336105\n",
      "epoch: 8 step: 261, loss is 0.09847337\n",
      "epoch: 8 step: 262, loss is 0.09936571\n",
      "epoch: 8 step: 263, loss is 0.106612384\n",
      "epoch: 8 step: 264, loss is 0.09114337\n",
      "epoch: 8 step: 265, loss is 0.078984916\n",
      "epoch: 8 step: 266, loss is 0.08943701\n",
      "epoch: 8 step: 267, loss is 0.08050007\n",
      "epoch: 8 step: 268, loss is 0.068766296\n",
      "epoch: 8 step: 269, loss is 0.08389431\n",
      "epoch: 8 step: 270, loss is 0.08439046\n",
      "epoch: 8 step: 271, loss is 0.091551006\n",
      "epoch: 8 step: 272, loss is 0.10392332\n",
      "epoch: 8 step: 273, loss is 0.060951233\n",
      "epoch: 8 step: 274, loss is 0.090283155\n",
      "epoch: 8 step: 275, loss is 0.088294685\n",
      "epoch: 8 step: 276, loss is 0.08102512\n",
      "epoch: 8 step: 277, loss is 0.090940475\n",
      "epoch: 8 step: 278, loss is 0.09219599\n",
      "epoch: 8 step: 279, loss is 0.08660555\n",
      "epoch: 8 step: 280, loss is 0.085282564\n",
      "epoch: 8 step: 281, loss is 0.093212366\n",
      "epoch: 8 step: 282, loss is 0.091926575\n",
      "epoch: 8 step: 283, loss is 0.09424728\n",
      "epoch: 8 step: 284, loss is 0.093684494\n",
      "epoch: 8 step: 285, loss is 0.09263986\n",
      "epoch: 8 step: 286, loss is 0.08788031\n",
      "epoch: 8 step: 287, loss is 0.08529323\n",
      "epoch: 8 step: 288, loss is 0.09281403\n",
      "epoch: 8 step: 289, loss is 0.090577304\n",
      "epoch: 8 step: 290, loss is 0.09013265\n",
      "epoch: 8 step: 291, loss is 0.08997375\n",
      "epoch: 8 step: 292, loss is 0.13481063\n",
      "epoch: 8 step: 293, loss is 0.09060335\n",
      "epoch: 8 step: 294, loss is 0.086375415\n",
      "epoch: 8 step: 295, loss is 0.08625716\n",
      "epoch: 8 step: 296, loss is 0.10377616\n",
      "epoch: 8 step: 297, loss is 0.069197655\n",
      "epoch: 8 step: 298, loss is 0.08413601\n",
      "epoch: 8 step: 299, loss is 0.08040398\n",
      "epoch: 8 step: 300, loss is 0.06603342\n",
      "epoch: 8 step: 301, loss is 0.08571148\n",
      "epoch: 8 step: 302, loss is 0.07427114\n",
      "epoch: 8 step: 303, loss is 0.08390266\n",
      "epoch: 8 step: 304, loss is 0.07385975\n",
      "epoch: 8 step: 305, loss is 0.06576228\n",
      "epoch: 8 step: 306, loss is 0.07860273\n",
      "epoch: 8 step: 307, loss is 0.07813531\n",
      "epoch: 8 step: 308, loss is 0.08910298\n",
      "epoch: 8 step: 309, loss is 0.084418476\n",
      "epoch: 8 step: 310, loss is 0.08115888\n",
      "epoch: 8 step: 311, loss is 0.07172698\n",
      "epoch: 8 step: 312, loss is 0.08635354\n",
      "epoch: 8 step: 313, loss is 0.085543156\n",
      "epoch: 8 step: 314, loss is 0.07571143\n",
      "epoch: 8 step: 315, loss is 0.08822793\n",
      "epoch: 8 step: 316, loss is 0.08398867\n",
      "epoch: 8 step: 317, loss is 0.07448411\n",
      "epoch: 8 step: 318, loss is 0.08361763\n",
      "epoch: 8 step: 319, loss is 0.0844962\n",
      "epoch: 8 step: 320, loss is 0.0842101\n",
      "epoch: 8 step: 321, loss is 0.08404851\n",
      "epoch: 8 step: 322, loss is 0.09047729\n",
      "epoch: 8 step: 323, loss is 0.09268111\n",
      "epoch: 8 step: 324, loss is 0.102176905\n",
      "epoch: 8 step: 325, loss is 0.097560406\n",
      "epoch: 8 step: 326, loss is 0.084480286\n",
      "epoch: 8 step: 327, loss is 0.088653326\n",
      "epoch: 8 step: 328, loss is 0.08991665\n",
      "epoch: 8 step: 329, loss is 0.09743047\n",
      "epoch: 8 step: 330, loss is 0.08750957\n",
      "epoch: 8 step: 331, loss is 0.09996992\n",
      "epoch: 8 step: 332, loss is 0.10213822\n",
      "epoch: 8 step: 333, loss is 0.19360304\n",
      "epoch: 8 step: 334, loss is 0.113197625\n",
      "epoch: 8 step: 335, loss is 0.11368829\n",
      "epoch: 8 step: 336, loss is 0.12095475\n",
      "epoch: 8 step: 337, loss is 0.11310029\n",
      "epoch: 8 step: 338, loss is 0.1136567\n",
      "epoch: 8 step: 339, loss is 0.09846258\n",
      "epoch: 8 step: 340, loss is 0.10985279\n",
      "epoch: 8 step: 341, loss is 0.09475976\n",
      "epoch: 8 step: 342, loss is 0.10015684\n",
      "epoch: 8 step: 343, loss is 0.10939187\n",
      "epoch: 8 step: 344, loss is 0.10109061\n",
      "epoch: 8 step: 345, loss is 0.099870205\n",
      "epoch: 8 step: 346, loss is 0.116235256\n",
      "epoch: 8 step: 347, loss is 0.10060477\n",
      "epoch: 8 step: 348, loss is 0.09613782\n",
      "epoch: 8 step: 349, loss is 0.10845786\n",
      "epoch: 8 step: 350, loss is 0.09943789\n",
      "epoch: 8 step: 351, loss is 0.117423534\n",
      "epoch: 8 step: 352, loss is 0.10902089\n",
      "epoch: 8 step: 353, loss is 0.100821435\n",
      "epoch: 8 step: 354, loss is 0.10940331\n",
      "epoch: 8 step: 355, loss is 0.095761836\n",
      "epoch: 8 step: 356, loss is 0.11325973\n",
      "epoch: 8 step: 357, loss is 0.11068821\n",
      "epoch: 8 step: 358, loss is 0.11713958\n",
      "epoch: 8 step: 359, loss is 0.11517519\n",
      "epoch: 8 step: 360, loss is 0.12056023\n",
      "epoch: 8 step: 361, loss is 0.020080268\n",
      "epoch: 8 step: 362, loss is 0.122495234\n",
      "epoch: 8 step: 363, loss is 0.09148437\n",
      "epoch: 8 step: 364, loss is 0.11406785\n",
      "epoch: 8 step: 365, loss is 0.097675025\n",
      "epoch: 8 step: 366, loss is 0.11557871\n",
      "epoch: 8 step: 367, loss is 0.11344975\n",
      "epoch: 8 step: 368, loss is 0.10592562\n",
      "epoch: 8 step: 369, loss is 0.10108286\n",
      "epoch: 8 step: 370, loss is 0.10340911\n",
      "epoch: 8 step: 371, loss is 0.09407258\n",
      "epoch: 8 step: 372, loss is 0.10043126\n",
      "epoch: 8 step: 373, loss is 0.09717816\n",
      "epoch: 8 step: 374, loss is 0.10779762\n",
      "epoch: 8 step: 375, loss is 0.1074844\n",
      "epoch: 8 step: 376, loss is 0.11337823\n",
      "epoch: 8 step: 377, loss is 0.11015326\n",
      "epoch: 8 step: 378, loss is 0.09530276\n",
      "epoch: 8 step: 379, loss is 0.10770351\n",
      "epoch: 8 step: 380, loss is 0.10898137\n",
      "epoch: 8 step: 381, loss is 0.10052103\n",
      "epoch: 8 step: 382, loss is -0.0058555603\n",
      "epoch: 8 step: 383, loss is 0.1151371\n",
      "epoch: 8 step: 384, loss is 0.09910959\n",
      "epoch: 8 step: 385, loss is 0.10873556\n",
      "epoch: 8 step: 386, loss is 0.102404356\n",
      "epoch: 8 step: 387, loss is 0.09854251\n",
      "epoch: 8 step: 388, loss is 0.10469097\n",
      "epoch: 8 step: 389, loss is 0.116912186\n",
      "epoch: 8 step: 390, loss is 0.097789824\n",
      "epoch: 8 step: 391, loss is 0.096958876\n",
      "epoch: 8 step: 392, loss is 0.10742968\n",
      "epoch: 8 step: 393, loss is 0.100450516\n",
      "epoch: 8 step: 394, loss is 0.10441589\n",
      "epoch: 8 step: 395, loss is 0.103628874\n",
      "epoch: 8 step: 396, loss is 0.10524607\n",
      "epoch: 8 step: 397, loss is 0.11276537\n",
      "epoch: 8 step: 398, loss is 0.111242056\n",
      "epoch: 8 step: 399, loss is 0.17286777\n",
      "epoch: 8 step: 400, loss is 0.11001378\n",
      "epoch: 8 step: 401, loss is 0.10739201\n",
      "epoch: 8 step: 402, loss is 0.110529244\n",
      "epoch: 8 step: 403, loss is 0.114733994\n",
      "epoch: 8 step: 404, loss is 0.113024235\n",
      "epoch: 8 step: 405, loss is 0.11664605\n",
      "epoch: 8 step: 406, loss is 0.10576439\n",
      "epoch: 8 step: 407, loss is 0.115903914\n",
      "epoch: 8 step: 408, loss is 0.10344434\n",
      "epoch: 8 step: 409, loss is 0.107229054\n",
      "epoch: 8 step: 410, loss is 0.11310053\n",
      "epoch: 8 step: 411, loss is 0.10547966\n",
      "epoch: 8 step: 412, loss is 0.12612057\n",
      "epoch: 8 step: 413, loss is 0.120838344\n",
      "epoch: 8 step: 414, loss is 0.116646826\n",
      "epoch: 8 step: 415, loss is 0.123960316\n",
      "epoch: 8 step: 416, loss is 0.10851383\n",
      "epoch: 8 step: 417, loss is 0.11415458\n",
      "epoch: 8 step: 418, loss is 0.1137082\n",
      "epoch: 8 step: 419, loss is 0.11109418\n",
      "epoch: 8 step: 420, loss is 0.09965581\n",
      "epoch: 8 step: 421, loss is 0.100367546\n",
      "epoch: 8 step: 422, loss is 0.099827826\n",
      "epoch: 8 step: 423, loss is 0.105983734\n",
      "epoch: 8 step: 424, loss is 0.09852785\n",
      "epoch: 8 step: 425, loss is 0.11551601\n",
      "epoch: 8 step: 426, loss is 0.26400185\n",
      "epoch: 8 step: 427, loss is 0.09669018\n",
      "epoch: 8 step: 428, loss is 0.09673697\n",
      "epoch: 8 step: 429, loss is 0.10401982\n",
      "epoch: 8 step: 430, loss is 0.10618144\n",
      "epoch: 8 step: 431, loss is 0.08595306\n",
      "epoch: 8 step: 432, loss is 0.108807504\n",
      "epoch: 8 step: 433, loss is 0.115620315\n",
      "epoch: 8 step: 434, loss is 0.10182142\n",
      "epoch: 8 step: 435, loss is 0.10810536\n",
      "epoch: 8 step: 436, loss is 0.10485464\n",
      "epoch: 8 step: 437, loss is 0.08950108\n",
      "epoch: 8 step: 438, loss is 0.104947746\n",
      "epoch: 8 step: 439, loss is 0.1113655\n",
      "epoch: 8 step: 440, loss is 0.105555475\n",
      "epoch: 8 step: 441, loss is 0.11127561\n",
      "epoch: 8 step: 442, loss is 0.104745805\n",
      "epoch: 8 step: 443, loss is 0.10791153\n",
      "epoch: 8 step: 444, loss is 0.10246277\n",
      "epoch: 8 step: 445, loss is 0.118807614\n",
      "epoch: 8 step: 446, loss is 0.10494828\n",
      "epoch: 8 step: 447, loss is 0.11490536\n",
      "epoch: 8 step: 448, loss is 0.10797423\n",
      "epoch: 8 step: 449, loss is 0.11131054\n",
      "epoch: 8 step: 450, loss is 0.09892434\n",
      "epoch: 8 step: 451, loss is 0.093970954\n",
      "epoch: 8 step: 452, loss is 0.0998618\n",
      "epoch: 8 step: 453, loss is 0.1069532\n",
      "epoch: 8 step: 454, loss is 0.097955406\n",
      "epoch: 8 step: 455, loss is 0.09764385\n",
      "epoch: 8 step: 456, loss is 0.107583284\n",
      "epoch: 8 step: 457, loss is 0.094773054\n",
      "epoch: 8 step: 458, loss is 0.09989625\n",
      "epoch: 8 step: 459, loss is 0.1270296\n",
      "epoch: 8 step: 460, loss is 0.12464309\n",
      "epoch: 8 step: 461, loss is 0.10589892\n",
      "epoch: 8 step: 462, loss is 0.10964966\n",
      "epoch: 8 step: 463, loss is 0.11618394\n",
      "epoch: 8 step: 464, loss is 0.10315257\n",
      "epoch: 8 step: 465, loss is 0.117868245\n",
      "epoch: 8 step: 466, loss is 0.104964495\n",
      "epoch: 8 step: 467, loss is 0.11234528\n",
      "epoch: 8 step: 468, loss is 0.10845786\n",
      "epoch: 8 step: 469, loss is 0.10754377\n",
      "epoch: 8 step: 470, loss is 0.119541645\n",
      "epoch: 8 step: 471, loss is 0.1105755\n",
      "epoch: 8 step: 472, loss is 0.09756118\n",
      "epoch: 8 step: 473, loss is 0.11041278\n",
      "epoch: 8 step: 474, loss is 0.11092043\n",
      "epoch: 8 step: 475, loss is 0.1076687\n",
      "epoch: 8 step: 476, loss is 0.10389185\n",
      "epoch: 8 step: 477, loss is 0.1024819\n",
      "epoch: 8 step: 478, loss is 0.10825634\n",
      "epoch: 8 step: 479, loss is 0.11137438\n",
      "epoch: 8 step: 480, loss is 0.10942322\n",
      "epoch: 8 step: 481, loss is 0.14336234\n",
      "epoch: 8 step: 482, loss is 0.11655742\n",
      "epoch: 8 step: 483, loss is 0.11735088\n",
      "epoch: 8 step: 484, loss is 0.12657356\n",
      "epoch: 8 step: 485, loss is 0.107987165\n",
      "epoch: 8 step: 486, loss is 0.10690278\n",
      "epoch: 8 step: 487, loss is 0.11041647\n",
      "epoch: 8 step: 488, loss is 0.106761456\n",
      "epoch: 8 step: 489, loss is 0.118407965\n",
      "epoch: 8 step: 490, loss is 0.10515374\n",
      "epoch: 8 step: 491, loss is 0.11681247\n",
      "epoch: 8 step: 492, loss is 0.12132853\n",
      "epoch: 8 step: 493, loss is 0.10245639\n",
      "epoch: 8 step: 494, loss is 0.10206652\n",
      "epoch: 8 step: 495, loss is 0.10527873\n",
      "epoch: 8 step: 496, loss is 0.08929825\n",
      "epoch: 8 step: 497, loss is 0.09668422\n",
      "epoch: 8 step: 498, loss is 0.107886374\n",
      "epoch: 8 step: 499, loss is 0.105433464\n",
      "epoch: 8 step: 500, loss is 0.08599949\n",
      "epoch: 8 step: 501, loss is 0.102766514\n",
      "epoch: 8 step: 502, loss is 0.10950595\n",
      "epoch: 8 step: 503, loss is 0.102072716\n",
      "epoch: 8 step: 504, loss is 0.11515659\n",
      "epoch: 8 step: 505, loss is 0.11070871\n",
      "epoch: 8 step: 506, loss is 0.11429626\n",
      "epoch: 8 step: 507, loss is 0.09912419\n",
      "epoch: 8 step: 508, loss is 0.17069024\n",
      "epoch: 8 step: 509, loss is 0.0994609\n",
      "epoch: 8 step: 510, loss is 0.10647577\n",
      "epoch: 8 step: 511, loss is 0.09522909\n",
      "epoch: 8 step: 512, loss is 0.086845696\n",
      "epoch: 8 step: 513, loss is 0.09955168\n",
      "epoch: 8 step: 514, loss is 0.10670215\n",
      "epoch: 8 step: 515, loss is 0.098847866\n",
      "epoch: 8 step: 516, loss is 0.09418219\n",
      "epoch: 8 step: 517, loss is 0.09933835\n",
      "epoch: 8 step: 518, loss is 0.10008407\n",
      "epoch: 8 step: 519, loss is 0.11590952\n",
      "epoch: 8 step: 520, loss is 0.098650634\n",
      "epoch: 8 step: 521, loss is 0.112032115\n",
      "epoch: 8 step: 522, loss is 0.10236758\n",
      "epoch: 8 step: 523, loss is 0.104398966\n",
      "epoch: 8 step: 524, loss is 0.10277468\n",
      "epoch: 8 step: 525, loss is 0.100664794\n",
      "epoch: 8 step: 526, loss is 0.09902668\n",
      "epoch: 8 step: 527, loss is 0.10820174\n",
      "epoch: 8 step: 528, loss is 0.093826115\n",
      "epoch: 8 step: 529, loss is 0.1013937\n",
      "epoch: 8 step: 530, loss is 0.09490585\n",
      "epoch: 8 step: 531, loss is 0.1123206\n",
      "epoch: 8 step: 532, loss is 0.09320974\n",
      "epoch: 8 step: 533, loss is 0.08704132\n",
      "epoch: 8 step: 534, loss is 0.10573596\n",
      "epoch: 8 step: 535, loss is 0.106585324\n",
      "epoch: 8 step: 536, loss is 0.10143787\n",
      "epoch: 8 step: 537, loss is 0.10248828\n",
      "epoch: 8 step: 538, loss is 0.09988046\n",
      "epoch: 8 step: 539, loss is 0.10146308\n",
      "epoch: 8 step: 540, loss is 0.088425815\n",
      "epoch: 8 step: 541, loss is 0.099184215\n",
      "epoch: 8 step: 542, loss is 0.1204927\n",
      "epoch: 8 step: 543, loss is 0.10738754\n",
      "epoch: 8 step: 544, loss is 0.113296926\n",
      "epoch: 8 step: 545, loss is 0.10174483\n",
      "epoch: 8 step: 546, loss is 0.11446196\n",
      "epoch: 8 step: 547, loss is 0.09078699\n",
      "epoch: 8 step: 548, loss is 0.10921794\n",
      "epoch: 8 step: 549, loss is 0.10486555\n",
      "epoch: 8 step: 550, loss is 0.10131121\n",
      "epoch: 8 step: 551, loss is 0.112488925\n",
      "epoch: 8 step: 552, loss is 0.1049881\n",
      "epoch: 8 step: 553, loss is 0.28907967\n",
      "epoch: 8 step: 554, loss is 0.09834552\n",
      "epoch: 8 step: 555, loss is 0.0847733\n",
      "epoch: 8 step: 556, loss is 0.103485644\n",
      "epoch: 8 step: 557, loss is 0.09507543\n",
      "epoch: 8 step: 558, loss is 0.1025812\n",
      "epoch: 8 step: 559, loss is 0.09978777\n",
      "epoch: 8 step: 560, loss is 0.10315204\n",
      "epoch: 8 step: 561, loss is 0.09154028\n",
      "epoch: 8 step: 562, loss is 0.09101945\n",
      "epoch: 8 step: 563, loss is 0.0976941\n",
      "epoch: 8 step: 564, loss is 0.09023267\n",
      "epoch: 8 step: 565, loss is 0.11215663\n",
      "epoch: 8 step: 566, loss is 0.09758896\n",
      "epoch: 8 step: 567, loss is 0.10381049\n",
      "epoch: 8 step: 568, loss is 0.10900545\n",
      "epoch: 8 step: 569, loss is 0.106954515\n",
      "epoch: 8 step: 570, loss is 0.091805935\n",
      "epoch: 8 step: 571, loss is 0.11173272\n",
      "epoch: 8 step: 572, loss is 0.12967068\n",
      "epoch: 8 step: 573, loss is 0.10672331\n",
      "epoch: 8 step: 574, loss is 0.1164214\n",
      "epoch: 8 step: 575, loss is 0.110996544\n",
      "epoch: 8 step: 576, loss is 0.11696339\n",
      "epoch: 8 step: 577, loss is 0.11714959\n",
      "epoch: 8 step: 578, loss is 0.11451435\n",
      "epoch: 8 step: 579, loss is 0.09638041\n",
      "epoch: 8 step: 580, loss is 0.11626226\n",
      "epoch: 8 step: 581, loss is 0.11755866\n",
      "epoch: 8 step: 582, loss is 0.10487205\n",
      "epoch: 8 step: 583, loss is 0.11581421\n",
      "epoch: 8 step: 584, loss is 0.11916685\n",
      "epoch: 8 step: 585, loss is 0.10948306\n",
      "epoch: 8 step: 586, loss is 0.11203998\n",
      "epoch: 8 step: 587, loss is 0.123948395\n",
      "epoch: 8 step: 588, loss is 0.097200096\n",
      "epoch: 8 step: 589, loss is 0.10705614\n",
      "epoch: 8 step: 590, loss is 0.10888845\n",
      "epoch: 8 step: 591, loss is 0.09876585\n",
      "epoch: 8 step: 592, loss is 0.12030488\n",
      "epoch: 8 step: 593, loss is 0.14421362\n",
      "epoch: 8 step: 594, loss is 0.10370737\n",
      "epoch: 8 step: 595, loss is 0.10601902\n",
      "epoch: 8 step: 596, loss is 0.11755395\n",
      "epoch: 8 step: 597, loss is 0.09877086\n",
      "epoch: 8 step: 598, loss is 0.1119321\n",
      "epoch: 8 step: 599, loss is 0.094543934\n",
      "epoch: 8 step: 600, loss is 0.106220424\n",
      "epoch: 8 step: 601, loss is 0.10260606\n",
      "epoch: 8 step: 602, loss is 0.09645957\n",
      "epoch: 8 step: 603, loss is 0.09774071\n",
      "epoch: 8 step: 604, loss is 0.108177245\n",
      "epoch: 8 step: 605, loss is 0.0864346\n",
      "epoch: 8 step: 606, loss is 0.09047121\n",
      "epoch: 8 step: 607, loss is 0.10576469\n",
      "epoch: 8 step: 608, loss is 0.09300333\n",
      "epoch: 8 step: 609, loss is 0.103336394\n",
      "epoch: 8 step: 610, loss is 0.09635383\n",
      "epoch: 8 step: 611, loss is 0.10813218\n",
      "epoch: 8 step: 612, loss is 0.10868186\n",
      "epoch: 8 step: 613, loss is 0.109523\n",
      "epoch: 8 step: 614, loss is 0.102758884\n",
      "epoch: 8 step: 615, loss is 0.10952973\n",
      "epoch: 8 step: 616, loss is 0.1079728\n",
      "epoch: 8 step: 617, loss is 0.09844303\n",
      "epoch: 8 step: 618, loss is 0.093780756\n",
      "epoch: 8 step: 619, loss is 0.11739069\n",
      "epoch: 8 step: 620, loss is 0.11483407\n",
      "epoch: 8 step: 621, loss is 0.108048975\n",
      "epoch: 8 step: 622, loss is 0.110657215\n",
      "epoch: 8 step: 623, loss is 0.10889739\n",
      "epoch: 8 step: 624, loss is 0.092602074\n",
      "epoch: 8 step: 625, loss is 0.0924592\n",
      "epoch: 8 step: 626, loss is 0.11660105\n",
      "epoch: 8 step: 627, loss is 0.10187906\n",
      "epoch: 8 step: 628, loss is 0.10294652\n",
      "epoch: 8 step: 629, loss is 0.09409779\n",
      "epoch: 8 step: 630, loss is 0.10065216\n",
      "epoch: 8 step: 631, loss is 0.099861324\n",
      "epoch: 8 step: 632, loss is 0.10424012\n",
      "epoch: 8 step: 633, loss is 0.10829669\n",
      "epoch: 8 step: 634, loss is 0.099603415\n",
      "epoch: 8 step: 635, loss is 0.096060276\n",
      "epoch: 8 step: 636, loss is 0.10000974\n",
      "epoch: 8 step: 637, loss is 0.10374373\n",
      "epoch: 8 step: 638, loss is 0.09339172\n",
      "epoch: 8 step: 639, loss is 0.10359669\n",
      "epoch: 8 step: 640, loss is 0.11084324\n",
      "epoch: 8 step: 641, loss is 0.10275489\n",
      "epoch: 8 step: 642, loss is 0.0989635\n",
      "epoch: 8 step: 643, loss is 0.101898134\n",
      "epoch: 8 step: 644, loss is 0.1052677\n",
      "epoch: 8 step: 645, loss is 0.110829294\n",
      "epoch: 8 step: 646, loss is 0.10767692\n",
      "epoch: 8 step: 647, loss is 0.11514956\n",
      "epoch: 8 step: 648, loss is 0.11321515\n",
      "epoch: 8 step: 649, loss is 0.118840516\n",
      "epoch: 8 step: 650, loss is 0.10321075\n",
      "epoch: 8 step: 651, loss is 0.106277466\n",
      "epoch: 8 step: 652, loss is 0.08795315\n",
      "epoch: 8 step: 653, loss is 0.092060745\n",
      "epoch: 8 step: 654, loss is 0.10064155\n",
      "epoch: 8 step: 655, loss is 0.106677115\n",
      "epoch: 8 step: 656, loss is 0.09937048\n",
      "epoch: 8 step: 657, loss is 0.10440129\n",
      "epoch: 8 step: 658, loss is 0.08566934\n",
      "epoch: 8 step: 659, loss is 0.10059023\n",
      "epoch: 8 step: 660, loss is 0.107102096\n",
      "epoch: 8 step: 661, loss is 0.10644412\n",
      "epoch: 8 step: 662, loss is 0.08034366\n",
      "epoch: 8 step: 663, loss is 0.10620147\n",
      "epoch: 8 step: 664, loss is 0.09546393\n",
      "epoch: 8 step: 665, loss is 0.10486442\n",
      "epoch: 8 step: 666, loss is 0.09343028\n",
      "epoch: 8 step: 667, loss is 0.09279066\n",
      "epoch: 8 step: 668, loss is 0.10454363\n",
      "epoch: 8 step: 669, loss is 0.11241764\n",
      "epoch: 8 step: 670, loss is 0.11972588\n",
      "epoch: 8 step: 671, loss is 0.13899112\n",
      "epoch: 8 step: 672, loss is 0.10729438\n",
      "epoch: 8 step: 673, loss is 0.107985795\n",
      "epoch: 8 step: 674, loss is 0.11153406\n",
      "epoch: 8 step: 675, loss is 0.10628408\n",
      "epoch: 8 step: 676, loss is 0.11013764\n",
      "epoch: 8 step: 677, loss is 0.103283346\n",
      "epoch: 8 step: 678, loss is 0.10812813\n",
      "epoch: 8 step: 679, loss is 0.11075139\n",
      "epoch: 8 step: 680, loss is 0.1101554\n",
      "epoch: 8 step: 681, loss is 0.10072416\n",
      "epoch: 8 step: 682, loss is 0.11090976\n",
      "epoch: 8 step: 683, loss is 0.10718781\n",
      "epoch: 8 step: 684, loss is 0.10077667\n",
      "epoch: 8 step: 685, loss is 0.11174244\n",
      "epoch: 8 step: 686, loss is 0.078247786\n",
      "epoch: 8 step: 687, loss is 0.122601986\n",
      "epoch: 8 step: 688, loss is 0.11485505\n",
      "epoch: 8 step: 689, loss is 0.1068635\n",
      "epoch: 8 step: 690, loss is 0.092228234\n",
      "epoch: 8 step: 691, loss is 0.114813745\n",
      "epoch: 8 step: 692, loss is 0.11314708\n",
      "epoch: 8 step: 693, loss is 0.12203461\n",
      "epoch: 8 step: 694, loss is 0.10479164\n",
      "epoch: 8 step: 695, loss is 0.12656862\n",
      "epoch: 8 step: 696, loss is 0.104893744\n",
      "epoch: 8 step: 697, loss is 0.10520172\n",
      "epoch: 8 step: 698, loss is 0.14164704\n",
      "epoch: 8 step: 699, loss is 0.104323626\n",
      "epoch: 8 step: 700, loss is 0.09554815\n",
      "epoch: 8 step: 701, loss is 0.10085964\n",
      "epoch: 8 step: 702, loss is 0.10828\n",
      "epoch: 8 step: 703, loss is 0.10236853\n",
      "epoch: 8 step: 704, loss is 0.11489701\n",
      "epoch: 8 step: 705, loss is 0.09623569\n",
      "epoch: 8 step: 706, loss is 0.10164076\n",
      "epoch: 8 step: 707, loss is 0.096368134\n",
      "epoch: 8 step: 708, loss is 0.09997392\n",
      "epoch: 8 step: 709, loss is 0.09295839\n",
      "epoch: 8 step: 710, loss is 0.121403754\n",
      "epoch: 8 step: 711, loss is 0.104379475\n",
      "epoch: 8 step: 712, loss is 0.105823696\n",
      "epoch: 8 step: 713, loss is 0.107075036\n",
      "epoch: 8 step: 714, loss is 0.10434979\n",
      "epoch: 8 step: 715, loss is 0.1019156\n",
      "epoch: 8 step: 716, loss is 0.095275104\n",
      "epoch: 8 step: 717, loss is 0.04058224\n",
      "epoch: 8 step: 718, loss is 0.10874766\n",
      "epoch: 8 step: 719, loss is 0.10487074\n",
      "epoch: 8 step: 720, loss is 0.11065292\n",
      "epoch: 8 step: 721, loss is 0.104764044\n",
      "epoch: 8 step: 722, loss is 0.10734773\n",
      "epoch: 8 step: 723, loss is 0.10471827\n",
      "epoch: 8 step: 724, loss is 0.10805386\n",
      "epoch: 8 step: 725, loss is 0.10757214\n",
      "epoch: 8 step: 726, loss is 0.104280055\n",
      "epoch: 8 step: 727, loss is 0.1094563\n",
      "epoch: 8 step: 728, loss is 0.11381531\n",
      "epoch: 8 step: 729, loss is 0.1079815\n",
      "epoch: 8 step: 730, loss is 0.10897201\n",
      "epoch: 8 step: 731, loss is 0.105823934\n",
      "epoch: 8 step: 732, loss is 0.105341256\n",
      "epoch: 8 step: 733, loss is 0.095159054\n",
      "epoch: 8 step: 734, loss is 0.104195535\n",
      "epoch: 8 step: 735, loss is 0.102721035\n",
      "epoch: 8 step: 736, loss is 0.10623139\n",
      "epoch: 8 step: 737, loss is 0.10514784\n",
      "epoch: 8 step: 738, loss is 0.10524303\n",
      "epoch: 8 step: 739, loss is 0.10548979\n",
      "epoch: 8 step: 740, loss is 0.10866052\n",
      "epoch: 8 step: 741, loss is 0.10474473\n",
      "epoch: 8 step: 742, loss is 0.11171371\n",
      "epoch: 8 step: 743, loss is 0.106273174\n",
      "epoch: 8 step: 744, loss is 0.10634154\n",
      "epoch: 8 step: 745, loss is 0.10654527\n",
      "epoch: 8 step: 746, loss is 0.11136764\n",
      "epoch: 8 step: 747, loss is 0.33859348\n",
      "epoch: 8 step: 748, loss is 0.10448533\n",
      "epoch: 8 step: 749, loss is 0.10780144\n",
      "epoch: 8 step: 750, loss is 0.11208612\n",
      "epoch: 8 step: 751, loss is 0.12507361\n",
      "epoch: 8 step: 752, loss is 0.10696894\n",
      "epoch: 8 step: 753, loss is 0.11039424\n",
      "epoch: 8 step: 754, loss is 0.11455327\n",
      "epoch: 8 step: 755, loss is 0.10825032\n",
      "epoch: 8 step: 756, loss is 0.11762619\n",
      "epoch: 8 step: 757, loss is 0.1125887\n",
      "epoch: 8 step: 758, loss is 0.12280965\n",
      "epoch: 8 step: 759, loss is 0.12971401\n",
      "epoch: 8 step: 760, loss is 0.10869497\n",
      "epoch: 8 step: 761, loss is 0.117932856\n",
      "epoch: 8 step: 762, loss is 0.10779625\n",
      "epoch: 8 step: 763, loss is 0.097620726\n",
      "epoch: 8 step: 764, loss is 0.108925045\n",
      "epoch: 8 step: 765, loss is 0.11428571\n",
      "epoch: 8 step: 766, loss is 0.116393566\n",
      "epoch: 8 step: 767, loss is 0.07664204\n",
      "epoch: 8 step: 768, loss is 0.1073156\n",
      "epoch: 8 step: 769, loss is 0.10320711\n",
      "epoch: 8 step: 770, loss is 0.105629444\n",
      "epoch: 8 step: 771, loss is 0.10224718\n",
      "epoch: 8 step: 772, loss is 0.09543055\n",
      "epoch: 8 step: 773, loss is 0.10510057\n",
      "epoch: 8 step: 774, loss is 0.11374575\n",
      "epoch: 8 step: 775, loss is 0.10578364\n"
     ]
    }
   ],
   "source": [
    "###training \n",
    "#Loss=MISS_1()\n",
    "Loss2=SLoss()\n",
    "trainDataset=imgDataset(tra)\n",
    "trainData=ds.GeneratorDataset(trainDataset,column_names=['img','label'],num_parallel_workers=4)\n",
    "trainData=trainData.batch(1)\n",
    "\n",
    "optim=nn.RMSProp(params=NET.trainable_params(), learning_rate=0.001)\n",
    "trainnet=Model(NET,loss_fn=Loss2,optimizer=optim)\n",
    "loss_cb = LossMonitor(per_print_times=1)\n",
    "ckpt_config = CheckpointConfig(save_checkpoint_steps=1, keep_checkpoint_max=1)\n",
    "ckpoint_cb = ModelCheckpoint(prefix='coloring', directory='./model', config=ckpt_config)\n",
    "print('start    training')\n",
    "trainnet.train(8,trainData,callbacks=[loss_cb,ckpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e4b14cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6, 80, 128)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mindspore/ccsrc/backend/session/kernel_build_client.h:97 Request] Try to send request before Open()\n\n# ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-69e94377091b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mgt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mlgt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mrgt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmoveaxis\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/MindSpore-python3.7-aarch64/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mmoveaxis\u001b[0;34m(a, source, destination)\u001b[0m\n\u001b[1;32m   1463\u001b[0m         \u001b[0morder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/MindSpore-python3.7-aarch64/lib/python3.7/site-packages/mindspore/common/tensor.py\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(self, *axes)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0mperm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_transpose_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_operator_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'transpose'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/MindSpore-python3.7-aarch64/lib/python3.7/site-packages/mindspore/ops/primitive.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshould_elim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_run_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/MindSpore-python3.7-aarch64/lib/python3.7/site-packages/mindspore/common/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*arg, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_convert_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/MindSpore-python3.7-aarch64/lib/python3.7/site-packages/mindspore/ops/primitive.py\u001b[0m in \u001b[0;36m_run_op\u001b[0;34m(obj, op_name, args)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_run_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;34m\"\"\"Single op execution function supported by ge in PyNative mode.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_run_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mindspore/ccsrc/backend/session/kernel_build_client.h:97 Request] Try to send request before Open()\n\n# "
     ]
    }
   ],
   "source": [
    "#推理部分\n",
    "from mindspore import load_checkpoint, load_param_into_net\n",
    "ckpt_file_name = \"./model/coloring_2-8_775.ckpt\"\n",
    "param_set = load_checkpoint(ckpt_file_name)\n",
    "net=cyclecolornet()\n",
    "load_param_into_net(net, param_set)\n",
    "Loss3=SLoss()\n",
    "optim=nn.RMSProp(params=NET.trainable_params(), learning_rate=0.001)\n",
    "model=Model(net,loss_fn=Loss3,optimizer=optim)\n",
    "\n",
    "testDataset=imgDataset(val)\n",
    "testdataset=ds.GeneratorDataset(testDataset,column_names=['img','label'],num_parallel_workers=4)\n",
    "testdataset=testdataset.batch(1)\n",
    "\n",
    "testdata_iter = testdataset.create_dict_iterator()\n",
    "testdata = next(testdata_iter)\n",
    "#print(Tensor(testdata['img']).shape)\n",
    "predicted = model.predict(Tensor(testdata['img']))\n",
    "predicted=predicted[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "683e1c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1748381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae726614",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore-python3.7-aarch64",
   "language": "python",
   "name": "mindspore-python3.7-aarch64"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
